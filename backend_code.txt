

==================================================
FILE: _ops/debug-api.js
==================================================

const fs = require('fs');

async function main() {
  try {
    const output = {};

    console.log('--- Fetching Structure ---');
    const structureRes = await fetch('http://localhost:3001/api/public/assessments/active/structure');
    output.structure = await structureRes.json();

    console.log('--- Fetching Recommendations Definition ---');
    const recDefRes = await fetch('http://localhost:3001/api/public/recommendations/definition');
    output.recDef = await recDefRes.json();

    console.log('--- Fetching Results ---');
    const resultsRes = await fetch('http://localhost:3001/api/public/responses/c3510d28-478d-4a57-876c-56c06da5da56/results');
    output.results = await resultsRes.json();

    fs.writeFileSync('debug-output.json', JSON.stringify(output, null, 2));
    console.log('Done writing debug-output.json');

  } catch (e) {
    console.error('Error:', e);
  }
}

main();


==================================================
FILE: _ops/fix-admin-password.ts
==================================================

﻿import bcrypt from 'bcrypt';
import { query } from '../src/config/database';

const ADMIN_EMAIL = process.env.ADMIN_EMAIL || 'admin@leadership.com';

const getPassword = (): string => {
  const password = process.env.SEED_ADMIN_PASSWORD;
  if (!password || password.length < 12) {
    throw new Error('[SEED] SEED_ADMIN_PASSWORD env var is required and must be >=12 chars');
  }
  return password;
};

async function fixPassword() {
  try {
    const password = getPassword();
    const passwordHash = await bcrypt.hash(password, 10);
    const result = await query(
      'UPDATE users SET password_hash = $1 WHERE email = $2 RETURNING id, email',
      [passwordHash, ADMIN_EMAIL]
    );
    console.log('Password updated for admin user count:', result.rowCount || 0);
  } catch (error) {
    console.error('Error:', error);
    process.exit(1);
  }
}

fixPassword();


==================================================
FILE: _ops/fix-table.ts
==================================================

﻿import { pool } from '../src/config/database';

async function fixTable() {
  const client = await pool.connect();
  try {
    console.log('Dropping topic_responses table...');
    await client.query('DROP TABLE IF EXISTS topic_responses CASCADE');

    console.log('Creating topic_responses with DECIMAL columns...');
    await client.query(`
      CREATE TABLE topic_responses (
        id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
        response_id UUID NOT NULL REFERENCES assessment_responses(id),
        topic_id UUID NOT NULL REFERENCES topics(id),
        current_rating DECIMAL(5,2) CHECK (current_rating >= 1 AND current_rating <= 5),
        target_rating DECIMAL(5,2) CHECK (target_rating >= 1 AND target_rating <= 5),
        gap DECIMAL(5,2),
        normalized_gap DECIMAL(5,2),
        time_spent_seconds INTEGER DEFAULT 0,
        notes TEXT,
        answered_at TIMESTAMP DEFAULT NOW(),
        created_at TIMESTAMP DEFAULT NOW(),
        updated_at TIMESTAMP DEFAULT NOW(),
        UNIQUE(response_id, topic_id)
      )
    `);

    console.log('Creating indexes...');
    await client.query('CREATE INDEX idx_topic_responses_response ON topic_responses(response_id)');
    await client.query('CREATE INDEX idx_topic_responses_topic ON topic_responses(topic_id)');

    console.log('Table recreated with DECIMAL columns');
    process.exit(0);
  } catch (error) {
    console.error(error);
    process.exit(1);
  } finally {
    client.release();
    await pool.end();
  }
}

fixTable();


==================================================
FILE: _ops/reset-admin-password.ts
==================================================

﻿/**
 * Reset admin password and unlock account.
 * Run: npm run reset-admin-password
 */

import bcrypt from 'bcrypt';
import { query } from '../src/config/database';
import { logger } from '../src/utils/logger';

const ADMIN_EMAIL = 'admin@leadership.com';

const getResetPassword = (): string => {
  const resetPassword = process.env.SEED_ADMIN_PASSWORD;
  if (!resetPassword || resetPassword.length < 12) {
    throw new Error('[SEED] SEED_ADMIN_PASSWORD env var is required and must be >=12 chars');
  }
  return resetPassword;
};

async function reset() {
  try {
    const resetPassword = getResetPassword();
    const hash = await bcrypt.hash(resetPassword, 10);
    const res = await query(
      `UPDATE users
       SET password_hash = $1, login_attempts = 0, locked_until = NULL
       WHERE email = $2
       RETURNING id, email`,
      [hash, ADMIN_EMAIL]
    );

    if (res.rowCount && res.rowCount > 0) {
      logger.info(`Password reset and account unlocked for: ${ADMIN_EMAIL}`);
    } else {
      logger.warn(`No user found with email: ${ADMIN_EMAIL}. Run: npm run seed`);
    }

    process.exit(0);
  } catch (err) {
    logger.error('Reset failed', err);
    process.exit(1);
  }
}

reset();


==================================================
FILE: _ops/unlock-admin.ts
==================================================

/**
 * Unlock admin user after too many failed login attempts.
 * Run: npm run unlock-admin
 */

import { query } from '../src/config/database';
import { logger } from '../src/utils/logger';

const ADMIN_EMAIL = 'admin@leadership.com';

async function unlock() {
  try {
    const res = await query(
      `UPDATE users SET login_attempts = 0, locked_until = NULL WHERE email = $1 RETURNING id, email`,
      [ADMIN_EMAIL]
    );
    if (res.rowCount && res.rowCount > 0) {
      logger.info(`✅ Admin unlocked: ${ADMIN_EMAIL}`);
    } else {
      logger.warn(`No user found with email: ${ADMIN_EMAIL}`);
    }
    process.exit(0);
  } catch (err) {
    logger.error('Unlock failed', err);
    process.exit(1);
  }
}

unlock();


==================================================
FILE: migrations/001_initial_schema.sql
==================================================

-- Users Table
CREATE TABLE IF NOT EXISTS users (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    email VARCHAR(255) NOT NULL UNIQUE,
    password_hash VARCHAR(255) NOT NULL,
    full_name VARCHAR(255) NOT NULL,
    role VARCHAR(50) DEFAULT 'admin',
    is_active BOOLEAN DEFAULT true,
    last_login_at TIMESTAMP,
    login_attempts INTEGER DEFAULT 0,
    locked_until TIMESTAMP,
    reset_token VARCHAR(255),
    reset_token_expires_at TIMESTAMP,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX IF NOT EXISTS idx_users_email ON users(email);
CREATE INDEX IF NOT EXISTS idx_users_role ON users(role);

-- Admin Sessions Table
CREATE TABLE IF NOT EXISTS admin_sessions (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id UUID NOT NULL REFERENCES users(id) ON DELETE CASCADE,
    token VARCHAR(500) NOT NULL UNIQUE,
    ip_address INET,
    user_agent TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    expires_at TIMESTAMP NOT NULL,
    last_activity_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX IF NOT EXISTS idx_sessions_token ON admin_sessions(token);
CREATE INDEX IF NOT EXISTS idx_sessions_user ON admin_sessions(user_id);
CREATE INDEX IF NOT EXISTS idx_sessions_expires ON admin_sessions(expires_at);

-- Assessments Table
CREATE TABLE IF NOT EXISTS assessments (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    version INTEGER NOT NULL,
    title VARCHAR(255) NOT NULL,
    description TEXT,
    is_active BOOLEAN DEFAULT true,
    is_published BOOLEAN DEFAULT false,
    published_at TIMESTAMP,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    created_by UUID REFERENCES users(id),
    estimated_duration_minutes INTEGER,
    instructions TEXT
);

-- Dimensions Table
CREATE TABLE IF NOT EXISTS dimensions (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    assessment_id UUID NOT NULL REFERENCES assessments(id) ON DELETE CASCADE,
    dimension_key VARCHAR(100) NOT NULL,
    title VARCHAR(255) NOT NULL,
    description TEXT,
    category VARCHAR(100),
    order_index INTEGER NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    CONSTRAINT unique_dimension_per_assessment UNIQUE (assessment_id, dimension_key)
);

-- Topics Table
CREATE TABLE IF NOT EXISTS topics (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    dimension_id UUID NOT NULL REFERENCES dimensions(id) ON DELETE CASCADE,
    topic_key VARCHAR(100) NOT NULL,
    label VARCHAR(255) NOT NULL,
    prompt TEXT NOT NULL,
    order_index INTEGER NOT NULL,
    help_text TEXT,
    example TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    CONSTRAINT unique_topic_per_dimension UNIQUE (dimension_id, topic_key)
);

-- Participants Table
CREATE TABLE IF NOT EXISTS participants (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    email VARCHAR(255) NOT NULL UNIQUE,
    full_name VARCHAR(255) NOT NULL,
    company_name VARCHAR(255),
    job_title VARCHAR(255),
    phone VARCHAR(50),
    industry VARCHAR(100),
    company_size VARCHAR(50),
    country VARCHAR(100),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    consent_given BOOLEAN DEFAULT false,
    consent_date TIMESTAMP
);

CREATE INDEX IF NOT EXISTS idx_participants_email ON participants(email);
CREATE INDEX IF NOT EXISTS idx_participants_company ON participants(company_name);

-- Assessment Responses Table
CREATE TABLE IF NOT EXISTS assessment_responses (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    assessment_id UUID NOT NULL REFERENCES assessments(id),
    participant_id UUID NOT NULL REFERENCES participants(id),
    status VARCHAR(50) DEFAULT 'in_progress',
    started_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    completed_at TIMESTAMP,
    last_updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    total_questions INTEGER,
    answered_questions INTEGER DEFAULT 0,
    progress_percentage DECIMAL(5,2) DEFAULT 0.00,
    session_token VARCHAR(255) UNIQUE,
    ip_address INET,
    user_agent TEXT,
    overall_score DECIMAL(5,2),
    overall_gap DECIMAL(5,2)
);

CREATE INDEX IF NOT EXISTS idx_responses_participant ON assessment_responses(participant_id);
CREATE INDEX IF NOT EXISTS idx_responses_assessment ON assessment_responses(assessment_id);
CREATE INDEX IF NOT EXISTS idx_responses_status ON assessment_responses(status);
CREATE INDEX IF NOT EXISTS idx_responses_completed ON assessment_responses(completed_at);

-- Topic Responses Table
CREATE TABLE IF NOT EXISTS topic_responses (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    response_id UUID NOT NULL REFERENCES assessment_responses(id) ON DELETE CASCADE,
    topic_id UUID NOT NULL REFERENCES topics(id),
    current_rating DECIMAL(5,2) CHECK (current_rating >= 1 AND current_rating <= 5),
    target_rating DECIMAL(5,2) CHECK (target_rating >= 1 AND target_rating <= 5),
    gap DECIMAL(5,2),
    normalized_gap DECIMAL(5,2),
    answered_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    time_spent_seconds INTEGER,
    notes TEXT,
    CONSTRAINT unique_topic_response UNIQUE (response_id, topic_id)
);

CREATE INDEX IF NOT EXISTS idx_topic_responses_response ON topic_responses(response_id);
CREATE INDEX IF NOT EXISTS idx_topic_responses_topic ON topic_responses(topic_id);

-- Computed Priorities Table
CREATE TABLE IF NOT EXISTS computed_priorities (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    response_id UUID NOT NULL REFERENCES assessment_responses(id) ON DELETE CASCADE,
    dimension_id UUID NOT NULL REFERENCES dimensions(id),
    dimension_score DECIMAL(5,2),
    dimension_gap DECIMAL(5,2),
    priority_score DECIMAL(5,2),
    rank_order INTEGER,
    recommendations JSONB,
    computed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    CONSTRAINT unique_dimension_priority UNIQUE (response_id, dimension_id)
);

CREATE INDEX IF NOT EXISTS idx_priorities_response ON computed_priorities(response_id);
CREATE INDEX IF NOT EXISTS idx_priorities_score ON computed_priorities(priority_score DESC);

-- Recommendations Table
CREATE TABLE IF NOT EXISTS recommendations (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    dimension_id UUID NOT NULL REFERENCES dimensions(id),
    title VARCHAR(255) NOT NULL,
    description TEXT NOT NULL,
    action_items JSONB,
    resources JSONB,
    min_gap DECIMAL(5,2),
    max_gap DECIMAL(5,2),
    priority_level VARCHAR(50),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX IF NOT EXISTS idx_recommendations_dimension ON recommendations(dimension_id);

-- System Settings Table
CREATE TABLE IF NOT EXISTS system_settings (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    setting_key VARCHAR(100) NOT NULL UNIQUE,
    setting_value JSONB NOT NULL,
    description TEXT,
    updated_by UUID REFERENCES users(id),
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX IF NOT EXISTS idx_settings_key ON system_settings(setting_key);

-- Audit Logs Table
CREATE TABLE IF NOT EXISTS audit_logs (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    entity_type VARCHAR(100),
    entity_id UUID,
    action VARCHAR(50),
    user_id UUID,
    changes JSONB,
    ip_address INET,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX IF NOT EXISTS idx_audit_entity ON audit_logs(entity_type, entity_id);

-- Admin Notifications Table
CREATE TABLE IF NOT EXISTS admin_notifications (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id UUID REFERENCES users(id),
    type VARCHAR(50),
    title VARCHAR(255) NOT NULL,
    message TEXT NOT NULL,
    link VARCHAR(500),
    is_read BOOLEAN DEFAULT false,
    read_at TIMESTAMP,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX IF NOT EXISTS idx_notifications_user ON admin_notifications(user_id);


==================================================
FILE: migrations/002_fix_rating_columns.sql
==================================================

﻿-- Migration: Normalize topic_responses rating columns safely (non-destructive)
BEGIN;

ALTER TABLE topic_responses
  ADD COLUMN IF NOT EXISTS current_rating NUMERIC,
  ADD COLUMN IF NOT EXISTS target_rating NUMERIC;

ALTER TABLE topic_responses
  ALTER COLUMN current_rating TYPE NUMERIC(5,2) USING current_rating::NUMERIC,
  ALTER COLUMN target_rating TYPE NUMERIC(5,2) USING target_rating::NUMERIC;

ALTER TABLE topic_responses
  ADD COLUMN IF NOT EXISTS time_spent_seconds INTEGER DEFAULT 0,
  ADD COLUMN IF NOT EXISTS created_at TIMESTAMP DEFAULT NOW(),
  ADD COLUMN IF NOT EXISTS updated_at TIMESTAMP DEFAULT NOW();

ALTER TABLE topic_responses DROP CONSTRAINT IF EXISTS topic_responses_current_rating_check;
ALTER TABLE topic_responses DROP CONSTRAINT IF EXISTS topic_responses_target_rating_check;

ALTER TABLE topic_responses
  ADD CONSTRAINT topic_responses_current_rating_check CHECK (current_rating >= 1 AND current_rating <= 5),
  ADD CONSTRAINT topic_responses_target_rating_check CHECK (target_rating >= 1 AND target_rating <= 5);

CREATE INDEX IF NOT EXISTS idx_topic_responses_response ON topic_responses(response_id);
CREATE INDEX IF NOT EXISTS idx_topic_responses_topic ON topic_responses(topic_id);

COMMIT;


==================================================
FILE: migrations/003_topic_levels_and_recommendations.sql
==================================================

BEGIN;

ALTER TABLE topics
  ADD COLUMN IF NOT EXISTS level_1_label TEXT,
  ADD COLUMN IF NOT EXISTS level_2_label TEXT,
  ADD COLUMN IF NOT EXISTS level_3_label TEXT,
  ADD COLUMN IF NOT EXISTS level_4_label TEXT,
  ADD COLUMN IF NOT EXISTS level_5_label TEXT;

CREATE TABLE IF NOT EXISTS topic_recommendations (
  id             UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  topic_id       UUID NOT NULL REFERENCES topics(id) ON DELETE CASCADE,
  score_min      DECIMAL(3,1),
  score_max      DECIMAL(3,1),
  target_min     DECIMAL(3,1),
  target_max     DECIMAL(3,1),
  gap_min        DECIMAL(3,1),
  gap_max        DECIMAL(3,1),
  title          VARCHAR(255) NOT NULL,
  description    TEXT,
  why            TEXT,
  what           TEXT,
  how            TEXT,
  action_items   JSONB        NOT NULL DEFAULT '[]'::jsonb,
  category       VARCHAR(50)  NOT NULL DEFAULT 'Project'
                   CHECK (category IN ('Quick Win','Project','Big Bet')),
  priority       INTEGER      NOT NULL DEFAULT 50
                   CHECK (priority BETWEEN 0 AND 100),
  tags           TEXT[]       NOT NULL DEFAULT '{}',
  is_active      BOOLEAN      NOT NULL DEFAULT true,
  order_index    INTEGER      NOT NULL DEFAULT 0,
  created_at     TIMESTAMPTZ  NOT NULL DEFAULT NOW(),
  updated_at     TIMESTAMPTZ  NOT NULL DEFAULT NOW()
);

CREATE INDEX IF NOT EXISTS idx_topic_recs_topic_id
  ON topic_recommendations(topic_id);

CREATE INDEX IF NOT EXISTS idx_topic_recs_active
  ON topic_recommendations(topic_id, is_active)
  WHERE is_active = true;

DO $$ BEGIN
  IF NOT EXISTS (
    SELECT 1 FROM pg_proc WHERE proname = 'update_updated_at_column'
  ) THEN
    CREATE FUNCTION update_updated_at_column()
    RETURNS TRIGGER AS $func$
    BEGIN
      NEW.updated_at = NOW();
      RETURN NEW;
    END;
    $func$ LANGUAGE plpgsql;
  END IF;
END $$;

DROP TRIGGER IF EXISTS trg_topic_recommendations_updated_at ON topic_recommendations;
CREATE TRIGGER trg_topic_recommendations_updated_at
  BEFORE UPDATE ON topic_recommendations
  FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();

COMMIT;

==================================================
FILE: migrations/004_backfill_level_anchors.sql
==================================================

-- Backfill level labels from questions.json data
-- Run AFTER migration 003
-- This is a one-time data migration, safe to re-run

UPDATE topics SET
  level_1_label = 'Data is siloed, inconsistent, and often manually managed with no quality checks.',
  level_2_label = 'Some clean data exists but accessibility and cataloguing are limited.',
  level_3_label = 'Key data assets are governed, catalogued, and accessible to project teams.',
  level_4_label = 'Automated quality checks and pipelines ensure reliable data across the organization.',
  level_5_label = 'Data is democratized, real-time, and continuously optimized for AI workloads.'
WHERE topic_key = 'data-quality-availability';

UPDATE topics SET
  level_1_label = 'Legacy on-premise systems with manual ETL and no scalability.',
  level_2_label = 'Basic cloud or hybrid setup; limited automation in data pipelines.',
  level_3_label = 'Scalable cloud data platform with automated ingestion and processing.',
  level_4_label = 'Modern lakehouse architecture with streaming and batch capabilities.',
  level_5_label = 'Fully automated, event-driven data mesh with self-serve access.'
WHERE topic_key = 'data-infrastructure';

UPDATE topics SET
  level_1_label = 'No formal data governance; ownership and policies are undefined.',
  level_2_label = 'Informal ownership exists for some datasets; policies are ad-hoc.',
  level_3_label = 'Formal governance council with defined owners and documented policies.',
  level_4_label = 'Governance is embedded in workflows with automated compliance checks.',
  level_5_label = 'Organization-wide data governance drives strategic decision-making.'
WHERE topic_key = 'data-governance-framework';

UPDATE topics SET
  level_1_label = 'Reporting is manual, spreadsheet-based, and backward-looking only.',
  level_2_label = 'Basic BI dashboards provide descriptive analytics for some functions.',
  level_3_label = 'Diagnostic and trend analytics are used across key business areas.',
  level_4_label = 'Predictive models are operationalized and inform business decisions.',
  level_5_label = 'Prescriptive analytics and AI-driven insights are embedded enterprise-wide.'
WHERE topic_key = 'data-analytics-capabilities';

UPDATE topics SET
  level_1_label = 'No dedicated AI/ML talent; reliance on external vendors for all efforts.',
  level_2_label = 'A few individuals have AI skills but no formal team or career path.',
  level_3_label = 'A dedicated data science team delivers projects with defined methodologies.',
  level_4_label = 'Cross-functional AI teams with MLOps, research, and engineering capabilities.',
  level_5_label = 'World-class AI talent embedded across the organization with continuous upskilling.'
WHERE topic_key = 'ai-ml-expertise';

UPDATE topics SET
  level_1_label = 'No AI-specific tools or platforms; experiments use ad-hoc scripts.',
  level_2_label = 'Basic ML frameworks in use; no standardized platform or MLOps tooling.',
  level_3_label = 'Standardized AI platform with experiment tracking and model registry.',
  level_4_label = 'End-to-end MLOps pipeline with CI/CD, monitoring, and automated retraining.',
  level_5_label = 'Cutting-edge platform with AutoML, feature stores, and real-time serving.'
WHERE topic_key = 'technology-stack';

UPDATE topics SET
  level_1_label = 'No training programs; employees learn informally on their own time.',
  level_2_label = 'Occasional workshops or external courses available but not tracked.',
  level_3_label = 'Structured learning paths and certifications for key roles.',
  level_4_label = 'Comprehensive AI academy with role-based curricula and hands-on labs.',
  level_5_label = 'Continuous learning culture with AI literacy programs for all employees.'
WHERE topic_key = 'training-opportunities';

UPDATE topics SET
  level_1_label = 'No process for evaluating or adopting AI innovations.',
  level_2_label = 'Innovation is sporadic; pilots rarely move beyond proof-of-concept.',
  level_3_label = 'Structured innovation process with stage-gate evaluation of AI pilots.',
  level_4_label = 'Innovation lab with fast prototyping and clear paths to production.',
  level_5_label = 'Innovation is embedded in culture with systematic horizon scanning.'
WHERE topic_key = 'innovation-adoption';

UPDATE topics SET
  level_1_label = 'AI initiatives are disconnected from business strategy and pursued in isolation.',
  level_2_label = 'Some AI projects align with strategic objectives, but efforts remain fragmented.',
  level_3_label = 'AI initiatives are systematically aligned with business goals and key functions.',
  level_4_label = 'AI strategy is integrated with overall business strategy and drives measurable outcomes.',
  level_5_label = 'AI is a core enabler of business strategy and creates competitive advantage.'
WHERE topic_key = 'ai-strategy-alignment';

UPDATE topics SET
  level_1_label = 'No executive ownership; AI efforts are ad-hoc and unsupported.',
  level_2_label = 'A few leaders sponsor isolated initiatives without consistent governance.',
  level_3_label = 'Clear executive sponsorship exists with defined goals and accountability.',
  level_4_label = 'Leadership actively steers AI portfolio with KPIs and cross-functional alignment.',
  level_5_label = 'Executives champion AI as a strategic capability and continuously invest to scale impact.'
WHERE topic_key = 'leadership-sponsorship';

UPDATE topics SET
  level_1_label = 'No change management; AI projects create confusion and resistance.',
  level_2_label = 'Basic communication about AI projects but no structured change process.',
  level_3_label = 'Change management plans accompany major AI initiatives with stakeholder engagement.',
  level_4_label = 'Dedicated change team manages adoption, training, and impact assessment.',
  level_5_label = 'Change agility is a core competency; organization adapts fluidly to AI-driven transformation.'
WHERE topic_key = 'change-management';

UPDATE topics SET
  level_1_label = 'No AI roadmap; initiatives are reactive and uncoordinated.',
  level_2_label = 'Informal list of AI ideas without prioritization or resource planning.',
  level_3_label = 'Documented AI roadmap with prioritized initiatives and milestones.',
  level_4_label = 'Multi-year AI investment plan with portfolio management and governance.',
  level_5_label = 'Dynamic AI roadmap continuously updated based on outcomes and market signals.'
WHERE topic_key = 'roadmap-planning';

UPDATE topics SET
  level_1_label = 'No AI governance; decisions are made ad-hoc without oversight.',
  level_2_label = 'Basic guidelines exist but are not enforced or widely known.',
  level_3_label = 'Formal governance framework with clear roles, policies, and review boards.',
  level_4_label = 'Governance is embedded in AI lifecycle with automated compliance gates.',
  level_5_label = 'Best-in-class governance that balances innovation speed with risk management.'
WHERE topic_key = 'governance-framework';

UPDATE topics SET
  level_1_label = 'No awareness of AI ethics; bias and fairness are not considered.',
  level_2_label = 'Informal awareness but no documented guidelines or review process.',
  level_3_label = 'Published AI ethics principles with bias testing for high-risk models.',
  level_4_label = 'Ethics review board evaluates models pre-deployment with fairness metrics.',
  level_5_label = 'Ethics-by-design approach with continuous monitoring and external auditing.'
WHERE topic_key = 'ethical-issues';

UPDATE topics SET
  level_1_label = 'AI risks are not identified, tracked, or mitigated.',
  level_2_label = 'Some risks are recognized but management is reactive and inconsistent.',
  level_3_label = 'Risk assessment is part of AI project planning with defined mitigation steps.',
  level_4_label = 'Enterprise AI risk register with quantified impact and automated monitoring.',
  level_5_label = 'Proactive risk intelligence informs AI strategy and investment decisions.'
WHERE topic_key = 'risk-management';

UPDATE topics SET
  level_1_label = 'No awareness of AI-related regulations or industry standards.',
  level_2_label = 'Basic awareness but no formal compliance program or documentation.',
  level_3_label = 'Compliance requirements are documented and tracked for AI systems.',
  level_4_label = 'Automated compliance checks integrated into the AI development lifecycle.',
  level_5_label = 'Proactive regulatory engagement with industry-leading standards adoption.'
WHERE topic_key = 'compliance-standards';

UPDATE topics SET
  level_1_label = 'No systematic process for identifying AI opportunities.',
  level_2_label = 'Ad-hoc ideas collected but no structured evaluation or prioritization.',
  level_3_label = 'Use case pipeline with business impact scoring and feasibility analysis.',
  level_4_label = 'Portfolio approach with strategic alignment, ROI estimation, and stage-gating.',
  level_5_label = 'AI opportunity radar with continuous scanning and rapid validation.'
WHERE topic_key = 'use-cases';

UPDATE topics SET
  level_1_label = 'AI ROI is never measured or discussed.',
  level_2_label = 'Anecdotal success stories but no formal metrics or tracking.',
  level_3_label = 'KPIs defined for AI projects with periodic ROI reviews.',
  level_4_label = 'Standardized value measurement framework across all AI initiatives.',
  level_5_label = 'Real-time value dashboards with attribution modeling and impact analytics.'
WHERE topic_key = 'roi-measurement';

UPDATE topics SET
  level_1_label = 'Business stakeholders are uninvolved; AI is purely a technical exercise.',
  level_2_label = 'Stakeholders are consulted occasionally but not actively engaged.',
  level_3_label = 'Business owners co-design AI solutions and validate outcomes.',
  level_4_label = 'Cross-functional teams with embedded business and AI expertise.',
  level_5_label = 'Stakeholders champion AI adoption and drive demand for new capabilities.'
WHERE topic_key = 'stakeholder-engagement';

UPDATE topics SET
  level_1_label = 'No AI solutions in production; all efforts are experimental.',
  level_2_label = 'One or two pilots running but none scaled to production impact.',
  level_3_label = 'Several AI solutions in production delivering measurable business value.',
  level_4_label = 'AI solutions are core to key business processes with continuous improvement.',
  level_5_label = 'AI-native products and services generate significant competitive advantage.'
WHERE topic_key = 'ai-solutions';

UPDATE topics SET
  level_1_label = 'No dedicated AI/ML platform; teams use disparate tools and scripts.',
  level_2_label = 'Basic ML framework adoption but no shared platform or standards.',
  level_3_label = 'Centralized ML platform with experiment tracking and shared resources.',
  level_4_label = 'Enterprise-grade platform with AutoML, feature stores, and model serving.',
  level_5_label = 'Best-in-class AI platform with real-time inference and continuous learning.'
WHERE topic_key = 'ai-ml-platforms';

UPDATE topics SET
  level_1_label = 'Infrastructure is legacy, on-premise, and unsuited for AI workloads.',
  level_2_label = 'Some cloud usage but infrastructure lacks scalability for AI.',
  level_3_label = 'Hybrid or cloud setup supports basic model training and deployment.',
  level_4_label = 'Scalable MLOps infrastructure is in place for efficient model lifecycle.',
  level_5_label = 'Fully automated, serverless, and optimized for AI at scale.'
WHERE topic_key = 'cloud-infrastructure';

UPDATE topics SET
  level_1_label = 'AI outputs are copy-pasted manually; no system integration.',
  level_2_label = 'Basic API integration for a few use cases; mostly point-to-point.',
  level_3_label = 'Standard integration patterns with APIs and event-driven architecture.',
  level_4_label = 'AI services are composable microservices integrated into core platforms.',
  level_5_label = 'Seamless AI-native integration across the entire digital ecosystem.'
WHERE topic_key = 'integration-capabilities';

UPDATE topics SET
  level_1_label = 'No AI-specific security measures; models and data are unprotected.',
  level_2_label = 'Basic access controls but no adversarial testing or privacy measures.',
  level_3_label = 'Security reviews for AI systems with data privacy impact assessments.',
  level_4_label = 'Automated vulnerability scanning for models with differential privacy.',
  level_5_label = 'Zero-trust AI security with federated learning and privacy-preserving AI.'
WHERE topic_key = 'security-privacy';


==================================================
FILE: migrations/005_performance_indexes.sql
==================================================

BEGIN;

ALTER TABLE topics
  ADD COLUMN IF NOT EXISTS is_active BOOLEAN NOT NULL DEFAULT true;

CREATE INDEX IF NOT EXISTS idx_topic_recs_topic_active
ON topic_recommendations(topic_id)
WHERE is_active = true;

CREATE INDEX IF NOT EXISTS idx_topic_recs_priority
ON topic_recommendations(topic_id, priority DESC)
WHERE is_active = true;

CREATE INDEX IF NOT EXISTS idx_computed_priorities_response
ON computed_priorities(response_id, priority_score DESC);

CREATE INDEX IF NOT EXISTS idx_topics_dimension_active
ON topics(dimension_id, order_index)
WHERE is_active = true;

COMMIT;


==================================================
FILE: migrations/006_single_published_assessment.sql
==================================================

BEGIN;

WITH ranked_published AS (
  SELECT
    id,
    ROW_NUMBER() OVER (
      ORDER BY published_at DESC NULLS LAST, updated_at DESC NULLS LAST, created_at DESC, id DESC
    ) AS rn
  FROM assessments
  WHERE is_published = true
),
to_unpublish AS (
  SELECT id
  FROM ranked_published
  WHERE rn > 1
)
UPDATE assessments a
SET
  is_published = false,
  is_active = false
FROM to_unpublish u
WHERE a.id = u.id;

CREATE UNIQUE INDEX IF NOT EXISTS ux_assessments_single_published
ON assessments ((1))
WHERE is_published = true;

COMMIT;


==================================================
FILE: migrations/007_session_token.sql
==================================================

BEGIN;

CREATE EXTENSION IF NOT EXISTS pgcrypto;

ALTER TABLE assessment_responses
  ADD COLUMN IF NOT EXISTS session_token VARCHAR(255);

UPDATE assessment_responses
SET session_token = encode(gen_random_bytes(32), 'hex')
WHERE session_token IS NULL;

CREATE UNIQUE INDEX IF NOT EXISTS idx_assessment_responses_session_token_unique
ON assessment_responses(session_token)
WHERE session_token IS NOT NULL;

COMMIT;


==================================================
FILE: migrations/008_participant_token.sql
==================================================

BEGIN;

CREATE EXTENSION IF NOT EXISTS pgcrypto;

ALTER TABLE participants
  ADD COLUMN IF NOT EXISTS participant_token VARCHAR(255);

UPDATE participants
SET participant_token = encode(gen_random_bytes(32), 'hex')
WHERE participant_token IS NULL;

CREATE UNIQUE INDEX IF NOT EXISTS idx_participants_token_unique
ON participants(participant_token)
WHERE participant_token IS NOT NULL;

COMMIT;


==================================================
FILE: migrations/1708035600000_add_recommendation_and_narrative_tables.ts
==================================================

import { MigrationInterface, QueryRunner } from 'typeorm';

export class AddRecommendationAndNarrativeTables1708035600000 implements MigrationInterface {
  public async up(queryRunner: QueryRunner): Promise<void> {
    // 1. Create recommendation_rules table
    await queryRunner.query(`
      CREATE TABLE recommendation_rules (
        id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
        rule_key VARCHAR(100) UNIQUE NOT NULL,
        dimension_key VARCHAR(50) NOT NULL,
        title VARCHAR(255) NOT NULL,
        description TEXT NOT NULL,
        conditions JSONB NOT NULL,
        why TEXT,
        what TEXT,
        how TEXT,
        priority_score DECIMAL(3,2) DEFAULT 0.5,
        tags JSONB,
        impact_level VARCHAR(20),
        effort_level VARCHAR(20),
        timeframe VARCHAR(50),
        action_items JSONB,
        resources JSONB,
        kpis JSONB,
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
      )
    `);

    await queryRunner.query(`
      CREATE INDEX idx_recommendation_rules_dimension ON recommendation_rules(dimension_key)
    `);

    await queryRunner.query(`
      CREATE INDEX idx_recommendation_rules_tags ON recommendation_rules USING GIN(tags)
    `);

    // 2. Create recommendation_meta table
    await queryRunner.query(`
      CREATE TABLE recommendation_meta (
        id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
        meta_key VARCHAR(100) UNIQUE NOT NULL,
        meta_value JSONB NOT NULL,
        description TEXT,
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
      )
    `);

    // 3. Create narrative_templates table
    await queryRunner.query(`
      CREATE TABLE narrative_templates (
        id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
        template_key VARCHAR(100) UNIQUE NOT NULL,
        template_type VARCHAR(50) NOT NULL,
        category VARCHAR(50),
        template TEXT NOT NULL,
        conditions JSONB,
        priority INTEGER DEFAULT 100,
        tags JSONB,
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
      )
    `);

    await queryRunner.query(`
      CREATE INDEX idx_narrative_templates_type ON narrative_templates(template_type)
    `);

    await queryRunner.query(`
      CREATE INDEX idx_narrative_templates_key ON narrative_templates(template_key)
    `);

    // 4. Create narrative_theme_map table
    await queryRunner.query(`
      CREATE TABLE narrative_theme_map (
        id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
        theme_key VARCHAR(100) UNIQUE NOT NULL,
        theme_label VARCHAR(255) NOT NULL,
        description TEXT,
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
      )
    `);

    await queryRunner.query(`
      CREATE INDEX idx_narrative_theme_map_key ON narrative_theme_map(theme_key)
    `);

    // 5. Create narrative_config table
    await queryRunner.query(`
      CREATE TABLE narrative_config (
        id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
        config_key VARCHAR(100) UNIQUE NOT NULL,
        config_value JSONB NOT NULL,
        description TEXT,
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
      )
    `);
  }

  public async down(queryRunner: QueryRunner): Promise<void> {
    await queryRunner.query(`DROP TABLE IF EXISTS narrative_config CASCADE`);
    await queryRunner.query(`DROP TABLE IF EXISTS narrative_theme_map CASCADE`);
    await queryRunner.query(`DROP TABLE IF EXISTS narrative_templates CASCADE`);
    await queryRunner.query(`DROP TABLE IF EXISTS recommendation_meta CASCADE`);
    await queryRunner.query(`DROP TABLE IF EXISTS recommendation_rules CASCADE`);
  }
}


==================================================
FILE: migrations/run.js
==================================================

"use strict";
/**
 * File: migrations/run.ts
 * Purpose: Executes SQL and TypeScript migrations exactly once, with tracking.
 */
var __importDefault = (this && this.__importDefault) || function (mod) {
    return (mod && mod.__esModule) ? mod : { "default": mod };
};
Object.defineProperty(exports, "__esModule", { value: true });
exports.applyMigrations = void 0;
const fs_1 = __importDefault(require("fs"));
const path_1 = __importDefault(require("path"));
const database_1 = require("../src/config/database");
const logger_1 = require("../src/utils/logger");
const MIGRATIONS_TABLE_SQL = `
CREATE TABLE IF NOT EXISTS schema_migrations (
  filename TEXT PRIMARY KEY,
  applied_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);
`;
const isMigrationFile = (filename) => {
    if (filename === 'run.ts') {
        return false;
    }
    return /^\d+.*\.(sql|ts)$/i.test(filename);
};
const listMigrationFiles = (migrationsDir) => {
    return fs_1.default
        .readdirSync(migrationsDir)
        .filter(isMigrationFile)
        .sort((a, b) => a.localeCompare(b, undefined, { numeric: true }));
};
const normalizeSqlForTransaction = (sql) => {
    const lines = sql.split(/\r?\n/);
    const filtered = lines.filter((line) => {
        const trimmed = line.trim().toUpperCase();
        return trimmed !== 'BEGIN;' && trimmed !== 'COMMIT;';
    });
    return filtered.join('\n');
};
const resolveMigrationConstructor = (moduleExports) => {
    for (const exportedValue of Object.values(moduleExports)) {
        if (typeof exportedValue !== 'function') {
            continue;
        }
        const candidate = exportedValue;
        const hasUpMethod = typeof candidate.prototype === 'object' &&
            candidate.prototype !== null &&
            typeof candidate.prototype.up === 'function';
        if (hasUpMethod) {
            return candidate;
        }
    }
    throw new Error('No migration class export with an up() method was found');
};
const runTypeScriptMigration = async (migrationsDir, filename, queryRunner) => {
    const migrationPath = path_1.default.join(migrationsDir, filename);
    // eslint-disable-next-line @typescript-eslint/no-var-requires
    const migrationModule = require(migrationPath);
    const MigrationClass = resolveMigrationConstructor(migrationModule);
    const migrationInstance = new MigrationClass();
    await migrationInstance.up(queryRunner);
};
const applyMigrations = async (migrationsDir = __dirname) => {
    console.warn('[MIGRATE] Ensure a DB backup exists before running migrations in production.');
    const client = await database_1.pool.connect();
    try {
        logger_1.logger.info('Starting database migrations...');
        await client.query(MIGRATIONS_TABLE_SQL);
        const appliedResult = await client.query('SELECT filename FROM schema_migrations');
        const appliedFiles = new Set(appliedResult.rows.map((row) => row.filename));
        const migrationFiles = listMigrationFiles(migrationsDir);
        for (const filename of migrationFiles) {
            if (appliedFiles.has(filename)) {
                logger_1.logger.info(`Skipping already applied migration: ${filename}`);
                continue;
            }
            await client.query('BEGIN');
            try {
                const migrationPath = path_1.default.join(migrationsDir, filename);
                if (filename.toLowerCase().endsWith('.sql')) {
                    const sql = fs_1.default.readFileSync(migrationPath, 'utf-8');
                    const normalizedSql = normalizeSqlForTransaction(sql);
                    await client.query(normalizedSql);
                }
                else {
                    await runTypeScriptMigration(migrationsDir, filename, {
                        query: async (queryText, parameters) => client.query(queryText, parameters),
                    });
                }
                await client.query('INSERT INTO schema_migrations (filename) VALUES ($1)', [filename]);
                await client.query('COMMIT');
                logger_1.logger.info(`Applied migration: ${filename}`);
            }
            catch (error) {
                await client.query('ROLLBACK');
                throw error;
            }
        }
        logger_1.logger.info('Migrations completed successfully');
    }
    finally {
        client.release();
    }
};
exports.applyMigrations = applyMigrations;
async function runMigrations() {
    try {
        await (0, exports.applyMigrations)();
        process.exit(0);
    }
    catch (error) {
        logger_1.logger.error('Migration failed:', error);
        process.exit(1);
    }
}
if (require.main === module) {
    runMigrations();
}


==================================================
FILE: migrations/run.ts
==================================================

﻿/**
 * File: migrations/run.ts
 * Purpose: Executes SQL and TypeScript migrations exactly once, with tracking.
 */

import fs from 'fs';
import path from 'path';
import { pool } from '../src/config/database';
import { logger } from '../src/utils/logger';

const MIGRATIONS_TABLE_SQL = `
CREATE TABLE IF NOT EXISTS schema_migrations (
  filename TEXT PRIMARY KEY,
  applied_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);
`;

interface MinimalQueryRunner {
  query: (queryText: string, parameters?: unknown[]) => Promise<unknown>;
}

interface MigrationInstance {
  up: (queryRunner: MinimalQueryRunner) => Promise<void>;
}

interface MigrationConstructor {
  new (): MigrationInstance;
}

const isMigrationFile = (filename: string): boolean => {
  if (filename === 'run.ts') {
    return false;
  }

  return /^\d+.*\.(sql|ts)$/i.test(filename);
};

const listMigrationFiles = (migrationsDir: string): string[] => {
  return fs
    .readdirSync(migrationsDir)
    .filter(isMigrationFile)
    .sort((a, b) => a.localeCompare(b, undefined, { numeric: true }));
};

const normalizeSqlForTransaction = (sql: string): string => {
  const lines = sql.split(/\r?\n/);
  const filtered = lines.filter((line) => {
    const trimmed = line.trim().toUpperCase();
    return trimmed !== 'BEGIN;' && trimmed !== 'COMMIT;';
  });
  return filtered.join('\n');
};

const resolveMigrationConstructor = (moduleExports: Record<string, unknown>): MigrationConstructor => {
  for (const exportedValue of Object.values(moduleExports)) {
    if (typeof exportedValue !== 'function') {
      continue;
    }

    const candidate = exportedValue as unknown as MigrationConstructor;
    const hasUpMethod =
      typeof candidate.prototype === 'object' &&
      candidate.prototype !== null &&
      typeof (candidate.prototype as { up?: unknown }).up === 'function';

    if (hasUpMethod) {
      return candidate;
    }
  }

  throw new Error('No migration class export with an up() method was found');
};

const runTypeScriptMigration = async (
  migrationsDir: string,
  filename: string,
  queryRunner: MinimalQueryRunner
): Promise<void> => {
  const migrationPath = path.join(migrationsDir, filename);
  // eslint-disable-next-line @typescript-eslint/no-var-requires
  const migrationModule = require(migrationPath) as Record<string, unknown>;
  const MigrationClass = resolveMigrationConstructor(migrationModule);
  const migrationInstance = new MigrationClass();
  await migrationInstance.up(queryRunner);
};

export const applyMigrations = async (migrationsDir: string = __dirname): Promise<void> => {
  console.warn('[MIGRATE] Ensure a DB backup exists before running migrations in production.');
  const client = await pool.connect();

  try {
    logger.info('Starting database migrations...');
    await client.query(MIGRATIONS_TABLE_SQL);

    const appliedResult = await client.query<{ filename: string }>('SELECT filename FROM schema_migrations');
    const appliedFiles = new Set(appliedResult.rows.map((row) => row.filename));
    const migrationFiles = listMigrationFiles(migrationsDir);

    for (const filename of migrationFiles) {
      if (appliedFiles.has(filename)) {
        logger.info(`Skipping already applied migration: ${filename}`);
        continue;
      }

      await client.query('BEGIN');
      try {
        const migrationPath = path.join(migrationsDir, filename);

        if (filename.toLowerCase().endsWith('.sql')) {
          const sql = fs.readFileSync(migrationPath, 'utf-8');
          const normalizedSql = normalizeSqlForTransaction(sql);
          await client.query(normalizedSql);
        } else {
          await runTypeScriptMigration(migrationsDir, filename, {
            query: async (queryText: string, parameters?: unknown[]) => client.query(queryText, parameters),
          });
        }

        await client.query('INSERT INTO schema_migrations (filename) VALUES ($1)', [filename]);
        await client.query('COMMIT');
        logger.info(`Applied migration: ${filename}`);
      } catch (error) {
        await client.query('ROLLBACK');
        throw error;
      }
    }

    logger.info('Migrations completed successfully');
  } finally {
    client.release();
  }
};

async function runMigrations() {
  try {
    await applyMigrations();
    process.exit(0);
  } catch (error) {
    logger.error('Migration failed:', error);
    process.exit(1);
  }
}

if (require.main === module) {
  runMigrations();
}


==================================================
FILE: scripts/e2e-flow-test.ts
==================================================

import { pool } from '../src/config/database';

const API_BASE = process.env.API_BASE_URL || 'http://localhost:3001/api';

const values = [1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0];

const section = (name: string): void => {
  console.log('\n============================================================');
  console.log(name);
  console.log('============================================================');
};

const waitForServer = async (): Promise<void> => {
  for (let i = 0; i < 40; i += 1) {
    try {
      const response = await fetch('http://localhost:3001/');
      if (response.ok) return;
    } catch {
      // wait and retry
    }
    await new Promise((resolve) => setTimeout(resolve, 250));
  }
  throw new Error('Server not ready at http://localhost:3001/');
};

const fetchJson = async <T>(url: string, init?: RequestInit): Promise<{ ok: boolean; status: number; json: T }> => {
  const response = await fetch(url, init);
  const json = (await response.json()) as T;
  return { ok: response.ok, status: response.status, json };
};

const pickCurrent = (idx: number): number => values[idx % values.length];
const pickTarget = (current: number, idx: number): number => {
  const bump = values[(idx + 3) % values.length];
  return Math.max(current, bump);
};

const main = async (): Promise<void> => {
  section('E2E Flow Test');
  await waitForServer();

  const structureResp = await fetchJson<{
    assessment: { id: string; title: string };
    dimensions: Array<{ dimensionKey: string; title: string; topics: Array<{ id: string; label: string }> }>;
  }>(`${API_BASE}/public/assessments/active/structure`);

  if (!structureResp.ok) {
    throw new Error(`Failed to load structure: HTTP ${structureResp.status}`);
  }

  const structure = structureResp.json;
  const topics = structure.dimensions.flatMap((d) =>
    d.topics.map((t) => ({ dimension: d.dimensionKey, ...t }))
  );
  console.log(`Loaded assessment: ${structure.assessment.title}`);
  console.log(`Total topics: ${topics.length}`);

  const participantEmail = `e2e+${Date.now()}@example.com`;
  const participantResp = await fetchJson<{
    success: boolean;
    data?: { participantId: string };
    error?: string;
  }>(`${API_BASE}/public/participants`, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
      email: participantEmail,
      fullName: 'E2E Test User',
      companyName: 'QA Company',
      consentGiven: true,
    }),
  });

  if (!participantResp.ok || !participantResp.json.success || !participantResp.json.data) {
    throw new Error(`Failed to register participant: ${JSON.stringify(participantResp.json)}`);
  }

  const participantId = participantResp.json.data.participantId;
  console.log(`Participant created: ${participantId}`);

  const startResp = await fetchJson<{
    success: boolean;
    data?: { responseId: string; sessionToken: string };
    error?: string;
  }>(`${API_BASE}/public/responses/start`, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
      assessmentId: structure.assessment.id,
      participantId,
    }),
  });

  if (!startResp.ok || !startResp.json.success || !startResp.json.data) {
    throw new Error(`Failed to start response: ${JSON.stringify(startResp.json)}`);
  }

  const responseId = startResp.json.data.responseId;
  const sessionToken = startResp.json.data.sessionToken;
  console.log(`Response started: ${responseId}`);

  let answered = 0;
  for (let i = 0; i < topics.length; i += 1) {
    const topic = topics[i];
    const current = pickCurrent(i);
    const target = pickTarget(current, i);

    const answerResp = await fetchJson<{
      success: boolean;
      error?: string;
    }>(`${API_BASE}/public/responses/${responseId}/answer`, {
      method: 'PUT',
      headers: {
        'Content-Type': 'application/json',
        'x-session-token': sessionToken,
      },
      body: JSON.stringify({
        topicId: topic.id,
        currentRating: current,
        targetRating: target,
      }),
    });

    if (!answerResp.ok || !answerResp.json.success) {
      throw new Error(
        `Failed to answer topic ${topic.label} (${topic.id}): ${JSON.stringify(answerResp.json)}`
      );
    }
    answered += 1;
  }

  const completeResp = await fetchJson<{ success: boolean; error?: string }>(
    `${API_BASE}/public/responses/${responseId}/complete`,
    {
      method: 'POST',
      headers: { 'x-session-token': sessionToken },
    }
  );

  if (!completeResp.ok || !completeResp.json.success) {
    throw new Error(`Failed to complete response: ${JSON.stringify(completeResp.json)}`);
  }
  console.log('Assessment completed.');

  // The results endpoint is session protected.
  const resultsResp = await fetchJson<{
    success: boolean;
    data?: {
      dimensions: Array<{ title: string; recommendations: unknown[] }>;
      topRecommendations: unknown[];
    };
    error?: string;
  }>(`${API_BASE}/public/responses/${responseId}/results`, {
    headers: { 'x-session-token': sessionToken },
  });

  if (!resultsResp.ok || !resultsResp.json.success || !resultsResp.json.data) {
    throw new Error(`Failed to fetch results: ${JSON.stringify(resultsResp.json)}`);
  }

  const data = resultsResp.json.data;
  const totalRecs = data.dimensions.reduce(
    (sum, d) => sum + (Array.isArray(d.recommendations) ? d.recommendations.length : 0),
    0
  );

  section('E2E Summary');
  console.log(`Topics answered: ${answered}`);
  console.log(`Dimension recommendation entries: ${totalRecs}`);
  console.log(`Top recommendations returned: ${(data.topRecommendations || []).length}`);
  console.log('PASS: End-to-end flow completed and recommendations are present in results payload.');
};

main()
  .catch((error) => {
    console.error('E2E flow test failed:', error);
    process.exit(1);
  })
  .finally(async () => {
    await pool.end();
  });


==================================================
FILE: scripts/post-implementation-verification.ts
==================================================

import fs from 'fs';
import path from 'path';
import { pool } from '../src/config/database';

const API_BASE = process.env.API_BASE_URL || 'http://localhost:3001/api';
const ADMIN_TEST_PASSWORD = process.env.SEED_ADMIN_PASSWORD;

type CookieJar = Map<string, string>;

const addCookiesFromResponse = (response: Response, jar: CookieJar): void => {
  const anyHeaders = response.headers as unknown as { getSetCookie?: () => string[] };
  const setCookies = anyHeaders.getSetCookie?.() || [];
  for (const header of setCookies) {
    const firstPart = header.split(';')[0];
    const eqIndex = firstPart.indexOf('=');
    if (eqIndex <= 0) continue;
    const name = firstPart.slice(0, eqIndex).trim();
    const value = firstPart.slice(eqIndex + 1).trim();
    jar.set(name, value);
  }
};

const cookieHeader = (jar: CookieJar): string =>
  Array.from(jar.entries())
    .map(([k, v]) => `${k}=${v}`)
    .join('; ');

const jsonFetch = async <T>(
  url: string,
  options?: RequestInit,
  jar?: CookieJar
): Promise<{ response: Response; json: T }> => {
  const headers = new Headers(options?.headers || {});
  if (jar && jar.size > 0) {
    headers.set('Cookie', cookieHeader(jar));
  }
  const response = await fetch(url, { ...options, headers });
  if (jar) {
    addCookiesFromResponse(response, jar);
  }
  const json = (await response.json()) as T;
  return { response, json };
};

const section = (name: string): void => {
  console.log('\n============================================================');
  console.log(name);
  console.log('============================================================');
};

const waitForServer = async (): Promise<void> => {
  for (let i = 0; i < 40; i += 1) {
    try {
      const response = await fetch('http://localhost:3001/');
      if (response.ok) return;
    } catch {
      // wait and retry
    }
    await new Promise((resolve) => setTimeout(resolve, 250));
  }
  throw new Error('Server not ready at http://localhost:3001/');
};

const testA = async (): Promise<void> => {
  section('Test A: Public Survey Structure');
  const { response, json } = await jsonFetch<{
    assessment: { id: string; title: string };
    dimensions: Array<{
      title: string;
      topics: Array<{ id: string; label: string; levelAnchors: Array<string | null> | null }>;
    }>;
  }>(`${API_BASE}/public/assessments/active/structure`);

  if (!response.ok) {
    console.log(`FAILED: HTTP ${response.status}`);
    console.log(json);
    return;
  }

  const topics = json.dimensions.flatMap((d) =>
    d.topics.map((t) => ({ dimension: d.title, ...t }))
  );

  console.log(`Assessment: ${json.assessment.title}`);
  console.log(`Total topics: ${topics.length}`);
  console.log('First 2 topics with levelAnchors:');
  topics.slice(0, 2).forEach((topic, idx) => {
    console.log(
      `${idx + 1}. [${topic.dimension}] ${topic.label} -> ${JSON.stringify(topic.levelAnchors)}`
    );
  });

  const invalid = topics.filter(
    (t) =>
      !Array.isArray(t.levelAnchors) ||
      t.levelAnchors.length !== 5 ||
      t.levelAnchors.every((x) => x == null || String(x).trim() === '')
  );

  if (invalid.length === 0) {
    console.log('PASS: Every topic has levelAnchors with 5 items and not all-empty values.');
  } else {
    console.log(`FLAGGED TOPICS (${invalid.length}):`);
    invalid.forEach((t) => console.log(`- ${t.label} (${t.id}) levelAnchors=${JSON.stringify(t.levelAnchors)}`));
  }
};

const loginAdminWithCsrf = async (): Promise<{ jar: CookieJar; csrfToken: string }> => {
  if (!ADMIN_TEST_PASSWORD || ADMIN_TEST_PASSWORD.length < 12) {
    throw new Error('[VERIFY] SEED_ADMIN_PASSWORD env var is required and must be >=12 chars');
  }

  const jar: CookieJar = new Map();
  const csrf = await jsonFetch<{ csrfToken: string }>(`${API_BASE}/csrf-token`, { method: 'GET' }, jar);
  const csrfToken = csrf.json.csrfToken;

  const login = await jsonFetch<{ success: boolean; error?: string }>(
    `${API_BASE}/admin/auth/login`,
    {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ email: 'admin@leadership.com', password: ADMIN_TEST_PASSWORD }),
    },
    jar
  );

  if (!login.response.ok || !login.json.success) {
    throw new Error(`Admin login failed: ${JSON.stringify(login.json)}`);
  }

  return { jar, csrfToken };
};

const testB = async (): Promise<{ testedTopicIds: string[] }> => {
  section('Test B: Topic Recommendations Test Endpoint');
  const topicResult = await pool.query('SELECT id, label FROM topics ORDER BY label LIMIT 3');
  const topics = topicResult.rows as Array<{ id: string; label: string }>;
  const { jar, csrfToken } = await loginAdminWithCsrf();

  for (const topic of topics) {
    const { response, json } = await jsonFetch<{
      matchedRecommendations?: Array<{ title: string; priority: number }>;
      error?: string;
    }>(
      `${API_BASE}/admin/topics/${topic.id}/recommendations/test`,
      {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'x-csrf-token': csrfToken,
        },
        body: JSON.stringify({ score: 2.0, target: 4.0 }),
      },
      jar
    );

    if (!response.ok) {
      console.log(`- ${topic.label}: FAILED HTTP ${response.status} -> ${JSON.stringify(json)}`);
      continue;
    }

    const matched = json.matchedRecommendations || [];
    if (matched.length === 0) {
      console.log(`- ${topic.label}: none`);
    } else {
      console.log(`- ${topic.label}:`);
      matched.forEach((r) => console.log(`  * ${r.title} (priority ${r.priority})`));
    }
  }

  return { testedTopicIds: topics.map((t) => t.id) };
};

const testC = async (): Promise<void> => {
  section('Test C: Results with Recommendations');
  const responseRow = await pool.query(
    `SELECT id, session_token
     FROM assessment_responses
     WHERE status = 'completed'
       AND session_token IS NOT NULL
     ORDER BY completed_at DESC NULLS LAST, started_at DESC
     LIMIT 1`
  );

  if (responseRow.rows.length === 0) {
    console.log('No completed responses exist yet.');
    return;
  }

  const responseId = responseRow.rows[0].id as string;
  const sessionToken = responseRow.rows[0].session_token as string;
  const { response, json } = await jsonFetch<{
    success: boolean;
    data?: {
      dimensions: Array<{ title: string; recommendations: unknown[] }>;
      topRecommendations: unknown[];
    };
  }>(`${API_BASE}/public/responses/${responseId}/results`, {
    headers: {
      'x-session-token': sessionToken,
    },
  });

  if (!response.ok || !json.success || !json.data) {
    console.log(`FAILED: HTTP ${response.status} -> ${JSON.stringify(json)}`);
    return;
  }

  const firstDimension = json.data.dimensions[0];
  console.log(`Response ID: ${responseId}`);
  console.log('dimensions[0].recommendations:');
  console.log(JSON.stringify(firstDimension?.recommendations || [], null, 2));
  console.log('topRecommendations (top 3):');
  console.log(JSON.stringify((json.data.topRecommendations || []).slice(0, 3), null, 2));
};

const printCodeSection = (
  filePath: string,
  startPattern: RegExp,
  endPattern?: RegExp
): string => {
  const content = fs.readFileSync(filePath, 'utf-8');
  const lines = content.split(/\r?\n/);
  const startIndex = lines.findIndex((line) => startPattern.test(line));
  if (startIndex === -1) return `Pattern not found in ${filePath}`;

  let endIndex = lines.length - 1;
  if (endPattern) {
    const relative = lines.slice(startIndex + 1).findIndex((line) => endPattern.test(line));
    if (relative >= 0) {
      endIndex = startIndex + relative;
    }
  }

  return lines.slice(startIndex, endIndex + 1).join('\n');
};

const testD = (): void => {
  section('Test D: Slider Code Verification');
  const repoRoot = path.resolve(__dirname, '..', '..');
  const dualSlider = path.join(repoRoot, 'HORVÁTH', 'src', 'components', 'ui', 'sliders', 'DualSlider.tsx');
  const readinessStore = path.join(repoRoot, 'HORVÁTH', 'src', 'store', 'readiness', 'readiness.store.ts');
  const topicCard = path.join(repoRoot, 'HORVÁTH', 'src', 'components', 'survey', 'TopicCard.tsx');

  console.log('\nDualSlider.tsx (valid values array + step default):');
  console.log(printCodeSection(dualSlider, /const VALID_SCORES =/, /;\s*$/));
  console.log(printCodeSection(dualSlider, /step = 0\.5/));

  console.log('\nreadiness.store.ts (submitAnswer function):');
  console.log(printCodeSection(readinessStore, /submitAnswer: async/, /completeAssessment: async/));

  console.log('\nTopicCard.tsx (step prop):');
  console.log(printCodeSection(topicCard, /step=\{0\.5\}/));
};

const diagnoseCoverage = async (): Promise<{ previouslyEmptyTopic: { id: string; label: string } | null }> => {
  section('Issue 2 - Step 1: Diagnose Recommendation Coverage');
  const recFile = path.resolve(__dirname, '..', 'recommendations.json');
  const recJson = JSON.parse(fs.readFileSync(recFile, 'utf-8')) as {
    rules: Array<{ topicId: string }>;
  };

  const rulesCount = recJson.rules.length;
  const topicKeys = new Set(
    (await pool.query('SELECT topic_key FROM topics')).rows.map((r) => r.topic_key as string)
  );

  const mapped = recJson.rules.filter((r) => topicKeys.has(r.topicId));
  const failed = recJson.rules
    .map((r) => r.topicId)
    .filter((key) => !topicKeys.has(key));

  const mappedTopicRows = await pool.query(
    `SELECT DISTINCT t.topic_key, t.label
     FROM topics t
     JOIN topic_recommendations tr ON tr.topic_id = t.id
     WHERE tr.is_active = true
     ORDER BY t.topic_key`
  );

  const emptyTopicRows = await pool.query(
    `SELECT t.id, t.label
     FROM topics t
     WHERE NOT EXISTS (
       SELECT 1
       FROM topic_recommendations tr
       WHERE tr.topic_id = t.id AND tr.is_active = true
     )
     ORDER BY t.label`
  );

  const previouslyEmptyTopic =
    emptyTopicRows.rows.length > 0
      ? ({ id: emptyTopicRows.rows[0].id as string, label: emptyTopicRows.rows[0].label as string } as const)
      : null;

  console.log(`Rules in recommendations.json: ${rulesCount}`);
  console.log(`Mapped successfully to existing topics: ${mapped.length}`);
  console.log(`Failed mappings: ${failed.length}`);
  if (failed.length > 0) {
    failed.forEach((key) => console.log(`- ${key}`));
  }

  console.log(`Topics currently with recommendations (${mappedTopicRows.rows.length}):`);
  mappedTopicRows.rows.forEach((row) => console.log(`- ${row.topic_key} (${row.label})`));

  console.log(`Topics currently with 0 active recommendations: ${emptyTopicRows.rows.length}`);
  if (previouslyEmptyTopic) {
    console.log(
      `Using previously-empty topic for retest after fix: ${previouslyEmptyTopic.label} (${previouslyEmptyTopic.id})`
    );
  }

  return { previouslyEmptyTopic };
};

const seedBaselineRecommendations = async (): Promise<void> => {
  section('Issue 2 - Step 2: Seed Missing Recommendations');
  const client = await pool.connect();
  try {
    await client.query('BEGIN');
    await client.query(`
      CREATE TEMP TABLE tmp_topics_without_recs AS
      SELECT t.id, t.label
      FROM topics t
      WHERE NOT EXISTS (
        SELECT 1 FROM topic_recommendations tr
        WHERE tr.topic_id = t.id AND tr.is_active = true
      );
    `);

    await client.query(`
      INSERT INTO topic_recommendations (
        topic_id, score_max, gap_min,
        title, description,
        why, what, how,
        action_items, category, priority, tags,
        is_active, order_index
      )
      SELECT
        t.id,
        2.5,
        0.5,
        'Build foundational ' || t.label || ' practices',
        'Current maturity is low. Focus on establishing basic processes and awareness.',
        'Without foundational practices, improvements in this area will be unsustainable.',
        'Establish basic processes, roles, and guidelines for ' || t.label,
        '1. Assess current state in detail
2. Identify quick wins
3. Assign ownership
4. Document basic procedures',
        '["Conduct gap assessment","Assign topic owner","Create basic documentation","Schedule monthly review"]'::jsonb,
        'Quick Win',
        70,
        ARRAY['foundation', 'quick-win'],
        true,
        0
      FROM tmp_topics_without_recs t;
    `);

    await client.query(`
      INSERT INTO topic_recommendations (
        topic_id, score_min, score_max, gap_min,
        title, description,
        why, what, how,
        action_items, category, priority, tags,
        is_active, order_index
      )
      SELECT
        t.id,
        2.0,
        3.5,
        1.5,
        'Advance ' || t.label || ' capability',
        'You have foundations but significant gap remains. A structured project approach needed.',
        'Closing this gap will create measurable competitive improvement.',
        'Develop structured programs to systematically improve ' || t.label,
        '1. Define target state clearly
2. Build capability roadmap
3. Allocate resources
4. Track progress quarterly',
        '["Define success metrics","Build 90-day roadmap","Assign project team","Establish review cadence"]'::jsonb,
        'Project',
        60,
        ARRAY['capability', 'structured'],
        true,
        1
      FROM tmp_topics_without_recs t;
    `);

    await client.query(`
      INSERT INTO topic_recommendations (
        topic_id, target_min, gap_min,
        title, description,
        why, what, how,
        action_items, category, priority, tags,
        is_active, order_index
      )
      SELECT
        t.id,
        4.0,
        2.0,
        'Transform ' || t.label || ' to industry leadership',
        'Your ambition is high and the gap is significant. This requires strategic investment.',
        'Industry leadership in this area will create lasting competitive advantage.',
        'Make ' || t.label || ' a strategic priority with dedicated resources and executive sponsorship',
        '1. Secure executive sponsorship
2. Allocate dedicated budget
3. Build or hire specialized capability
4. Set industry benchmark targets',
        '["Present business case to leadership","Benchmark against industry leaders","Build transformation roadmap","Establish innovation team"]'::jsonb,
        'Big Bet',
        50,
        ARRAY['transformation', 'strategic'],
        true,
        2
      FROM tmp_topics_without_recs t;
    `);

    const count = await client.query('SELECT COUNT(*)::int AS count FROM topic_recommendations');
    await client.query('COMMIT');
    console.log(`New total topic_recommendations count: ${count.rows[0].count}`);
  } catch (error) {
    await client.query('ROLLBACK');
    throw error;
  } finally {
    client.release();
  }
};

const retestTopic = async (topic: { id: string; label: string } | null): Promise<void> => {
  section('Issue 2 - Retest on Previously Empty Topic');
  if (!topic) {
    console.log('No previously-empty topic found, skipping retest.');
    return;
  }

  const { jar, csrfToken } = await loginAdminWithCsrf();
  const { response, json } = await jsonFetch<{ matchedRecommendations: Array<{ title: string; priority: number }> }>(
    `${API_BASE}/admin/topics/${topic.id}/recommendations/test`,
    {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'x-csrf-token': csrfToken,
      },
      body: JSON.stringify({ score: 2.0, target: 4.0 }),
    },
    jar
  );

  if (!response.ok) {
    console.log(`FAILED: HTTP ${response.status}`);
    console.log(json);
    return;
  }

  console.log(`Topic: ${topic.label}`);
  if (!json.matchedRecommendations || json.matchedRecommendations.length === 0) {
    console.log('Result: none');
    return;
  }

  console.log('Triggered recommendations:');
  json.matchedRecommendations.forEach((r) =>
    console.log(`- ${r.title} (priority ${r.priority})`)
  );
};

const main = async (): Promise<void> => {
  try {
    await waitForServer();
    await testA();
    await testB();
    await testC();
    testD();
    const { previouslyEmptyTopic } = await diagnoseCoverage();
    await seedBaselineRecommendations();
    await retestTopic(previouslyEmptyTopic);
  } finally {
    await pool.end();
  }
};

main().catch((error) => {
  console.error('Verification script failed:', error);
  process.exit(1);
});


==================================================
FILE: scripts/run-migration.ts
==================================================

import { pool } from '../src/config/database';
import { logger } from '../src/utils/logger';
import { AddRecommendationAndNarrativeTables1708035600000 } from '../migrations/1708035600000_add_recommendation_and_narrative_tables';

// Mock TypeORM QueryRunner interface for our needs
class PostgresQueryRunner {
  async query(query: string, parameters?: any[]): Promise<any> {
    logger.info(`Executing query: ${query.substring(0, 50)}...`);
    return pool.query(query, parameters);
  }
}

async function runMigration() {
  try {
    logger.info('🚀 Starting migration shim runner...');
    
    const queryRunner = new PostgresQueryRunner();
    const migration = new AddRecommendationAndNarrativeTables1708035600000();
    
    logger.info('Running migration: AddRecommendationAndNarrativeTables1708035600000');
    await migration.up(queryRunner as any);
    
    logger.info('✅ Migration completed successfully');
    process.exit(0);
  } catch (error) {
    logger.error('❌ Migration failed:', error);
    process.exit(1);
  }
}

runMigration();


==================================================
FILE: scripts/test-endpoints.js
==================================================

const http = require('http');

const urls = [
  'http://localhost:3001/api/public/assessments/active/structure',
  'http://localhost:3001/api/public/recommendations/definition',
  'http://localhost:3001/api/public/narrative/definition'
];

console.log('🚀 Testing API Endpoints...\n');

urls.forEach(url => {
  http.get(url, (res) => {
    let data = '';
    res.on('data', (chunk) => data += chunk);
    res.on('end', () => {
      console.log(`✅ URL: ${url}`);
      console.log(`   Status: ${res.statusCode}`);
      if (res.statusCode === 200) {
        console.log(`   Body snippet: ${data.substring(0, 100).replace(/\n/g, ' ')}...`);
      } else {
        console.log(`   Error Body: ${data}`);
      }
      console.log('---');
    });
  }).on('error', (err) => {
    console.error(`❌ Error fetching ${url}:`, err.message);
  });
});


==================================================
FILE: scripts/validate-data.ts
==================================================

import { pool } from '../src/config/database';
import { logger } from '../src/utils/logger';

async function validateData() {
  try {
    logger.info('🔍 Starting data validation...');
    
    // 1. Check recommendation rules
    const rulesResult = await pool.query(`
      SELECT COUNT(*) as count FROM recommendation_rules
    `);
    logger.info(`✅ Recommendation Rules Count: ${rulesResult.rows[0].count} (Expected: ~8)`);

    // 2. Check recommendation meta
    const metaResult = await pool.query(`
      SELECT meta_key, jsonb_typeof(meta_value) as type 
      FROM recommendation_meta
    `);
    logger.info('✅ Recommendation Meta Keys:');
    metaResult.rows.forEach((r: any) => logger.info(`   - ${r.meta_key} (${r.type})`));

    // 3. Check narrative templates
    const templatesResult = await pool.query(`
      SELECT template_type, COUNT(*) as count 
      FROM narrative_templates 
      GROUP BY template_type
    `);
    logger.info('✅ Narrative Templates distribution:');
    templatesResult.rows.forEach((r: any) => logger.info(`   - ${r.template_type}: ${r.count}`));

    // 4. Check narrative theme map
    const themeResult = await pool.query(`
      SELECT COUNT(*) as count FROM narrative_theme_map
    `);
    logger.info(`✅ Narrative Theme Map Count: ${themeResult.rows[0].count} (Expected: 12)`);

    // 5. Check narrative config
    const configResult = await pool.query(`
      SELECT config_key FROM narrative_config
    `);
    logger.info('✅ Narrative Config Keys:');
    configResult.rows.forEach((r: any) => logger.info(`   - ${r.config_key}`));

    logger.info('✅ validation completed');
    process.exit(0);
  } catch (error) {
    logger.error('❌ Validation failed:', error);
    process.exit(1);
  }
}

validateData();


==================================================
FILE: seeds/seed-all.ts
==================================================

﻿/**
 * File: seeds/seed-all.ts
 * Purpose: Seeds the database with complete data from JSON files
 * - Assessment with dimensions and topics from questions.json
 * - Recommendations from recommendations.json
 * - Narrative from narrative.json
 */

import { pool, query } from '../src/config/database';
import { AuthService } from '../src/services/authService';
import { logger } from '../src/utils/logger';
import * as fs from 'fs';
import * as path from 'path';

interface Topic {
  id: string;
  label: string;
  prompt: string;
  order: number;
  anchors?: Record<string, string>;
}

interface Dimension {
  id: string;
  title: string;
  order: number;
  topics: Topic[];
}

interface QuestionsData {
  version: number;
  dimensions: Dimension[];
}

interface RecommendationRule {
  id: string;
  dimensionId: string;
  topicId: string;
  priority: number;
  conditions: Record<string, any>;
  title: string;
  summary: string;
  actions: string[];
  tags: string[];
}

interface RecommendationsData {
  version: number;
  rules: RecommendationRule[];
  meta: {
    themeMap: Record<string, string>;
    urgencyTags: string[];
    dimensionOrder: string[];
    dimensionWeights: Record<string, number>;
    dimensionColors: Record<string, string>;
    titleTemplates: Record<string, string[]>;
    descriptionTemplate: string;
  };
}

interface NarrativeData {
  version: number;
  themeMap: Record<string, string>;
  headlines: {
    lowConfidencePrefix: string;
    byStageId: Record<string, string>;
  };
  executiveSummary: {
    sentence1: string;
    sentence2: string;
    sentence3: string;
  };
  stageRationale: string;
  priorityWhyTemplate: string;
  notes: Record<string, string>;
  maturityThresholds: Record<string, number>;
  executiveTemplates: {
    maturityLevel: Record<string, string>;
    gapAnalysis: Record<string, string>;
    strengths: Record<string, string>;
    priorities: Record<string, string>;
  };
}

const getSeedAdminPassword = (): string => {
  const adminPassword = process.env.SEED_ADMIN_PASSWORD;
  if (!adminPassword || adminPassword.length < 12) {
    throw new Error('[SEED] SEED_ADMIN_PASSWORD env var is required and must be >=12 chars');
  }
  return adminPassword;
};

async function seedAssessment(client: any, questionsData: QuestionsData) {
  logger.info('ðŸ“ Seeding assessment, dimensions, and topics...');

  // Create admin user if not exists
  const adminCheck = await client.query('SELECT id, email FROM users WHERE email = $1', ['admin@leadership.com']);
  let admin;
  const adminPassword = getSeedAdminPassword();
  if (adminCheck.rows.length === 0) {
    admin = await AuthService.createUser('admin@leadership.com', adminPassword, 'System Administrator', 'super_admin');
    logger.info(`âœ… Admin created: ${admin.email}`);
  } else {
    admin = adminCheck.rows[0];
  }

  // Deactivate old assessments
  await client.query('UPDATE assessments SET is_active = false');

  // Create assessment
  const assessmentResult = await client.query(
    `INSERT INTO assessments (version, title, description, is_active, created_by, is_published, published_at)
     VALUES ($1, $2, $3, $4, $5, $6, NOW())
     RETURNING id`,
    [1, 'AI Readiness Assessment', 'Evaluate your organization\'s readiness for AI adoption across key dimensions.', true, admin.id, true]
  );
  const assessmentId = assessmentResult.rows[0].id;
  logger.info(`âœ… Assessment created: ${assessmentId}`);

  // Create dimensions and topics from questions.json
  for (const dim of questionsData.dimensions) {
    const dimResult = await client.query(
      `INSERT INTO dimensions (assessment_id, dimension_key, title, category, order_index)
       VALUES ($1, $2, $3, $4, $5)
       RETURNING id`,
      [assessmentId, dim.id, dim.title, dim.title, dim.order]
    );
    const dimensionId = dimResult.rows[0].id;

    for (const topic of dim.topics) {
      await client.query(
        `INSERT INTO topics (dimension_id, topic_key, label, prompt, order_index)
         VALUES ($1, $2, $3, $4, $5)`,
        [dimensionId, topic.id, topic.label, topic.prompt, topic.order]
      );
    }
    logger.info(`âœ… Created dimension: ${dim.title} with ${dim.topics.length} topics`);
  }
}

async function seedTopicLevelLabels(client: any, questionsData: QuestionsData) {
  logger.info('Seeding topic level labels from questions.json...');

  for (const dim of questionsData.dimensions) {
    for (const topic of dim.topics) {
      await client.query(
        `UPDATE topics
         SET level_1_label = $1,
             level_2_label = $2,
             level_3_label = $3,
             level_4_label = $4,
             level_5_label = $5,
             updated_at = NOW()
         WHERE topic_key = $6`,
        [
          topic.anchors?.['1'] ?? null,
          topic.anchors?.['2'] ?? null,
          topic.anchors?.['3'] ?? null,
          topic.anchors?.['4'] ?? null,
          topic.anchors?.['5'] ?? null,
          topic.id,
        ]
      );
    }
  }

  logger.info('Inserted level labels for seeded topics');
}

async function seedTopicRecommendations(client: any, recData: RecommendationsData) {
  logger.info('Seeding topic_recommendations...');

  await client.query('DELETE FROM topic_recommendations');

  const topicLookupResult = await client.query('SELECT id, topic_key FROM topics');
  const topicIdByKey = new Map<string, string>();
  for (const row of topicLookupResult.rows as Array<{ id: string; topic_key: string }>) {
    topicIdByKey.set(row.topic_key, row.id);
  }

  let inserted = 0;
  for (const rule of recData.rules) {
    const topicId = topicIdByKey.get(rule.topicId);
    if (!topicId) {
      logger.warn(`Skipping topic recommendation '${rule.id}' - no topic match for topicId='${rule.topicId}'`);
      continue;
    }

    const conditions = rule.conditions || {};
    await client.query(
      `INSERT INTO topic_recommendations (
         topic_id,
         score_min, score_max,
         target_min, target_max,
         gap_min, gap_max,
         title, description,
         action_items,
         category, priority, tags,
         is_active, order_index
       ) VALUES (
         $1,
         $2, $3,
         $4, $5,
         $6, $7,
         $8, $9,
         $10::jsonb,
         $11, $12, $13,
         true, $14
       )`,
      [
        topicId,
        conditions.currentMin ?? null,
        conditions.currentMax ?? null,
        conditions.targetMin ?? null,
        conditions.targetMax ?? null,
        conditions.gapMin ?? null,
        conditions.gapMax ?? null,
        rule.title,
        rule.summary,
        JSON.stringify(rule.actions ?? []),
        'Project',
        rule.priority ?? 50,
        rule.tags ?? [],
        0,
      ]
    );
    inserted += 1;
  }

  logger.info(`Inserted ${inserted} topic_recommendations rows`);
}

async function seedRecommendations(client: any, recData: RecommendationsData) {
  logger.info('ðŸ“ Seeding recommendations...');

  // Seed recommendation rules
  for (const rule of recData.rules) {
    const actionItems = rule.actions.map((text, idx) => ({ id: String(idx + 1), text }));

    await client.query(`
      INSERT INTO recommendation_rules (
        rule_key, dimension_key, title, description, conditions,
        priority_score, tags, impact_level, effort_level, timeframe, 
        action_items
      ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11)
      ON CONFLICT (rule_key) DO UPDATE SET
        title = EXCLUDED.title,
        description = EXCLUDED.description,
        conditions = EXCLUDED.conditions,
        priority_score = EXCLUDED.priority_score,
        tags = EXCLUDED.tags,
        action_items = EXCLUDED.action_items,
        updated_at = CURRENT_TIMESTAMP
    `, [
      rule.id,
      rule.dimensionId,
      rule.title,
      rule.summary,
      JSON.stringify(rule.conditions),
      (rule.priority / 100).toFixed(2),
      JSON.stringify(rule.tags),
      'high',
      'medium',
      'short-term',
      JSON.stringify(actionItems)
    ]);
  }
  logger.info(`âœ… Inserted ${recData.rules.length} recommendation rules`);

  // Seed meta
  const meta = recData.meta;
  const metaItems = [
    { key: 'dimension_colors', value: meta.dimensionColors },
    { key: 'dimension_weights', value: meta.dimensionWeights },
    { key: 'theme_map', value: meta.themeMap },
    { key: 'urgency_tags', value: meta.urgencyTags },
    { key: 'dimension_order', value: meta.dimensionOrder },
    { key: 'title_templates', value: meta.titleTemplates },
    { key: 'description_template', value: meta.descriptionTemplate }
  ];

  for (const item of metaItems) {
    await client.query(`
      INSERT INTO recommendation_meta (meta_key, meta_value, description)
      VALUES ($1, $2, $3)
      ON CONFLICT (meta_key) DO UPDATE SET meta_value = EXCLUDED.meta_value, updated_at = CURRENT_TIMESTAMP
    `, [item.key, JSON.stringify(item.value), `Seeded ${item.key}`]);
  }
  logger.info('âœ… Inserted recommendation metadata');
}

async function seedNarrative(client: any, narData: NarrativeData) {
  logger.info('ðŸ“ Seeding narrative...');

  // Seed theme map
  for (const [key, label] of Object.entries(narData.themeMap)) {
    await client.query(`
      INSERT INTO narrative_theme_map (theme_key, theme_label)
      VALUES ($1, $2)
      ON CONFLICT (theme_key) DO UPDATE SET theme_label = EXCLUDED.theme_label
    `, [key, label]);
  }
  logger.info(`âœ… Inserted ${Object.keys(narData.themeMap).length} theme mappings`);

  // Seed narrative config
  const configs = [
    { key: 'maturity_thresholds', value: narData.maturityThresholds },
    { key: 'headlines', value: narData.headlines },
    { key: 'executive_summary', value: narData.executiveSummary },
    { key: 'stage_rationale', value: narData.stageRationale },
    { key: 'priority_why_template', value: narData.priorityWhyTemplate },
    { key: 'notes', value: narData.notes }
  ];

  for (const config of configs) {
    await client.query(`
      INSERT INTO narrative_config (config_key, config_value, description)
      VALUES ($1, $2, $3)
      ON CONFLICT (config_key) DO UPDATE SET config_value = EXCLUDED.config_value, updated_at = CURRENT_TIMESTAMP
    `, [config.key, JSON.stringify(config.value), `Seeded ${config.key}`]);
  }
  logger.info('âœ… Inserted narrative config');

  // Seed executive templates
  const categories = ['maturityLevel', 'gapAnalysis', 'strengths', 'priorities'] as const;
  for (const category of categories) {
    const templates = narData.executiveTemplates[category];
    if (!templates) continue;

    for (const [key, templateString] of Object.entries(templates)) {
      await client.query(`
        INSERT INTO narrative_templates (template_key, template_type, category, template, priority)
        VALUES ($1, $2, $3, $4, $5)
        ON CONFLICT (template_key) DO UPDATE SET template = EXCLUDED.template, updated_at = CURRENT_TIMESTAMP
      `, [`executive_${category}_${key}`, 'executive', category, templateString, 0]);
    }
  }
  logger.info('âœ… Inserted executive templates');
}

async function main() {
  const client = await pool.connect();

  try {
    if (process.env.NODE_ENV === 'production') {
      throw new Error('[SEED] Seeding is disabled in production');
    }

    logger.info('ðŸš€ Starting complete database seeding...\n');

    // Read JSON files from backend root
    const questionsPath = path.join(__dirname, '../questions.json');
    const recPath = path.join(__dirname, '../recommendations.json');
    const narPath = path.join(__dirname, '../narrative.json');

    const questionsData: QuestionsData = JSON.parse(fs.readFileSync(questionsPath, 'utf-8'));
    const recData: RecommendationsData = JSON.parse(fs.readFileSync(recPath, 'utf-8'));
    const narData: NarrativeData = JSON.parse(fs.readFileSync(narPath, 'utf-8'));

    await client.query('BEGIN');

    await seedAssessment(client, questionsData);
    await seedTopicLevelLabels(client, questionsData);
    await seedTopicRecommendations(client, recData);
    await seedRecommendations(client, recData);
    await seedNarrative(client, narData);

    await client.query('COMMIT');

    logger.info('\nâœ…âœ…âœ… ALL SEEDING COMPLETED SUCCESSFULLY âœ…âœ…âœ…');
    logger.info('   Email: admin@leadership.com');

    process.exit(0);
  } catch (error) {
    await client.query('ROLLBACK');
    logger.error('\nâŒâŒâŒ SEEDING FAILED âŒâŒâŒ', error);
    process.exit(1);
  } finally {
    client.release();
    await pool.end();
  }
}

main();


==================================================
FILE: seeds/seed-data.ts
==================================================

﻿/**
 * File: seeds/seed-data.ts
 * Purpose: Seeds the database with initial admin, assessment, dimensions, and settings
 */

import { query } from '../src/config/database';
import { AuthService } from '../src/services/authService';
import { logger } from '../src/utils/logger';

const getSeedAdminPassword = (): string => {
  const adminPassword = process.env.SEED_ADMIN_PASSWORD;
  if (!adminPassword || adminPassword.length < 12) {
    throw new Error('[SEED] SEED_ADMIN_PASSWORD env var is required and must be >=12 chars');
  }
  return adminPassword;
};

async function seedDatabase() {
  try {
    logger.info('Starting database seeding...');
    const adminPassword = getSeedAdminPassword();

    logger.info('Creating admin user...');
    const adminCheck = await query('SELECT id, email FROM users WHERE email = $1', ['admin@leadership.com']);
    let admin: { id: string; email: string };

    if (adminCheck.rows.length === 0) {
      admin = await AuthService.createUser(
        'admin@leadership.com',
        adminPassword,
        'System Administrator',
        'super_admin'
      );
      logger.info(`Admin created: ${admin.email}`);
    } else {
      admin = adminCheck.rows[0] as { id: string; email: string };
      logger.info(`Admin already exists: ${admin.email}`);
    }

    logger.info('Creating AI Readiness Assessment...');
    let assessmentId: string;
    const assessmentCheck = await query('SELECT id FROM assessments WHERE title = $1', ['AI Readiness Assessment']);

    if (assessmentCheck.rows.length === 0) {
      await query('UPDATE assessments SET is_active = false');

      const assessmentResult = await query(
        `INSERT INTO assessments (version, title, description, is_active, created_by, is_published, published_at)
         VALUES ($1, $2, $3, $4, $5, $6, NOW())
         RETURNING id`,
        [
          1,
          'AI Readiness Assessment',
          "Evaluate your organization's readiness for AI adoption across key dimensions.",
          true,
          admin.id,
          true,
        ]
      );
      assessmentId = assessmentResult.rows[0].id as string;
      logger.info(`Assessment created with ID: ${assessmentId}`);

      const sampleDimensions = [
        { key: 'strategy', title: 'AI Strategy', category: 'Strategy & Governance', desc: 'Alignment of AI with business goals and roadmap.' },
        { key: 'governance', title: 'AI Governance', category: 'Strategy & Governance', desc: 'Policies, ethics, and risk management.' },
        { key: 'data', title: 'Data & Analytics', category: 'Data Foundations', desc: 'Data quality, availability, and management.' },
        { key: 'technology', title: 'Technology Infrastructure', category: 'Technology & Infrastructure', desc: 'Compute, cloud, and tools for AI.' },
        { key: 'value', title: 'Value Generation', category: 'Strategy & Governance', desc: 'Use case identification and ROI measurement.' },
        { key: 'capabilities', title: 'Skills & Capabilities', category: 'Leadership Enablement', desc: 'Talent, training, and organizational culture.' },
      ];

      for (let i = 0; i < sampleDimensions.length; i += 1) {
        const dim = sampleDimensions[i];
        const dimResult = await query(
          `INSERT INTO dimensions
             (assessment_id, dimension_key, title, description, category, order_index)
           VALUES ($1, $2, $3, $4, $5, $6)
           RETURNING id`,
          [assessmentId, dim.key, dim.title, dim.desc, dim.category, i + 1]
        );

        const dimensionId = dimResult.rows[0].id as string;

        for (let j = 1; j <= 5; j += 1) {
          await query(
            `INSERT INTO topics
               (dimension_id, topic_key, label, prompt, order_index)
             VALUES ($1, $2, $3, $4, $5)`,
            [
              dimensionId,
              `${dim.key}_topic_${j}`,
              `${dim.title} - Key Area ${j}`,
              `How would you rate your organization's maturity in ${dim.title} area ${j}?`,
              j,
            ]
          );
        }

        logger.info(`Created dimension: ${dim.title} with 5 topics`);
      }
    } else {
      assessmentId = assessmentCheck.rows[0].id as string;
      await query('UPDATE assessments SET is_active = true WHERE id = $1', [assessmentId]);
      logger.info(`Assessment already exists (ID: ${assessmentId}) - activated`);
    }

    const settings = [
      { key: 'site_name', value: '"AI Readiness Assessment"' },
      { key: 'max_login_attempts', value: '5' },
      { key: 'session_timeout_hours', value: '24' },
      { key: 'enable_email_notifications', value: 'true' },
      { key: 'results_access_duration_days', value: '30' },
      { key: 'admin_email', value: '"admin@leadership.com"' },
    ];

    for (const setting of settings) {
      await query(
        `INSERT INTO system_settings (setting_key, setting_value)
         VALUES ($1, $2)
         ON CONFLICT (setting_key) DO UPDATE SET setting_value = EXCLUDED.setting_value`,
        [setting.key, setting.value]
      );
    }

    logger.info('System settings created');
    logger.info('Database seeding completed successfully');
    logger.info('Admin user seeded or already present: admin@leadership.com');
    process.exit(0);
  } catch (error) {
    logger.error('Seeding failed:', error);
    process.exit(1);
  }
}

seedDatabase();


==================================================
FILE: seeds/seed-recommendations-and-narrative.ts
==================================================

import { pool } from '../src/config/database';
import * as fs from 'fs';
import * as path from 'path';
import { logger } from '../src/utils/logger';

// Valid interface matching recommendations.json
interface RecommendationRule {
  id: string;
  dimensionId: string;
  topicId: string;
  priority: number;
  conditions: Record<string, any>;
  title: string;
  summary: string;
  actions: string[];
  tags: string[];
}

interface RecommendationsData {
  version: number;
  rules: RecommendationRule[];
  meta: {
    themeMap: Record<string, string>;
    urgencyTags: string[];
    dimensionOrder: string[];
    dimensionWeights: Record<string, number>;
    dimensionColors: Record<string, string>;
    titleTemplates: Record<string, string[]>;
    descriptionTemplate: string;
  };
}

// Valid interface matching narrative.json
interface NarrativeData {
  version: number;
  themeMap: Record<string, string>;
  headlines: {
    lowConfidencePrefix: string;
    byStageId: Record<string, string>;
  };
  executiveSummary: {
    sentence1: string;
    sentence2: string;
    sentence3: string;
  };
  stageRationale: string;
  priorityWhyTemplate: string;
  notes: Record<string, string>;
  maturityThresholds: Record<string, number>;
  executiveTemplates: {
    maturityLevel: Record<string, string>;
    gapAnalysis: Record<string, string>;
    strengths: Record<string, string>;
    priorities: Record<string, string>;
  };
}

async function seedRecommendations() {
  logger.info('🌱 Starting recommendations seeding...');
  
  // Read recommendations.json
  const recommendationsPath = path.join(__dirname, '../recommendations.json');
  
  if (!fs.existsSync(recommendationsPath)) {
    throw new Error(`Recommendations file not found at ${recommendationsPath}`);
  }

  const fileContent = fs.readFileSync(recommendationsPath, 'utf-8');
  const data: RecommendationsData = JSON.parse(fileContent);

  const client = await pool.connect();
  
  try {
    await client.query('BEGIN');

    // 1. Seed recommendation_rules
    logger.info(`📝 Seeding ${data.rules.length} recommendation_rules...`);
    let ruleCount = 0;
    
    for (const rule of data.rules) {
      // Map JSON fields to DB columns
      // Note: 'action_items' in DB expects JSONB array of objects, but JSON has array of strings
      // We'll wrap strings into simple objects: { text: "..." }
      const actionItems = rule.actions.map((text, idx) => ({ 
        id: String(idx + 1), 
        text 
      }));

      await client.query(`
        INSERT INTO recommendation_rules (
          rule_key, dimension_key, title, description, conditions,
          priority_score, tags, impact_level, effort_level, timeframe, 
          action_items
        ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11)
        ON CONFLICT (rule_key) DO UPDATE SET
          title = EXCLUDED.title,
          description = EXCLUDED.description,
          conditions = EXCLUDED.conditions,
          priority_score = EXCLUDED.priority_score,
          tags = EXCLUDED.tags,
          action_items = EXCLUDED.action_items,
          updated_at = CURRENT_TIMESTAMP
      `, [
        rule.id,                        // rule_key
        rule.dimensionId,               // dimension_key
        rule.title,                     // title
        rule.summary,                   // description
        JSON.stringify(rule.conditions),// conditions
        (rule.priority / 100).toFixed(2), // priority_score (convert 0-100 to 0.0-1.0)
        JSON.stringify(rule.tags),      // tags
        'high',                         // impact_level (default)
        'medium',                       // effort_level (default)
        'short-term',                   // timeframe (default)
        JSON.stringify(actionItems)     // action_items
      ]);
      ruleCount++;
    }
    logger.info(`✅ Inserted/Updated ${ruleCount} recommendation rules`);

    // 2. Seed recommendation_meta
    logger.info('📝 Seeding recommendation_meta...');
    const meta = data.meta;
    
    const metaItems = [
      { key: 'dimension_colors', value: meta.dimensionColors, desc: 'Color codes for each dimension' },
      { key: 'dimension_weights', value: meta.dimensionWeights, desc: 'Priority weights for each dimension' },
      { key: 'theme_map', value: meta.themeMap, desc: 'Tag to theme label mapping' },
      { key: 'urgency_tags', value: meta.urgencyTags, desc: 'Tags that imply urgency' },
      { key: 'dimension_order', value: meta.dimensionOrder, desc: 'Display order of dimensions' },
      { key: 'title_templates', value: meta.titleTemplates, desc: 'Templates for generated titles' },
      { key: 'description_template', value: meta.descriptionTemplate, desc: 'Template for generated description' }
    ];

    for (const item of metaItems) {
      await client.query(`
        INSERT INTO recommendation_meta (meta_key, meta_value, description)
        VALUES ($1, $2, $3)
        ON CONFLICT (meta_key) DO UPDATE SET
          meta_value = EXCLUDED.meta_value,
          description = EXCLUDED.description,
          updated_at = CURRENT_TIMESTAMP
      `, [item.key, JSON.stringify(item.value), item.desc]);
    }

    logger.info('✅ Inserted recommendation metadata');

    await client.query('COMMIT');
    logger.info('✅ Recommendations seeding completed successfully');
  } catch (error) {
    await client.query('ROLLBACK');
    logger.error('❌ Error seeding recommendations:', error);
    throw error;
  } finally {
    client.release();
  }
}

async function seedNarrative() {
  logger.info('🌱 Starting narrative seeding...');
  
  // Read narrative.json
  const narrativePath = path.join(__dirname, '../narrative.json');
  
  if (!fs.existsSync(narrativePath)) {
    throw new Error(`Narrative file not found at ${narrativePath}`);
  }

  const fileContent = fs.readFileSync(narrativePath, 'utf-8');
  const data: NarrativeData = JSON.parse(fileContent);

  const client = await pool.connect();
  
  try {
    await client.query('BEGIN');

    // 1. Seed narrative_theme_map
    logger.info('📝 Seeding narrative_theme_map...');
    let themeCount = 0;
    for (const [key, label] of Object.entries(data.themeMap)) {
      await client.query(`
        INSERT INTO narrative_theme_map (theme_key, theme_label)
        VALUES ($1, $2)
        ON CONFLICT (theme_key) DO UPDATE SET
          theme_label = EXCLUDED.theme_label
      `, [key, label]);
      themeCount++;
    }
    logger.info(`✅ Inserted ${themeCount} theme mappings`);

    // 2. Seed narrative_config
    logger.info('📝 Seeding narrative_config...');
    
    // Transform maturity thresholds object to sorted arrays if needed, or store as is
    // The DB schema expects flexible JSONB, so we store the exact structure from JSON
    const configs = [
      { key: 'maturity_thresholds', value: data.maturityThresholds, desc: 'Threshold values for maturity levels' },
      { key: 'headlines', value: data.headlines, desc: 'Headline templates' },
      { key: 'executive_summary', value: data.executiveSummary, desc: 'Executive summary templates' },
      { key: 'stage_rationale', value: data.stageRationale, desc: 'Rationale template' },
      { key: 'priority_why_template', value: data.priorityWhyTemplate, desc: 'Template for priority explanation' },
      { key: 'notes', value: data.notes, desc: 'Confidence notes' }
    ];

    for (const config of configs) {
      await client.query(`
        INSERT INTO narrative_config (config_key, config_value, description)
        VALUES ($1, $2, $3)
        ON CONFLICT (config_key) DO UPDATE SET
          config_value = EXCLUDED.config_value,
          description = EXCLUDED.description,
          updated_at = CURRENT_TIMESTAMP
      `, [config.key, JSON.stringify(config.value), config.desc]);
    }

    logger.info('✅ Inserted narrative config');

    // 3. Seed narrative_templates
    // The narrative.json structure is quite different from what was initially assumed.
    // It has "executiveTemplates" which contains specific categories like "maturityLevel", "gapAnalysis", etc.
    // It DOES NOT have "dimensionTemplates" or "gapTemplates" arrays as top-level properties.
    // We will flattening this structure into the definitions table.
    
    logger.info('📝 Seeding narrative_templates...');
    let templateCount = 0;

    const categories = ['maturityLevel', 'gapAnalysis', 'strengths', 'priorities'] as const;
    
    for (const category of categories) {
      const templates = data.executiveTemplates[category];
      if (!templates) continue;

      for (const [key, templateString] of Object.entries(templates)) {
         // specific key for this template, e.g. "executive_maturityLevel_leading"
         const templateKey = `executive_${category}_${key}`;
         
         await client.query(`
          INSERT INTO narrative_templates (
            template_key, template_type, category, template, priority
          ) VALUES ($1, $2, $3, $4, $5)
          ON CONFLICT (template_key) DO UPDATE SET
            template = EXCLUDED.template,
            updated_at = CURRENT_TIMESTAMP
        `, [
          templateKey,
          'executive',     // template_type
          category,        // category (e.g. maturityLevel)
          templateString,  // template content
          0                // priority default
        ]);
        templateCount++;
      }
    }

    logger.info(`✅ Inserted ${templateCount} narrative templates`);

    await client.query('COMMIT');
    logger.info('✅ Narrative seeding completed successfully');
  } catch (error) {
    await client.query('ROLLBACK');
    logger.error('❌ Error seeding narrative:', error);
    throw error;
  } finally {
    client.release();
  }
}

// Main execution
async function main() {
  try {
    logger.info('🚀 Starting complete seeding process...\n');
    
    await seedRecommendations();
    logger.info('\n');
    await seedNarrative();
    
    logger.info('\n✅✅✅ ALL SEEDING COMPLETED SUCCESSFULLY ✅✅✅');
    process.exit(0);
  } catch (error) {
    logger.error('\n❌❌❌ SEEDING FAILED ❌❌❌');
    logger.error(error);
    process.exit(1);
  } finally {
    // Force close pool to exit
    await pool.end();
  }
}

main();


==================================================
FILE: src/config/database.ts
==================================================

/**
 * File: src/config/database.ts
 * Purpose: Configures PostgreSQL connection pool and query execution
 */

import { Pool, PoolClient, QueryResult } from 'pg';
import { config } from './env';
import { logger } from '../utils/logger';

export const pool = new Pool({
  host: config.database.host,
  port: config.database.port,
  database: config.database.database,
  user: config.database.user,
  password: config.database.password,
  ssl: config.database.ssl ? { rejectUnauthorized: false } : false,
  max: 20,
  idleTimeoutMillis: 30000,
  connectionTimeoutMillis: 3000,
  ...(config.database.statementTimeout
    ? { options: `--statement_timeout=${config.database.statementTimeout}` }
    : {}),
});

pool.on('connect', () => {
  logger.info('Database connected successfully');
});

pool.on('error', (err) => {
  logger.error('[DB_POOL] Unexpected error on idle client', {
    message: err.message,
    stack: err.stack,
  });
});

export const query = async (text: string, params?: unknown[]): Promise<QueryResult> => {
  const start = Date.now();
  try {
    const res = await pool.query(text, params);
    const duration = Date.now() - start;
    logger.debug('Executed query', { text, duration, rows: res.rowCount });
    return res;
  } catch (error) {
    logger.error('Query error', { text, error });
    throw error;
  }
};

type ManagedPoolClient = PoolClient & {
  // Using `any` here is unavoidable because pg's query signature is heavily overloaded.
  query: (...args: any[]) => any;
  release: () => void;
};

export const getClient = async (): Promise<ManagedPoolClient> => {
  const client = await pool.connect();
  const query = client.query;
  const release = client.release;
  
  // Set a timeout to warn if client is checked out for too long
  const timeout = setTimeout(() => {
    logger.error('A client has been checked out for more than 5 seconds!');
  }, 5000);
  
  // Monkey patch query to keep original signature but allow interception if needed
  // Using `any` is unavoidable to preserve all pg query overloads safely.
  // eslint-disable-next-line @typescript-eslint/no-explicit-any
  client.query = (...args: any[]) => {
    const queryFn = query as unknown as (...innerArgs: any[]) => any;
    return queryFn.apply(client, args);
  };
  
  client.release = () => {
    clearTimeout(timeout);
    client.query = query;
    client.release = release;
    return release.apply(client);
  };
  
  return client as ManagedPoolClient;
};


==================================================
FILE: src/config/env.ts
==================================================

/**
 * File: src/config/env.ts
 * Purpose: Loads and exports environment variables with defaults
 */

import dotenv from 'dotenv';
import path from 'path';

// Load .env file
dotenv.config({ path: path.resolve(__dirname, '../../.env') });

const FORBIDDEN_JWT_SUBSTRINGS = ['default', 'secret', 'password', 'change-me', 'your-'];

const getRequiredJwtSecret = (): string => {
  const secret = process.env.JWT_SECRET;

  if (!secret) {
    throw new Error(
      'FATAL: JWT_SECRET environment variable is required. Generate one with: openssl rand -base64 32'
    );
  }

  if (secret.length < 32) {
    throw new Error('FATAL: JWT_SECRET must be at least 32 characters.');
  }

  const lower = secret.toLowerCase();
  if (FORBIDDEN_JWT_SUBSTRINGS.some((token) => lower.includes(token))) {
    throw new Error('FATAL: JWT_SECRET appears to be a default/insecure value. Set a strong random secret.');
  }

  return secret;
};

const jwtSecret = getRequiredJwtSecret();

const parseInteger = (value: string | undefined, fallback: number): number => {
  const parsed = Number.parseInt(value || '', 10);
  return Number.isFinite(parsed) ? parsed : fallback;
};

const parseBoolean = (value: string | undefined, fallback: boolean): boolean => {
  if (typeof value === 'undefined') return fallback;
  return value.toLowerCase() === 'true';
};

export const config = {
  port: parseInteger(process.env.PORT, 3001),
  nodeEnv: process.env.NODE_ENV || 'development',
  
  database: {
    host: process.env.DB_HOST || 'localhost',
    port: parseInteger(process.env.DB_PORT, 5432),
    database: process.env.DB_NAME || 'leadership_assessment',
    user: process.env.DB_USER || 'postgres',
    password: process.env.DB_PASSWORD || '',
    ssl: process.env.DB_SSL === 'true',
    statementTimeout: parseInteger(process.env.DB_STATEMENT_TIMEOUT, 10000),
  },
  
  jwt: {
    secret: jwtSecret,
    expiresIn: process.env.JWT_EXPIRES_IN || '24h',
  },
  rateLimit: {
    windowMs: parseInteger(process.env.RATE_LIMIT_WINDOW_MS, 15 * 60 * 1000),
    max: parseInteger(process.env.RATE_LIMIT_MAX, 100),
  },
  requireMigrations: parseBoolean(process.env.REQUIRE_MIGRATIONS, false),
  
  cors: {
    origin: process.env.CORS_ORIGIN || 'http://localhost:3000',
    credentials: true,
  },
};


==================================================
FILE: src/middleware/auth.ts
==================================================

/**
 * File: src/middleware/auth.ts
 * Purpose: Authentication middleware for verifying JWT tokens and sessions
 */

import { Request, Response, NextFunction } from 'express';
import jwt from 'jsonwebtoken';
import { config } from '../config/env';
import { query } from '../config/database';
import { logger } from '../utils/logger';

export const ADMIN_COOKIE_NAME = 'admin_token';

export interface AuthRequest extends Request {
  user?: {
    userId: string;
    email: string;
    fullName: string;
    role: string;
  };
  authToken?: string;
}

const getCookieValue = (cookieHeader: string | undefined, name: string): string | null => {
  if (!cookieHeader) return null;

  const match = cookieHeader
    .split(';')
    .map((cookie) => cookie.trim())
    .find((cookie) => cookie.startsWith(`${name}=`));

  if (!match) return null;
  return decodeURIComponent(match.slice(name.length + 1));
};

const extractAuthToken = (req: Request): string | null => {
  // Cookie-based auth is the primary mechanism. Bearer support is kept for compatibility.
  const cookieToken = (req as Request & { cookies?: Record<string, string> }).cookies?.[ADMIN_COOKIE_NAME];
  if (cookieToken) {
    return cookieToken;
  }

  const bearerToken = req.headers.authorization?.replace(/^Bearer\s+/i, '').trim();
  if (bearerToken) {
    return bearerToken;
  }

  return getCookieValue(req.headers.cookie, ADMIN_COOKIE_NAME);
};

export const authenticateAdmin = async (
  req: AuthRequest,
  res: Response,
  next: NextFunction
) => {
  try {
    const token = extractAuthToken(req);

    if (!token) {
      return res.status(401).json({ error: 'Authentication required' });
    }

    // Verify JWT integrity
    jwt.verify(token, config.jwt.secret);

    // Verify session in database (prevents using revoked tokens or logged-out sessions)
    const sessionResult = await query(
      `SELECT s.*, u.email, u.full_name, u.role, u.is_active
       FROM admin_sessions s
       JOIN users u ON s.user_id = u.id
       WHERE s.token = $1 AND s.expires_at > NOW()`,
      [token]
    );

    if (sessionResult.rows.length === 0) {
      return res.status(401).json({ error: 'Invalid or expired session' });
    }

    const session = sessionResult.rows[0];

    if (!session.is_active) {
      return res.status(403).json({ error: 'Account is deactivated' });
    }

    // Update last activity
    await query(
      'UPDATE admin_sessions SET last_activity_at = NOW() WHERE token = $1',
      [token]
    );

    req.user = {
      userId: session.user_id,
      email: session.email,
      fullName: session.full_name,
      role: session.role,
    };
    req.authToken = token;

    next();
  } catch (error) {
    logger.error('Authentication error:', error);
    return res.status(401).json({ error: 'Invalid token' });
  }
};

export const requireRole = (roles: string[]) => {
  return (req: AuthRequest, res: Response, next: NextFunction) => {
    if (!req.user) {
      return res.status(401).json({ error: 'Authentication required' });
    }

    if (!roles.includes(req.user.role)) {
      return res.status(403).json({ error: 'Insufficient permissions' });
    }

    next();
  };
};

// Variadic wrapper used by route layers that pass roles as individual args.
export const requireAnyRole = (...roles: string[]) => requireRole(roles);


==================================================
FILE: src/middleware/csrf.ts
==================================================

import { Request, Response, NextFunction } from 'express';
import crypto from 'crypto';

const CSRF_COOKIE_NAME = 'csrf_token';
const CSRF_HEADER_NAME = 'x-csrf-token';
const CSRF_TOKEN_BYTES = 32;

const isSafeMethod = (method: string): boolean => ['GET', 'HEAD', 'OPTIONS'].includes(method.toUpperCase());

const readCookie = (req: Request, key: string): string | undefined => {
  const cookieMap = (req as Request & { cookies?: Record<string, string> }).cookies;
  if (cookieMap?.[key]) {
    return cookieMap[key];
  }

  const rawCookie = req.headers.cookie;
  if (!rawCookie) {
    return undefined;
  }

  const item = rawCookie
    .split(';')
    .map((entry) => entry.trim())
    .find((entry) => entry.startsWith(`${key}=`));

  return item ? decodeURIComponent(item.slice(key.length + 1)) : undefined;
};

export const issueCsrfToken = (req: Request, res: Response): void => {
  const token = crypto.randomBytes(CSRF_TOKEN_BYTES).toString('hex');

  res.cookie(CSRF_COOKIE_NAME, token, {
    httpOnly: true,
    secure: process.env.NODE_ENV === 'production',
    sameSite: 'strict',
    path: '/',
    maxAge: 24 * 60 * 60 * 1000,
  });

  res.json({ csrfToken: token });
};

export const csrfProtection = (req: Request, res: Response, next: NextFunction): void => {
  if (isSafeMethod(req.method)) {
    next();
    return;
  }

  const cookieToken = readCookie(req, CSRF_COOKIE_NAME);
  const headerToken = req.header(CSRF_HEADER_NAME);

  if (!cookieToken || !headerToken || cookieToken !== headerToken) {
    res.status(403).json({ error: 'Invalid CSRF token' });
    return;
  }

  next();
};


==================================================
FILE: src/middleware/errorHandler.ts
==================================================

/**
 * File: src/middleware/errorHandler.ts
 * Purpose: Global error handling middleware
 */

import { Request, Response, NextFunction } from 'express';
import { logger } from '../utils/logger';

export interface AppError extends Error {
  statusCode?: number;
  isOperational?: boolean;
}

export const errorHandler = (
  err: AppError,
  req: Request,
  res: Response,
  next: NextFunction
) => {
  logger.error('Error:', {
    message: err.message,
    stack: err.stack,
    url: req.url,
    method: req.method,
  });

  const statusCode = err.statusCode || 500;
  const message = err.isOperational ? err.message : 'Internal server error';

  res.status(statusCode).json({
    error: message,
    ...(process.env.NODE_ENV === 'development' && { stack: err.stack }),
  });
};

export const notFoundHandler = (req: Request, res: Response) => {
  res.status(404).json({ error: 'Route not found' });
};


==================================================
FILE: src/middleware/rbac.ts
==================================================

import { NextFunction, Response } from 'express';
import { AuthRequest } from './auth';

const ROLE_LEVEL: Record<string, number> = {
  creator: 10,
  admin: 20,
  super_admin: 30,
};

const resolveRoleLevel = (role: string): number => ROLE_LEVEL[role] ?? -1;

export const requireRole = (...allowedRoles: string[]) => {
  return (req: AuthRequest, res: Response, next: NextFunction) => {
    if (!req.user) {
      return res.status(401).json({ error: 'Authentication required' });
    }

    const currentRole = req.user.role;
    const currentLevel = resolveRoleLevel(currentRole);
    const minAllowedLevel = Math.min(...allowedRoles.map(resolveRoleLevel));

    // Hierarchical RBAC:
    // super_admin >= admin >= creator
    const isAllowedByHierarchy = Number.isFinite(minAllowedLevel) && currentLevel >= minAllowedLevel;
    const isAllowedByDirectMatch = allowedRoles.includes(currentRole);

    if (!isAllowedByHierarchy && !isAllowedByDirectMatch) {
      return res.status(403).json({
        error: 'Insufficient permissions',
        required: allowedRoles,
        current: currentRole,
      });
    }

    next();
  };
};


==================================================
FILE: src/middleware/responseSession.ts
==================================================

import { NextFunction, Request, Response } from 'express';
import { query } from '../config/database';
import { logger } from '../utils/logger';

const INVALID_SESSION_MESSAGE = 'Invalid session';

export const requireResponseSession = async (req: Request, res: Response, next: NextFunction) => {
  const { responseId } = req.params;
  const sessionToken = req.header('x-session-token');

  if (!sessionToken) {
    return res.status(401).json({ success: false, error: INVALID_SESSION_MESSAGE });
  }

  try {
    const result = await query(
      `SELECT id
       FROM assessment_responses
       WHERE id = $1
         AND session_token = $2`,
      [responseId, sessionToken]
    );

    if (result.rows.length === 0) {
      return res.status(403).json({ success: false, error: INVALID_SESSION_MESSAGE });
    }

    return next();
  } catch (error) {
    logger.error('Failed to validate participant response session', error);
    return next(error);
  }
};


==================================================
FILE: src/middleware/validation.ts
==================================================

/**
 * File: src/middleware/validation.ts
 * Purpose: Request validation middleware using Zod
 */

import { Request, Response, NextFunction } from 'express';
import { z, ZodError, ZodTypeAny } from 'zod';

export const validate = (schema: ZodTypeAny) => {
  return async (req: Request, res: Response, next: NextFunction) => {
    try {
      const envelope = {
        body: req.body,
        query: req.query,
        params: req.params,
      };

      let parsed: unknown;
      try {
        parsed = await schema.parseAsync(envelope);
      } catch {
        // Backward compatibility: allow existing body-only schemas.
        parsed = await schema.parseAsync(req.body);
      }

      if (parsed && typeof parsed === 'object') {
        const maybeEnvelope = parsed as { body?: unknown; query?: unknown; params?: unknown };
        if (typeof maybeEnvelope.body !== 'undefined') {
          req.body = maybeEnvelope.body;
        }
        if (typeof maybeEnvelope.query !== 'undefined') {
          req.query = maybeEnvelope.query as Request['query'];
        }
        if (typeof maybeEnvelope.params !== 'undefined') {
          req.params = maybeEnvelope.params as Request['params'];
        }
      }

      next();
    } catch (error) {
      if (error instanceof ZodError) {
        return res.status(400).json({
          error: 'Validation failed',
          details: error.errors.map((e) => ({
            field: e.path.join('.'),
            message: e.message,
          })),
        });
      }
      next(error);
    }
  };
};

export const schemas = {
  login: z.object({
    email: z.string().email(),
    password: z.string().min(6),
  }),

  participant: z.object({
    email: z.string().email(),
    full_name: z.string().min(2),
    company_name: z.string().min(2),
    job_title: z.string().optional(),
    industry: z.string().optional(),
    phone: z.string().optional(),
    company_size: z.string().optional(),
    country: z.string().optional(),
    consent_given: z.boolean(),
  }),

  topicResponse: z.object({
    topic_id: z.string().uuid(),
    current_rating: z.number().min(1).max(5),
    target_rating: z.number().min(1).max(5),
    time_spent_seconds: z.number().optional(),
    notes: z.string().optional(),
  }),
};


==================================================
FILE: src/repositories/AssessmentRepository.ts
==================================================

import { query } from '../config/database';

export interface AssessmentFilters {
  isPublished?: boolean;
  isActive?: boolean;
}

export interface GuardedTopicResponseInput {
  responseId: string;
  topicId: string;
  currentRating: number;
  targetRating: number;
  gap: number;
  normalizedGap: number;
  timeSpentSeconds: number;
  notes: string | null;
}

export class AssessmentRepository {
  async findAll(filters?: AssessmentFilters) {
    let sql = `
      SELECT
        a.*,
        u.full_name AS created_by_name,
        (SELECT COUNT(*) FROM dimensions WHERE assessment_id = a.id) AS dimension_count,
        (SELECT COUNT(t.*) FROM topics t JOIN dimensions d ON t.dimension_id = d.id WHERE d.assessment_id = a.id) AS topic_count,
        (SELECT COUNT(*) FROM assessment_responses WHERE assessment_id = a.id) AS response_count
      FROM assessments a
      LEFT JOIN users u ON a.created_by = u.id
      WHERE 1=1
    `;

    const params: unknown[] = [];
    let paramIndex = 1;

    if (typeof filters?.isPublished === 'boolean') {
      sql += ` AND a.is_published = $${paramIndex++}`;
      params.push(filters.isPublished);
    }

    if (typeof filters?.isActive === 'boolean') {
      sql += ` AND a.is_active = $${paramIndex++}`;
      params.push(filters.isActive);
    }

    sql += ' ORDER BY a.created_at DESC';
    const result = await query(sql, params);
    return result.rows;
  }

  async findById(id: string) {
    const result = await query(
      `SELECT 
        a.*,
        json_agg(
          json_build_object(
            'id', d.id,
            'key', d.dimension_key,
            'title', d.title,
            'description', d.description,
            'category', d.category,
            'order', d.order_index,
            'topics', (
              SELECT json_agg(
                json_build_object(
                  'id', t.id,
                  'key', t.topic_key,
                  'label', t.label,
                  'prompt', t.prompt,
                  'help_text', t.help_text,
                  'order', t.order_index,
                  'level_1_label', t.level_1_label,
                  'level_2_label', t.level_2_label,
                  'level_3_label', t.level_3_label,
                  'level_4_label', t.level_4_label,
                  'level_5_label', t.level_5_label
                ) ORDER BY t.order_index
              )
              FROM topics t
              WHERE t.dimension_id = d.id
            )
          ) ORDER BY d.order_index
        ) FILTER (WHERE d.id IS NOT NULL) AS dimensions
      FROM assessments a
      LEFT JOIN dimensions d ON a.id = d.assessment_id
      WHERE a.id = $1
      GROUP BY a.id`,
      [id]
    );

    return result.rows[0] || null;
  }

  async delete(id: string) {
    const result = await query('DELETE FROM assessments WHERE id = $1', [id]);
    return (result.rowCount || 0) > 0;
  }

  async upsertTopicResponseForAssessment(input: GuardedTopicResponseInput): Promise<number> {
    const result = await query(
      `INSERT INTO topic_responses (
         response_id, topic_id, current_rating, target_rating,
         gap, normalized_gap, time_spent_seconds, notes, answered_at
       )
       SELECT
         $1, t.id, $3, $4, $5, $6, $7, $8, NOW()
       FROM assessment_responses ar
       JOIN topics t ON t.id = $2
       JOIN dimensions d ON d.id = t.dimension_id
       WHERE ar.id = $1
         AND d.assessment_id = ar.assessment_id
       ON CONFLICT (response_id, topic_id) DO UPDATE
         SET current_rating = EXCLUDED.current_rating,
             target_rating = EXCLUDED.target_rating,
             gap = EXCLUDED.gap,
             normalized_gap = EXCLUDED.normalized_gap,
             time_spent_seconds = EXCLUDED.time_spent_seconds,
             notes = EXCLUDED.notes,
             answered_at = EXCLUDED.answered_at`,
      [
        input.responseId,
        input.topicId,
        input.currentRating,
        input.targetRating,
        input.gap,
        input.normalizedGap,
        input.timeSpentSeconds,
        input.notes,
      ]
    );

    return result.rowCount || 0;
  }
}


==================================================
FILE: src/routes/admin/analytics.ts
==================================================

/**
 * File: src/routes/admin/analytics.ts
 * Purpose: Advanced analytics and data export
 */

import { Router } from 'express';
import { authenticateAdmin } from '../../middleware/auth';
import { requireRole } from '../../middleware/rbac';
import { query } from '../../config/database';
import { z } from 'zod';
import { validate } from '../../middleware/validation';

const router = Router();

const exportResponsesSchema = z.object({
  query: z.object({
    assessment_id: z.string().uuid().optional(),
  }),
});

router.get('/', authenticateAdmin, requireRole('admin'), async (req, res, next) => {
  try {
    const dimensionPerformance = await query(`
      SELECT 
        d.title,
        d.category,
        AVG(CASE WHEN cp.dimension_score::text = 'NaN' THEN NULL ELSE cp.dimension_score END) as avg_score,
        AVG(cp.dimension_gap) as avg_gap,
        MIN(CASE WHEN cp.dimension_score::text = 'NaN' THEN NULL ELSE cp.dimension_score END) as min_score,
        MAX(CASE WHEN cp.dimension_score::text = 'NaN' THEN NULL ELSE cp.dimension_score END) as max_score
      FROM computed_priorities cp
      JOIN dimensions d ON cp.dimension_id = d.id
      JOIN assessment_responses ar ON cp.response_id = ar.id
      WHERE ar.status = 'completed'
      GROUP BY d.id, d.title, d.category, d.order_index
      ORDER BY d.order_index
    `);

    const responseTrends = await query(`
        SELECT 
            DATE(completed_at) as date,
            COUNT(*) as count,
            AVG(CASE WHEN overall_score::text = 'NaN' THEN NULL ELSE overall_score END) as avg_score
        FROM assessment_responses
        WHERE status = 'completed'
        AND completed_at >= CURRENT_DATE - INTERVAL '30 days'
        GROUP BY DATE(completed_at)
        ORDER BY date
    `);

    const scoreDistribution = await query(`
      SELECT
        bucket,
        COUNT(*) as count
      FROM (
        SELECT
          CASE
            WHEN overall_score >= 1 AND overall_score < 2 THEN '1-2'
            WHEN overall_score >= 2 AND overall_score < 3 THEN '2-3'
            WHEN overall_score >= 3 AND overall_score < 4 THEN '3-4'
            WHEN overall_score >= 4 AND overall_score <= 5 THEN '4-5'
            ELSE 'other'
          END AS bucket
        FROM assessment_responses
        WHERE status = 'completed'
          AND overall_score::text <> 'NaN'
      ) s
      WHERE bucket <> 'other'
      GROUP BY bucket
    `);

    const orderedBuckets = ['1-2', '2-3', '3-4', '4-5'];
    const scoreDistributionMap = new Map<string, number>();
    for (const row of scoreDistribution.rows) {
      scoreDistributionMap.set(row.bucket, Number(row.count));
    }
    const normalizedDistribution = orderedBuckets.map((bucket) => ({
      range: bucket,
      label: bucket,
      count: scoreDistributionMap.get(bucket) ?? 0,
    }));

    res.json({
      dimension_performance: dimensionPerformance.rows,
      response_trends: responseTrends.rows,
      score_distribution: normalizedDistribution,
      generated_at: new Date()
    });
  } catch (error) {
    next(error);
  }
});

router.get(
  '/export/responses',
  authenticateAdmin,
  requireRole('admin'),
  validate(exportResponsesSchema),
  async (req, res, next) => {
  try {
    const { assessment_id } = req.query as unknown as z.infer<typeof exportResponsesSchema>['query'];
    
    let queryStr = `
      SELECT 
        ar.id as response_id,
        p.full_name,
        p.email,
        p.company_name,
        ar.overall_score,
        ar.overall_gap,
        ar.completed_at
      FROM assessment_responses ar
      JOIN participants p ON ar.participant_id = p.id
      WHERE ar.status = 'completed'
    `;
    
    const params: any[] = [];
    if (assessment_id) {
      queryStr += ` AND ar.assessment_id = $1`;
      params.push(assessment_id);
    }
    
    queryStr += ` ORDER BY ar.completed_at DESC`;
    
    const result = await query(queryStr, params);
    
    // In a real app, we would generate a CSV file here
    // For now, return JSON which the frontend can convert
    res.json(result.rows);
  } catch (error) {
    next(error);
  }
}
);

router.get('/dimension-breakdown', authenticateAdmin, requireRole('admin'), async (req, res, next) => {
  try {
    // Aggregated scores by dimension across all responses
    const result = await query(`
      SELECT 
        d.title,
        d.category,
        AVG(CASE WHEN cp.dimension_score::text = 'NaN' THEN NULL ELSE cp.dimension_score END) as avg_score,
        AVG(cp.dimension_gap) as avg_gap,
        MIN(CASE WHEN cp.dimension_score::text = 'NaN' THEN NULL ELSE cp.dimension_score END) as min_score,
        MAX(CASE WHEN cp.dimension_score::text = 'NaN' THEN NULL ELSE cp.dimension_score END) as max_score
      FROM computed_priorities cp
      JOIN dimensions d ON cp.dimension_id = d.id
      JOIN assessment_responses ar ON cp.response_id = ar.id
      WHERE ar.status = 'completed'
      GROUP BY d.id, d.title, d.category, d.order_index
      ORDER BY d.order_index
    `);
    
    res.json(result.rows);
  } catch (error) {
    next(error);
  }
});

export default router;


==================================================
FILE: src/routes/admin/assessments.ts
==================================================

/**
 * File: src/routes/admin/assessments.ts
 * Purpose: Manage assessments, dimensions, and topics
 */

import { Router } from 'express';
import { authenticateAdmin, AuthRequest } from '../../middleware/auth';
import { query, getClient } from '../../config/database';
import { z } from 'zod';
import { validate } from '../../middleware/validation';
import { requireRole } from '../../middleware/rbac';
import { csrfProtection } from '../../middleware/csrf';
import { AssessmentRepository } from '../../repositories/AssessmentRepository';

const router = Router();
const assessmentRepository = new AssessmentRepository();

const createAssessmentSchema = z.object({
  title: z.string().min(3),
  description: z.string().optional(),
  version: z.number().int().positive(),
  dimensions: z.array(
    z.object({
      key: z.string(),
      title: z.string(),
      description: z.string().optional(),
      category: z.string(),
      order: z.number(),
      topics: z.array(
        z.object({
          key: z.string(),
          label: z.string(),
          prompt: z.string(),
          help_text: z.string().optional(),
          order: z.number(),
        })
      ),
    })
  ),
});

const upsertTopicSchema = z.object({
  id: z.string().uuid().optional(),
  key: z.string(),
  label: z.string(),
  prompt: z.string(),
  help_text: z.string().optional(),
  order: z.number(),
  level_1_label: z.string().max(500).nullable().optional(),
  level_2_label: z.string().max(500).nullable().optional(),
  level_3_label: z.string().max(500).nullable().optional(),
  level_4_label: z.string().max(500).nullable().optional(),
  level_5_label: z.string().max(500).nullable().optional(),
});

const upsertDimensionSchema = z.object({
  id: z.string().uuid().optional(),
  key: z.string(),
  title: z.string(),
  description: z.string().optional(),
  category: z.string(),
  order: z.number(),
  topics: z.array(upsertTopicSchema),
});

const updateAssessmentSchema = z.object({
  params: z.object({
    id: z.string().uuid(),
  }),
  body: z.object({
    title: z.string().min(3),
    description: z.string().optional(),
    version: z.number().int().positive(),
    dimensions: z.array(upsertDimensionSchema),
  }),
});

const assessmentIdSchema = z.object({
  params: z.object({
    id: z.string().uuid(),
  }),
});

const publishAssessmentSchema = z.object({
  params: z.object({
    id: z.string().uuid(),
  }),
  body: z.object({
    is_published: z.boolean(),
  }),
});

router.get('/', authenticateAdmin, requireRole('admin', 'creator'), async (req, res, next) => {
  try {
    const assessments = await assessmentRepository.findAll();
    res.json(assessments);
  } catch (error) {
    next(error);
  }
});

router.get(
  '/:id',
  authenticateAdmin,
  requireRole('admin', 'creator'),
  validate(assessmentIdSchema),
  async (req, res, next) => {
  try {
    const { id } = req.params;
    const assessment = await assessmentRepository.findById(id);
    if (!assessment) {
      return res.status(404).json({ error: 'Assessment not found' });
    }

    res.json(assessment);
  } catch (error) {
    next(error);
  }
}
);

router.post(
  '/',
  authenticateAdmin,
  requireRole('admin', 'creator'),
  csrfProtection,
  validate(createAssessmentSchema),
  async (req: AuthRequest, res, next) => {
    const { title, description, version, dimensions } = req.body;
    const userId = req.user?.userId;

    const client = await getClient();

    try {
      await client.query('BEGIN');

      const assessmentResult = await client.query(
        `INSERT INTO assessments (title, description, version, created_by, is_active)
         VALUES ($1, $2, $3, $4, false)
         RETURNING id`,
        [title, description || null, version, userId]
      );

      const assessmentId = assessmentResult.rows[0].id;

      // Batch dimension insert to avoid per-dimension round-trips.
      const dimensionValues: string[] = [];
      const dimensionParams: unknown[] = [];
      let paramIndex = 1;

      for (const dim of dimensions) {
        dimensionValues.push(
          `($${paramIndex++}, $${paramIndex++}, $${paramIndex++}, $${paramIndex++}, $${paramIndex++}, $${paramIndex++})`
        );
        dimensionParams.push(
          assessmentId,
          dim.key,
          dim.title,
          dim.description || null,
          dim.category,
          dim.order
        );
      }

      const dimensionsResult = await client.query(
        `INSERT INTO dimensions (assessment_id, dimension_key, title, description, category, order_index)
         VALUES ${dimensionValues.join(', ')}
         RETURNING id, dimension_key`,
        dimensionParams
      );

      const dimensionIdMap = new Map<string, string>();
      dimensionsResult.rows.forEach((row: { dimension_key: string; id: string }) => {
        dimensionIdMap.set(row.dimension_key, row.id);
      });

      // Batch topic insert to avoid per-topic round-trips.
      const topicValues: string[] = [];
      const topicParams: unknown[] = [];
      paramIndex = 1;

      for (const dim of dimensions) {
        const dimensionId = dimensionIdMap.get(dim.key);
        if (!dimensionId) {
          throw new Error(`Dimension mapping missing for key: ${dim.key}`);
        }

        for (const topic of dim.topics) {
          topicValues.push(
            `($${paramIndex++}, $${paramIndex++}, $${paramIndex++}, $${paramIndex++}, $${paramIndex++}, $${paramIndex++})`
          );
          topicParams.push(
            dimensionId,
            topic.key,
            topic.label,
            topic.prompt,
            topic.help_text || null,
            topic.order
          );
        }
      }

      if (topicValues.length > 0) {
        await client.query(
          `INSERT INTO topics (dimension_id, topic_key, label, prompt, help_text, order_index)
           VALUES ${topicValues.join(', ')}`,
          topicParams
        );
      }

      await client.query(
        `INSERT INTO audit_logs (entity_type, entity_id, action, user_id, ip_address)
         VALUES ('assessment', $1, 'created', $2, $3)`,
        [assessmentId, userId, req.ip]
      );

      await client.query('COMMIT');

      res.status(201).json({
        id: assessmentId,
        message: 'Assessment created successfully',
      });
    } catch (error) {
      await client.query('ROLLBACK');
      next(error);
    } finally {
      client.release();
    }
  }
);

router.patch(
  '/:id/publish',
  authenticateAdmin,
  requireRole('admin', 'creator'),
  csrfProtection,
  validate(publishAssessmentSchema),
  async (req: AuthRequest, res, next) => {
  const { id } = req.params;
  const { is_published } = req.body;
  const userId = req.user?.userId;

  const client = await getClient();

  try {
    await client.query('BEGIN');

    const existsResult = await client.query(
      'SELECT id FROM assessments WHERE id = $1 FOR UPDATE',
      [id]
    );

    if (existsResult.rows.length === 0) {
      await client.query('ROLLBACK');
      return res.status(404).json({ error: 'Assessment not found' });
    }

    if (is_published) {
      await client.query(
        `UPDATE assessments
         SET is_published = false,
             is_active = false
         WHERE is_published = true
           AND id <> $1`,
        [id]
      );
    }

    const updateResult = await client.query(
      `UPDATE assessments 
       SET is_published = $1, 
           is_active = $1,
           published_at = CASE WHEN $1 THEN NOW() ELSE published_at END
       WHERE id = $2`,
      [is_published, id]
    );

    if (updateResult.rowCount === 0) {
      await client.query('ROLLBACK');
      return res.status(404).json({ error: 'Assessment not found' });
    }

    await client.query(
      `INSERT INTO audit_logs (entity_type, entity_id, action, user_id, ip_address)
       VALUES ('assessment', $1, $2, $3, $4)`,
      [id, is_published ? 'published' : 'unpublished', userId, req.ip]
    );

    await client.query('COMMIT');

    res.json({
      message: `Assessment ${is_published ? 'published' : 'unpublished'} successfully`,
    });
  } catch (error: any) {
    await client.query('ROLLBACK');
    if (error?.code === '23505') {
      return res.status(409).json({ error: 'Only one assessment can be published at a time' });
    }
    next(error);
  } finally {
    client.release();
  }
}
);

router.put(
  '/:id',
  authenticateAdmin,
  requireRole('admin', 'creator'),
  csrfProtection,
  validate(updateAssessmentSchema),
  async (req: AuthRequest, res, next) => {
    const { id: assessmentId } = req.params;
    const { title, description, version, dimensions } = req.body;
    const userId = req.user?.userId;
    const client = await getClient();

    try {
      await client.query('BEGIN');

      const hasIsActiveResult = await client.query(
        `SELECT EXISTS (
           SELECT 1
           FROM information_schema.columns
           WHERE table_name = 'dimensions' AND column_name = 'is_active'
         ) AS dimensions_has_is_active,
         EXISTS (
           SELECT 1
           FROM information_schema.columns
           WHERE table_name = 'topics' AND column_name = 'is_active'
         ) AS topics_has_is_active`
      );

      const { dimensions_has_is_active, topics_has_is_active } = hasIsActiveResult.rows[0] as {
        dimensions_has_is_active: boolean;
        topics_has_is_active: boolean;
      };

      const assessmentUpdate = await client.query(
        `UPDATE assessments
         SET title = $1,
             description = $2,
             version = $3,
             updated_at = NOW()
         WHERE id = $4
         RETURNING id`,
        [title, description || null, version, assessmentId]
      );

      if (assessmentUpdate.rows.length === 0) {
        await client.query('ROLLBACK');
        return res.status(404).json({ error: 'Assessment not found' });
      }

      const dimensionPayload = dimensions.map((dimension: {
        id?: string;
        key: string;
        title: string;
        description?: string;
        category: string;
        order: number;
        topics: Array<{
          id?: string;
          key: string;
          label: string;
          prompt: string;
          help_text?: string;
          order: number;
          level_1_label?: string | null;
          level_2_label?: string | null;
          level_3_label?: string | null;
          level_4_label?: string | null;
          level_5_label?: string | null;
        }>;
      }) => ({
        id: dimension.id ?? null,
        dimension_key: dimension.key,
        title: dimension.title,
        description: dimension.description ?? null,
        category: dimension.category,
        order_index: dimension.order,
      }));

      const upsertedDimensionsResult = await client.query(
        `WITH incoming AS (
           SELECT *
           FROM jsonb_to_recordset($1::jsonb) AS d(
             id uuid,
             dimension_key text,
             title text,
             description text,
             category text,
             order_index integer
           )
         )
         INSERT INTO dimensions (
           id, assessment_id, dimension_key, title, description, category, order_index
         )
         SELECT
           COALESCE(incoming.id, gen_random_uuid()),
           $2::uuid,
           incoming.dimension_key,
           incoming.title,
           incoming.description,
           incoming.category,
           incoming.order_index
         FROM incoming
         ON CONFLICT (assessment_id, dimension_key)
         DO UPDATE SET
           title = EXCLUDED.title,
           description = EXCLUDED.description,
           category = EXCLUDED.category,
           order_index = EXCLUDED.order_index,
           updated_at = NOW()
         RETURNING id, dimension_key`,
        [JSON.stringify(dimensionPayload), assessmentId]
      );

      const keptDimensionIds = upsertedDimensionsResult.rows.map((row: { id: string }) => row.id);

      const topicPayload = dimensions.flatMap((dimension: {
        key: string;
        topics: Array<{
          id?: string;
          key: string;
          label: string;
          prompt: string;
          help_text?: string;
          order: number;
          level_1_label?: string | null;
          level_2_label?: string | null;
          level_3_label?: string | null;
          level_4_label?: string | null;
          level_5_label?: string | null;
        }>;
      }) =>
        dimension.topics.map((topic) => ({
          id: topic.id ?? null,
          dimension_key: dimension.key,
          topic_key: topic.key,
          label: topic.label,
          prompt: topic.prompt,
          help_text: topic.help_text ?? null,
          order_index: topic.order,
          level_1_label: topic.level_1_label ?? null,
          level_2_label: topic.level_2_label ?? null,
          level_3_label: topic.level_3_label ?? null,
          level_4_label: topic.level_4_label ?? null,
          level_5_label: topic.level_5_label ?? null,
        }))
      );

      let keptTopicIds: string[] = [];

      if (topicPayload.length > 0) {
        const upsertedTopicsResult = await client.query(
          `WITH incoming AS (
             SELECT *
             FROM jsonb_to_recordset($1::jsonb) AS t(
               id uuid,
               dimension_key text,
               topic_key text,
               label text,
               prompt text,
               help_text text,
               order_index integer,
               level_1_label text,
               level_2_label text,
               level_3_label text,
               level_4_label text,
               level_5_label text
             )
           ),
           resolved AS (
             SELECT
               COALESCE(incoming.id, gen_random_uuid()) AS id,
               d.id AS dimension_id,
               incoming.topic_key,
               incoming.label,
               incoming.prompt,
               incoming.help_text,
               incoming.order_index,
               incoming.level_1_label,
               incoming.level_2_label,
               incoming.level_3_label,
               incoming.level_4_label,
               incoming.level_5_label
             FROM incoming
             JOIN dimensions d
               ON d.assessment_id = $2::uuid
              AND d.dimension_key = incoming.dimension_key
           )
           INSERT INTO topics (
             id, dimension_id, topic_key, label, prompt, help_text, order_index,
             level_1_label, level_2_label, level_3_label, level_4_label, level_5_label
           )
           SELECT
             id, dimension_id, topic_key, label, prompt, help_text, order_index,
             level_1_label, level_2_label, level_3_label, level_4_label, level_5_label
           FROM resolved
           ON CONFLICT (dimension_id, topic_key)
           DO UPDATE SET
             label = EXCLUDED.label,
             prompt = EXCLUDED.prompt,
             help_text = EXCLUDED.help_text,
             order_index = EXCLUDED.order_index,
             level_1_label = EXCLUDED.level_1_label,
             level_2_label = EXCLUDED.level_2_label,
             level_3_label = EXCLUDED.level_3_label,
             level_4_label = EXCLUDED.level_4_label,
             level_5_label = EXCLUDED.level_5_label,
             updated_at = NOW()
           RETURNING id`,
          [JSON.stringify(topicPayload), assessmentId]
        );

        keptTopicIds = upsertedTopicsResult.rows.map((row: { id: string }) => row.id);
      }

      if (topics_has_is_active && keptTopicIds.length > 0) {
        await client.query(
          `UPDATE topics
           SET is_active = true, updated_at = NOW()
           WHERE id = ANY($1::uuid[])`,
          [keptTopicIds]
        );
      }

      if (topics_has_is_active) {
        await client.query(
          `UPDATE topics t
           SET is_active = false, updated_at = NOW()
           FROM dimensions d
           WHERE t.dimension_id = d.id
             AND d.assessment_id = $1
             AND (cardinality($2::uuid[]) = 0 OR t.id <> ALL($2::uuid[]))`,
          [assessmentId, keptTopicIds]
        );
      } else {
        await client.query(
          `DELETE FROM topics t
           USING dimensions d
           WHERE t.dimension_id = d.id
             AND d.assessment_id = $1
             AND (cardinality($2::uuid[]) = 0 OR t.id <> ALL($2::uuid[]))`,
          [assessmentId, keptTopicIds]
        );
      }

      if (dimensions_has_is_active) {
        if (keptDimensionIds.length > 0) {
          await client.query(
            `UPDATE dimensions
             SET is_active = true, updated_at = NOW()
             WHERE id = ANY($1::uuid[])`,
            [keptDimensionIds]
          );
        }
        await client.query(
          `UPDATE dimensions
           SET is_active = false, updated_at = NOW()
           WHERE assessment_id = $1
             AND (cardinality($2::uuid[]) = 0 OR id <> ALL($2::uuid[]))`,
          [assessmentId, keptDimensionIds]
        );
      } else {
        await client.query(
          `DELETE FROM dimensions
           WHERE assessment_id = $1
             AND (cardinality($2::uuid[]) = 0 OR id <> ALL($2::uuid[]))`,
          [assessmentId, keptDimensionIds]
        );
      }

      await client.query(
        `INSERT INTO audit_logs (entity_type, entity_id, action, user_id, ip_address)
         VALUES ('assessment', $1, 'updated', $2, $3)`,
        [assessmentId, userId, req.ip]
      );

      await client.query('COMMIT');
      res.json({ id: assessmentId, message: 'Assessment updated successfully' });
    } catch (error) {
      await client.query('ROLLBACK');
      next(error);
    } finally {
      client.release();
    }
  }
);

router.delete(
  '/:id',
  authenticateAdmin,
  requireRole('admin', 'creator'),
  csrfProtection,
  validate(assessmentIdSchema),
  async (req: AuthRequest, res, next) => {
  const { id } = req.params;
  const userId = req.user?.userId;

  try {
    const hasResponses = await query(
      'SELECT COUNT(*) as count FROM assessment_responses WHERE assessment_id = $1',
      [id]
    );

    if (parseInt(hasResponses.rows[0].count) > 0) {
      return res.status(400).json({
        error: 'Cannot delete assessment with existing responses',
        response_count: hasResponses.rows[0].count,
      });
    }

    await assessmentRepository.delete(id);

    await query(
      `INSERT INTO audit_logs (entity_type, entity_id, action, user_id, ip_address)
       VALUES ('assessment', $1, 'deleted', $2, $3)`,
      [id, userId, req.ip]
    );

    res.json({ message: 'Assessment deleted successfully' });
  } catch (error) {
    next(error);
  }
}
);

export default router;


==================================================
FILE: src/routes/admin/auth.ts
==================================================

/**
 * File: src/routes/admin/auth.ts
 * Purpose: Admin authentication endpoints
 */

import { Router } from 'express';
import { AuthService, AUTH_FAIL_MSG } from '../../services/authService';
import { authenticateAdmin, AuthRequest, ADMIN_COOKIE_NAME } from '../../middleware/auth';
import { validate, schemas } from '../../middleware/validation';
import { config } from '../../config/env';
import { csrfProtection } from '../../middleware/csrf';

const router = Router();
const ADMIN_COOKIE_MAX_AGE_MS = 24 * 60 * 60 * 1000;

const buildCookieOptions = () => ({
  httpOnly: true,
  secure: config.nodeEnv === 'production',
  sameSite: 'strict' as const,
  path: '/',
  maxAge: ADMIN_COOKIE_MAX_AGE_MS,
  ...(process.env.COOKIE_DOMAIN ? { domain: process.env.COOKIE_DOMAIN } : {}),
});

const buildClearCookieOptions = () => ({
  httpOnly: true,
  secure: config.nodeEnv === 'production',
  sameSite: 'strict' as const,
  path: '/',
  ...(process.env.COOKIE_DOMAIN ? { domain: process.env.COOKIE_DOMAIN } : {}),
});

router.post('/login', validate(schemas.login), async (req, res, next) => {
  try {
    const { email, password } = req.body;
    const result = await AuthService.login(
      email,
      password,
      req.ip || '',
      req.headers['user-agent'] || ''
    );

    res.cookie(ADMIN_COOKIE_NAME, result.token, buildCookieOptions());
    res.json({
      success: true,
      user: {
        userId: result.user.id,
        email: result.user.email,
        fullName: result.user.full_name,
        role: result.user.role,
      },
    });
  } catch (error: unknown) {
    res.status(401).json({ success: false, error: AUTH_FAIL_MSG });
  }
});

router.post('/logout', authenticateAdmin, csrfProtection, async (req: AuthRequest, res, next) => {
  try {
    const token = req.authToken;
    if (token) {
      await AuthService.logout(token);
    }
    res.clearCookie(ADMIN_COOKIE_NAME, buildClearCookieOptions());
    res.json({ success: true });
  } catch (error) {
    next(error);
  }
});

router.get('/verify', authenticateAdmin, async (req: AuthRequest, res) => {
  if (!req.user) {
    return res.status(401).json({ valid: false, error: 'Authentication required' });
  }

  res.json({
    valid: true,
    user: req.user,
  });
});

export default router;


==================================================
FILE: src/routes/admin/dashboard.ts
==================================================

/**
 * File: src/routes/admin/dashboard.ts
 * Purpose: Admin dashboard statistics and analytics
 */

import { Router } from 'express';
import { authenticateAdmin } from '../../middleware/auth';
import { requireRole } from '../../middleware/rbac';
import { query } from '../../config/database';
import { z } from 'zod';
import { validate } from '../../middleware/validation';

const router = Router();

const statsQuerySchema = z.object({
  query: z.object({
    range: z.enum(['all', '30d', '90d', '1y']).default('all'),
  }),
});

router.get('/stats', authenticateAdmin, requireRole('admin'), validate(statsQuerySchema), async (req, res, next) => {
  try {
    const { range } = req.query as unknown as z.infer<typeof statsQuerySchema>['query'];

    const stats = await query(
      `
      WITH time_window AS (
        SELECT CASE 
          WHEN $1 = '30d' THEN INTERVAL '30 days'
          WHEN $1 = '90d' THEN INTERVAL '90 days'
          WHEN $1 = '1y' THEN INTERVAL '1 year'
          ELSE NULL
        END AS window_size
      ),
      filtered_responses AS (
        SELECT ar.*
        FROM assessment_responses ar
        CROSS JOIN time_window tw
        WHERE tw.window_size IS NULL OR ar.completed_at >= NOW() - tw.window_size
      ),
      overview_stats AS (
        SELECT
          (SELECT COUNT(*) FROM assessments WHERE is_active = true) AS active_assessments,
          (SELECT COUNT(*) FROM filtered_responses) AS total_responses,
          (SELECT COUNT(*) FROM filtered_responses WHERE status = 'completed') AS completed_responses,
          (SELECT COUNT(*) FROM filtered_responses WHERE status = 'in_progress') AS in_progress_responses,
          (SELECT COUNT(*) FROM participants) AS total_participants,
          (SELECT COUNT(DISTINCT participant_id) FROM filtered_responses WHERE status = 'completed') AS active_participants,
          (
            SELECT COALESCE(
              AVG(CASE WHEN overall_score::text = 'NaN' THEN NULL ELSE overall_score END),
              0
            )
            FROM filtered_responses
            WHERE status = 'completed'
          ) AS avg_overall_score,
          (SELECT COALESCE(AVG(overall_gap), 0) FROM filtered_responses WHERE status = 'completed') AS avg_overall_gap,
          (
            SELECT COALESCE(
              (COUNT(*) FILTER (WHERE status = 'completed')::DECIMAL / NULLIF(COUNT(*), 0) * 100),
              0
            )
            FROM filtered_responses
          ) AS completion_rate
      ),
      recent_activity AS (
        SELECT COALESCE(
          json_agg(
            json_build_object('date', activity.date, 'count', activity.count)
            ORDER BY activity.date
          ),
          '[]'::json
        ) AS data
        FROM (
          SELECT DATE(completed_at) AS date, COUNT(*) AS count
          FROM assessment_responses
          WHERE status = 'completed'
            AND completed_at >= CURRENT_DATE - INTERVAL '7 days'
          GROUP BY DATE(completed_at)
          ORDER BY DATE(completed_at)
        ) activity
      ),
      top_dimensions AS (
        SELECT COALESCE(
          json_agg(
            json_build_object('title', ranked.title, 'avg_score', ranked.avg_score)
            ORDER BY ranked.avg_score DESC
          ),
          '[]'::json
        ) AS data
        FROM (
          SELECT
            d.title,
            COALESCE(
              AVG(CASE WHEN cp.dimension_score::text = 'NaN' THEN NULL ELSE cp.dimension_score END),
              0
            ) AS avg_score
          FROM dimensions d
          JOIN computed_priorities cp ON d.id = cp.dimension_id
          JOIN filtered_responses ar ON cp.response_id = ar.id
          WHERE ar.status = 'completed'
          GROUP BY d.id, d.title
          ORDER BY avg_score DESC
          LIMIT 5
        ) ranked
      ),
      bottom_dimensions AS (
        SELECT COALESCE(
          json_agg(
            json_build_object('title', ranked.title, 'avg_gap', ranked.avg_gap, 'avg_priority', ranked.avg_priority)
            ORDER BY ranked.avg_gap DESC
          ),
          '[]'::json
        ) AS data
        FROM (
          SELECT
            d.title,
            COALESCE(AVG(cp.dimension_gap), 0) AS avg_gap,
            COALESCE(AVG(cp.priority_score), 0) AS avg_priority
          FROM dimensions d
          JOIN computed_priorities cp ON d.id = cp.dimension_id
          JOIN filtered_responses ar ON cp.response_id = ar.id
          WHERE ar.status = 'completed'
          GROUP BY d.id, d.title
          ORDER BY avg_gap DESC
          LIMIT 5
        ) ranked
      ),
      industry_stats AS (
        SELECT COALESCE(
          json_agg(
            json_build_object(
              'industry', ranked.industry,
              'total', ranked.total,
              'completed', ranked.completed,
              'completion_rate', ranked.completion_rate,
              'avg_score', ranked.avg_score
            )
            ORDER BY ranked.total DESC
          ),
          '[]'::json
        ) AS data
        FROM (
          SELECT
            p.industry,
            COUNT(ar.id) AS total,
            COUNT(*) FILTER (WHERE ar.status = 'completed') AS completed,
            (COUNT(*) FILTER (WHERE ar.status = 'completed')::DECIMAL / NULLIF(COUNT(ar.id), 0) * 100) AS completion_rate,
            COALESCE(
              AVG(
                CASE
                  WHEN ar.status = 'completed' AND ar.overall_score::text <> 'NaN' THEN ar.overall_score
                  ELSE NULL
                END
              ),
              0
            ) AS avg_score
          FROM participants p
          JOIN filtered_responses ar ON p.id = ar.participant_id
          WHERE p.industry IS NOT NULL
          GROUP BY p.industry
          HAVING COUNT(ar.id) > 0
          ORDER BY total DESC
          LIMIT 10
        ) ranked
      ),
      recent_completions AS (
        SELECT COALESCE(
          json_agg(
            json_build_object(
              'id', ranked.id,
              'completed_at', ranked.completed_at,
              'overall_score', ranked.overall_score,
              'full_name', ranked.full_name,
              'company_name', ranked.company_name,
              'industry', ranked.industry
            )
            ORDER BY ranked.completed_at DESC
          ),
          '[]'::json
        ) AS data
        FROM (
          SELECT
            ar.id,
            ar.completed_at,
            CASE WHEN ar.overall_score::text = 'NaN' THEN NULL ELSE ar.overall_score END AS overall_score,
            p.full_name,
            p.company_name,
            p.industry
          FROM filtered_responses ar
          JOIN participants p ON ar.participant_id = p.id
          WHERE ar.status = 'completed'
          ORDER BY ar.completed_at DESC
          LIMIT 10
        ) ranked
      )
      SELECT
        row_to_json(overview_stats.*) AS overview,
        recent_activity.data AS recent_activity,
        top_dimensions.data AS top_dimensions,
        bottom_dimensions.data AS bottom_dimensions,
        industry_stats.data AS industry_stats,
        recent_completions.data AS recent_completions
      FROM overview_stats, recent_activity, top_dimensions, bottom_dimensions, industry_stats, recent_completions;
      `,
      [range]
    );

    res.json({
      overview: stats.rows[0].overview,
      recentActivity: stats.rows[0].recent_activity || [],
      topDimensions: stats.rows[0].top_dimensions || [],
      bottomDimensions: stats.rows[0].bottom_dimensions || [],
      industryStats: stats.rows[0].industry_stats || [],
      recentCompletions: stats.rows[0].recent_completions || [],
    });
  } catch (error) {
    next(error);
  }
});

router.get('/activity', authenticateAdmin, requireRole('admin'), async (req, res, next) => {
  try {
    const activity = await query(`
      SELECT 
        'response' as type,
        ar.id,
        ar.status,
        ar.last_updated_at as timestamp,
        p.full_name as participant_name,
        p.company_name,
        a.title as assessment_title
      FROM assessment_responses ar
      JOIN participants p ON ar.participant_id = p.id
      JOIN assessments a ON ar.assessment_id = a.id
      WHERE ar.last_updated_at >= NOW() - INTERVAL '24 hours'
      ORDER BY ar.last_updated_at DESC
      LIMIT 20
    `);

    res.json(activity.rows);
  } catch (error) {
    next(error);
  }
});

export default router;


==================================================
FILE: src/routes/admin/import.ts
==================================================

import { Router } from 'express';
import { z } from 'zod';
import { authenticateAdmin } from '../../middleware/auth';
import { requireRole } from '../../middleware/rbac';
import { csrfProtection } from '../../middleware/csrf';
import { getClient, query } from '../../config/database';

const router = Router();

const MAX_JSON_SIZE = 5 * 1024 * 1024;

const RecommendationImportSchema = z.object({
  score_min: z.number().min(1).max(5).multipleOf(0.5).nullable().optional(),
  score_max: z.number().min(1).max(5).multipleOf(0.5).nullable().optional(),
  target_min: z.number().min(1).max(5).multipleOf(0.5).nullable().optional(),
  target_max: z.number().min(1).max(5).multipleOf(0.5).nullable().optional(),
  gap_min: z.number().min(0).max(4).multipleOf(0.5).nullable().optional(),
  gap_max: z.number().min(0).max(4).multipleOf(0.5).nullable().optional(),
  title: z.string().min(1).max(255),
  description: z.string().optional().default(''),
  why: z.string().optional().default(''),
  what: z.string().optional().default(''),
  how: z.string().optional().default(''),
  action_items: z.array(z.string().min(1)).default([]),
  category: z.enum(['Quick Win', 'Project', 'Big Bet']).default('Project'),
  priority: z.number().int().min(0).max(100).default(50),
  tags: z.array(z.string()).default([]),
});

const TopicImportSchema = z.object({
  topic_key: z.string().min(1).max(100),
  label: z.string().min(1).max(255),
  prompt: z.string().min(1),
  order_index: z.number().int().min(0).default(0),
  help_text: z.string().nullable().optional(),
  level_labels: z.tuple([
    z.string().min(1),
    z.string().min(1),
    z.string().min(1),
    z.string().min(1),
    z.string().min(1),
  ]),
  recommendations: z.array(RecommendationImportSchema).default([]),
});

const DimensionImportSchema = z.object({
  dimension_key: z.string().min(1).max(100),
  title: z.string().min(1).max(255),
  description: z.string().optional().default(''),
  category: z.string().optional().default(''),
  order_index: z.number().int().min(0).default(0),
  topics: z.array(TopicImportSchema).min(1),
});

const AssessmentImportSchema = z.object({
  assessment: z.object({
    title: z.string().min(1).max(255),
    description: z.string().optional().default(''),
    version: z.number().int().min(1).default(1),
    estimated_duration_minutes: z.number().int().nullable().optional(),
    instructions: z.string().nullable().optional(),
  }),
  dimensions: z.array(DimensionImportSchema).min(1),
});

const validateBodySchema = z.object({
  json: z.string().min(1),
});

const executeBodySchema = z.object({
  json: z.string().min(1),
  mode: z.enum(['create', 'update']),
});

type ImportSummary = {
  assessmentTitle: string;
  dimensionCount: number;
  topicCount: number;
  recommendationCount: number;
  topicsWithAllLevels: number;
  topicsWithRecommendations: number;
  duplicateExists: boolean;
  existingAssessmentId: string | null;
};

const emptySummary = (): ImportSummary => ({
  assessmentTitle: '',
  dimensionCount: 0,
  topicCount: 0,
  recommendationCount: 0,
  topicsWithAllLevels: 0,
  topicsWithRecommendations: 0,
  duplicateExists: false,
  existingAssessmentId: null,
});

const formatZodIssues = (issues: z.ZodIssue[]): string[] =>
  issues.map((issue) => {
    const path = issue.path.length > 0 ? issue.path.join('.') : 'root';
    return `${path}: ${issue.message}`;
  });

const findDuplicates = (keys: string[]): string[] => {
  const counts = new Map<string, number>();
  for (const key of keys) {
    counts.set(key, (counts.get(key) || 0) + 1);
  }
  return Array.from(counts.entries())
    .filter(([, count]) => count > 1)
    .map(([key]) => key);
};

const buildSummary = (
  data: z.infer<typeof AssessmentImportSchema>,
  duplicateExists: boolean,
  existingAssessmentId: string | null
): ImportSummary => {
  const topicCount = data.dimensions.reduce((sum, dimension) => sum + dimension.topics.length, 0);
  const recommendationCount = data.dimensions.reduce(
    (sum, dimension) =>
      sum + dimension.topics.reduce((topicSum, topic) => topicSum + topic.recommendations.length, 0),
    0
  );
  const topicsWithAllLevels = data.dimensions.reduce(
    (sum, dimension) =>
      sum +
      dimension.topics.filter((topic) => topic.level_labels.every((label) => label.trim().length > 0)).length,
    0
  );
  const topicsWithRecommendations = data.dimensions.reduce(
    (sum, dimension) => sum + dimension.topics.filter((topic) => topic.recommendations.length > 0).length,
    0
  );

  return {
    assessmentTitle: data.assessment.title,
    dimensionCount: data.dimensions.length,
    topicCount,
    recommendationCount,
    topicsWithAllLevels,
    topicsWithRecommendations,
    duplicateExists,
    existingAssessmentId,
  };
};

router.post('/validate', authenticateAdmin, requireRole('admin'), async (req, res, next) => {
  try {
    const parsedBody = validateBodySchema.safeParse(req.body);
    if (!parsedBody.success) {
      return res.status(400).json({
        valid: false,
        errors: formatZodIssues(parsedBody.error.issues),
        warnings: [],
        summary: emptySummary(),
      });
    }

    const { json } = parsedBody.data;
    if (json.length > MAX_JSON_SIZE) {
      return res.status(400).json({
        valid: false,
        errors: ['JSON payload too large (max 5MB)'],
        warnings: [],
        summary: emptySummary(),
      });
    }

    let rawData: unknown;
    try {
      rawData = JSON.parse(json);
    } catch (error) {
      const message = error instanceof Error ? error.message : 'Invalid JSON syntax';
      return res.status(400).json({
        valid: false,
        errors: [message],
        warnings: [],
        summary: emptySummary(),
      });
    }

    const schemaResult = AssessmentImportSchema.safeParse(rawData);
    if (!schemaResult.success) {
      return res.status(400).json({
        valid: false,
        errors: formatZodIssues(schemaResult.error.issues),
        warnings: [],
        summary: emptySummary(),
      });
    }

    const data = schemaResult.data;
    const errors: string[] = [];
    const warnings: string[] = [];

    const duplicateDimensionKeys = findDuplicates(data.dimensions.map((dimension) => dimension.dimension_key));
    if (duplicateDimensionKeys.length > 0) {
      errors.push(`Duplicate dimension_key values: ${duplicateDimensionKeys.join(', ')}`);
    }

    const topicKeys = data.dimensions.flatMap((dimension) =>
      dimension.topics.map((topic) => topic.topic_key)
    );
    const duplicateTopicKeys = findDuplicates(topicKeys);
    if (duplicateTopicKeys.length > 0) {
      errors.push(`Duplicate topic_key values: ${duplicateTopicKeys.join(', ')}`);
    }

    const existingAssessmentResult = await query(
      `SELECT id FROM assessments WHERE title = $1 ORDER BY created_at DESC LIMIT 1`,
      [data.assessment.title]
    );
    const existingAssessmentId =
      existingAssessmentResult.rows.length > 0 ? (existingAssessmentResult.rows[0].id as string) : null;
    const duplicateExists = Boolean(existingAssessmentId);

    if (duplicateExists) {
      warnings.push(`Assessment with title "${data.assessment.title}" already exists`);
    }

    return res.json({
      valid: errors.length === 0,
      errors,
      warnings,
      summary: buildSummary(data, duplicateExists, existingAssessmentId),
    });
  } catch (error) {
    next(error);
  }
});

router.post(
  '/execute',
  authenticateAdmin,
  requireRole('admin'),
  csrfProtection,
  async (req, res, next) => {
    const client = await getClient();
    let inTransaction = false;

    try {
      const parsedBody = executeBodySchema.safeParse(req.body);
      if (!parsedBody.success) {
        return res.status(400).json({ error: formatZodIssues(parsedBody.error.issues).join('; ') });
      }

      const { json, mode } = parsedBody.data;
      if (json.length > MAX_JSON_SIZE) {
        return res.status(400).json({ error: 'JSON payload too large (max 5MB)' });
      }

      const data = AssessmentImportSchema.parse(JSON.parse(json));

      const duplicateDimensionKeys = findDuplicates(data.dimensions.map((dimension) => dimension.dimension_key));
      if (duplicateDimensionKeys.length > 0) {
        return res.status(400).json({
          error: `Duplicate dimension_key values: ${duplicateDimensionKeys.join(', ')}`,
        });
      }

      const duplicateTopicKeys = findDuplicates(
        data.dimensions.flatMap((dimension) => dimension.topics.map((topic) => topic.topic_key))
      );
      if (duplicateTopicKeys.length > 0) {
        return res.status(400).json({
          error: `Duplicate topic_key values: ${duplicateTopicKeys.join(', ')}`,
        });
      }

      const existingAssessmentResult = await client.query(
        `SELECT id FROM assessments WHERE title = $1 ORDER BY created_at DESC LIMIT 1`,
        [data.assessment.title]
      );
      const existingId =
        existingAssessmentResult.rows.length > 0 ? (existingAssessmentResult.rows[0].id as string) : null;

      await client.query('BEGIN');
      inTransaction = true;

      let assessmentId: string;
      if (mode === 'update' && existingId) {
        await client.query(
          `UPDATE assessments
           SET title = $1,
               description = $2,
               version = $3,
               estimated_duration_minutes = $4,
               instructions = $5,
               updated_at = NOW()
           WHERE id = $6`,
          [
            data.assessment.title,
            data.assessment.description || null,
            data.assessment.version,
            data.assessment.estimated_duration_minutes ?? null,
            data.assessment.instructions ?? null,
            existingId,
          ]
        );
        assessmentId = existingId;
      } else {
        const assessmentInsertResult = await client.query(
          `INSERT INTO assessments
           (title, description, version, estimated_duration_minutes, instructions, is_active, is_published)
           VALUES ($1, $2, $3, $4, $5, true, false)
           RETURNING id`,
          [
            data.assessment.title,
            data.assessment.description || null,
            data.assessment.version,
            data.assessment.estimated_duration_minutes ?? null,
            data.assessment.instructions ?? null,
          ]
        );
        assessmentId = assessmentInsertResult.rows[0].id as string;
      }

      const dimensionValues: string[] = [];
      const dimensionParams: unknown[] = [];
      let paramIndex = 1;
      for (const dimension of data.dimensions) {
        dimensionValues.push(
          `($${paramIndex++}, $${paramIndex++}, $${paramIndex++}, $${paramIndex++}, $${paramIndex++}, $${paramIndex++})`
        );
        dimensionParams.push(
          assessmentId,
          dimension.dimension_key,
          dimension.title,
          dimension.description || null,
          dimension.category || null,
          dimension.order_index
        );
      }

      const dimensionUpsertResult = await client.query(
        `INSERT INTO dimensions (assessment_id, dimension_key, title, description, category, order_index)
         VALUES ${dimensionValues.join(', ')}
         ON CONFLICT (assessment_id, dimension_key)
         DO UPDATE SET
           title = EXCLUDED.title,
           description = EXCLUDED.description,
           category = EXCLUDED.category,
           order_index = EXCLUDED.order_index,
           updated_at = NOW()
         RETURNING id, dimension_key`,
        dimensionParams
      );

      const dimensionIdByKey = new Map<string, string>();
      for (const row of dimensionUpsertResult.rows as Array<{ id: string; dimension_key: string }>) {
        dimensionIdByKey.set(row.dimension_key, row.id);
      }

      const topicIdByCompositeKey = new Map<string, string>();
      let totalTopics = 0;
      let totalRecs = 0;

      for (const dimension of data.dimensions) {
        const dimensionId = dimensionIdByKey.get(dimension.dimension_key);
        if (!dimensionId) {
          throw new Error(`Failed to resolve dimension id for key: ${dimension.dimension_key}`);
        }

        const topicValues: string[] = [];
        const topicParams: unknown[] = [];
        paramIndex = 1;

        for (const topic of dimension.topics) {
          topicValues.push(
            `($${paramIndex++}, $${paramIndex++}, $${paramIndex++}, $${paramIndex++}, $${paramIndex++}, $${paramIndex++}, $${paramIndex++}, $${paramIndex++}, $${paramIndex++}, $${paramIndex++}, $${paramIndex++})`
          );
          topicParams.push(
            dimensionId,
            topic.topic_key,
            topic.label,
            topic.prompt,
            topic.order_index,
            topic.help_text ?? null,
            topic.level_labels[0],
            topic.level_labels[1],
            topic.level_labels[2],
            topic.level_labels[3],
            topic.level_labels[4]
          );
          totalTopics += 1;
          totalRecs += topic.recommendations.length;
        }

        const topicUpsertResult = await client.query(
          `INSERT INTO topics (
             dimension_id,
             topic_key,
             label,
             prompt,
             order_index,
             help_text,
             level_1_label,
             level_2_label,
             level_3_label,
             level_4_label,
             level_5_label
           )
           VALUES ${topicValues.join(', ')}
           ON CONFLICT (dimension_id, topic_key)
           DO UPDATE SET
             label = EXCLUDED.label,
             prompt = EXCLUDED.prompt,
             order_index = EXCLUDED.order_index,
             help_text = EXCLUDED.help_text,
             level_1_label = EXCLUDED.level_1_label,
             level_2_label = EXCLUDED.level_2_label,
             level_3_label = EXCLUDED.level_3_label,
             level_4_label = EXCLUDED.level_4_label,
             level_5_label = EXCLUDED.level_5_label,
             updated_at = NOW()
           RETURNING id, topic_key`,
          topicParams
        );

        for (const row of topicUpsertResult.rows as Array<{ id: string; topic_key: string }>) {
          topicIdByCompositeKey.set(`${dimension.dimension_key}:${row.topic_key}`, row.id);
        }
      }

      const importedTopicIds = Array.from(topicIdByCompositeKey.values());
      if (mode === 'update' && importedTopicIds.length > 0) {
        await client.query(
          `DELETE FROM topic_recommendations
           WHERE topic_id = ANY($1::uuid[])`,
          [importedTopicIds]
        );
      }

      const recValues: string[] = [];
      const recParams: unknown[] = [];
      paramIndex = 1;

      for (const dimension of data.dimensions) {
        for (const topic of dimension.topics) {
          const topicId = topicIdByCompositeKey.get(`${dimension.dimension_key}:${topic.topic_key}`);
          if (!topicId) {
            throw new Error(
              `Failed to resolve topic id for key: ${dimension.dimension_key}/${topic.topic_key}`
            );
          }

          topic.recommendations.forEach((rec, recIndex) => {
            recValues.push(
              `($${paramIndex++}, $${paramIndex++}, $${paramIndex++}, $${paramIndex++}, $${paramIndex++}, $${paramIndex++}, $${paramIndex++}, $${paramIndex++}, $${paramIndex++}, $${paramIndex++}, $${paramIndex++}, $${paramIndex++}, $${paramIndex++}::jsonb, $${paramIndex++}, $${paramIndex++}, $${paramIndex++}, true, $${paramIndex++})`
            );
            recParams.push(
              topicId,
              rec.score_min ?? null,
              rec.score_max ?? null,
              rec.target_min ?? null,
              rec.target_max ?? null,
              rec.gap_min ?? null,
              rec.gap_max ?? null,
              rec.title,
              rec.description || null,
              rec.why || null,
              rec.what || null,
              rec.how || null,
              JSON.stringify(rec.action_items ?? []),
              rec.category,
              rec.priority,
              rec.tags ?? [],
              recIndex
            );
          });
        }
      }

      if (recValues.length > 0) {
        await client.query(
          `INSERT INTO topic_recommendations (
             topic_id,
             score_min,
             score_max,
             target_min,
             target_max,
             gap_min,
             gap_max,
             title,
             description,
             why,
             what,
             how,
             action_items,
             category,
             priority,
             tags,
             is_active,
             order_index
           )
           VALUES ${recValues.join(', ')}`,
          recParams
        );
      }

      await client.query('COMMIT');
      inTransaction = false;

      return res.json({
        success: true,
        assessmentId,
        imported: {
          dimensions: data.dimensions.length,
          topics: totalTopics,
          recommendations: totalRecs,
        },
      });
    } catch (error) {
      if (inTransaction) {
        await client.query('ROLLBACK');
      }
      next(error);
    } finally {
      client.release();
    }
  }
);

export default router;


==================================================
FILE: src/routes/admin/index.ts
==================================================

import { Router } from 'express';
import authRoutes from './auth';
import dashboardRoutes from './dashboard';
import importRouter from './import';
import assessmentsRoutes from './assessments';
import responsesRoutes from './responses';
import analyticsRoutes from './analytics';
import recommendationsRoutes from './recommendations';
import topicLevelsRouter from './topic-levels';
import topicRecsRouter from './topic-recommendations';

const router = Router();

router.use('/auth', authRoutes);
router.use('/dashboard', dashboardRoutes);
router.use('/assessments/import', importRouter);
router.use('/assessments', assessmentsRoutes);
router.use('/responses', responsesRoutes);
router.use('/analytics', analyticsRoutes);
router.use('/recommendations', recommendationsRoutes);
router.use('/topics', topicLevelsRouter);
router.use('/topics', topicRecsRouter);

export default router;


==================================================
FILE: src/routes/admin/recommendations.ts
==================================================

/**
 * File: src/routes/admin/recommendations.ts
 * Purpose: Manage recommendations for scoring logic
 */

import { Router } from 'express';
import { authenticateAdmin } from '../../middleware/auth';
import { requireRole } from '../../middleware/rbac';
import { query } from '../../config/database';
import { z } from 'zod';
import { validate } from '../../middleware/validation';
import { csrfProtection } from '../../middleware/csrf';

const router = Router();

// Validation schema for recommendations
const recommendationSchema = z.object({
  dimension_id: z.string().uuid(),
  title: z.string().min(3),
  description: z.string().min(10),
  action_items: z.array(z.string()).min(1),
  resources: z.array(z.object({
    title: z.string(),
    url: z.string().url(),
    type: z.string().optional()
  })).optional(),
  min_gap: z.number().min(0).max(5),
  max_gap: z.number().min(0).max(5),
  priority_level: z.enum(['low', 'medium', 'high', 'critical']),
});

router.get('/', authenticateAdmin, requireRole('admin'), async (req, res, next) => {
  try {
    const { dimension_id } = req.query;
    
    let queryStr = `
      SELECT r.*, d.title as dimension_title
      FROM recommendations r
      JOIN dimensions d ON r.dimension_id = d.id
    `;
    
    const params: any[] = [];
    if (dimension_id) {
      queryStr += ` WHERE r.dimension_id = $1`;
      params.push(dimension_id);
    }
    
    queryStr += ` ORDER BY d.title, r.priority_level DESC`;
    
    const result = await query(queryStr, params);
    res.json(result.rows);
  } catch (error) {
    next(error);
  }
});

router.post(
  '/',
  authenticateAdmin,
  requireRole('admin'),
  csrfProtection,
  validate(recommendationSchema),
  async (req, res, next) => {
  try {
    const { 
      dimension_id, title, description, action_items, 
      resources, min_gap, max_gap, priority_level 
    } = req.body;

    const result = await query(
      `INSERT INTO recommendations 
        (dimension_id, title, description, action_items, resources, min_gap, max_gap, priority_level)
       VALUES ($1, $2, $3, $4, $5, $6, $7, $8)
       RETURNING *`,
      [
        dimension_id, title, description, JSON.stringify(action_items), 
        JSON.stringify(resources || []), min_gap, max_gap, priority_level
      ]
    );

    res.status(201).json(result.rows[0]);
  } catch (error) {
    next(error);
  }
}
);

router.put(
  '/:id',
  authenticateAdmin,
  requireRole('admin'),
  csrfProtection,
  validate(recommendationSchema),
  async (req, res, next) => {
  try {
    const { id } = req.params;
    const { 
      dimension_id, title, description, action_items, 
      resources, min_gap, max_gap, priority_level 
    } = req.body;

    const result = await query(
      `UPDATE recommendations 
       SET dimension_id = $1, title = $2, description = $3, 
           action_items = $4, resources = $5, min_gap = $6, 
           max_gap = $7, priority_level = $8, updated_at = NOW()
       WHERE id = $9
       RETURNING *`,
      [
        dimension_id, title, description, JSON.stringify(action_items), 
        JSON.stringify(resources || []), min_gap, max_gap, priority_level, id
      ]
    );

    if (result.rows.length === 0) {
      return res.status(404).json({ error: 'Recommendation not found' });
    }

    res.json(result.rows[0]);
  } catch (error) {
    next(error);
  }
}
);

router.delete('/:id', authenticateAdmin, requireRole('admin'), csrfProtection, async (req, res, next) => {
  try {
    const { id } = req.params;
    const result = await query('DELETE FROM recommendations WHERE id = $1 RETURNING id', [id]);
    
    if (result.rows.length === 0) {
      return res.status(404).json({ error: 'Recommendation not found' });
    }
    
    res.json({ message: 'Recommendation deleted successfully' });
  } catch (error) {
    next(error);
  }
});

export default router;


==================================================
FILE: src/routes/admin/responses.ts
==================================================

/**
 * File: src/routes/admin/responses.ts
 * Purpose: Manage assessment responses (view, filter, export)
 */

import { Router } from 'express';
import { authenticateAdmin } from '../../middleware/auth';
import { requireRole } from '../../middleware/rbac';
import { query } from '../../config/database';
import { z } from 'zod';
import { validate } from '../../middleware/validation';

const router = Router();

const listResponsesSchema = z.object({
  query: z.object({
    page: z.coerce.number().int().min(1).default(1),
    limit: z.coerce.number().int().min(1).max(100).default(10),
    status: z.string().optional(),
    assessment_id: z.string().uuid().optional(),
    search: z.string().max(200).optional(),
  }),
});

const responseIdSchema = z.object({
  params: z.object({
    id: z.string().uuid(),
  }),
});

router.get('/', authenticateAdmin, requireRole('admin'), validate(listResponsesSchema), async (req, res, next) => {
  try {
    const { page, limit, status, assessment_id, search } = req.query as unknown as z.infer<
      typeof listResponsesSchema
    >['query'];
    const offset = (page - 1) * limit;

    let queryStr = `
      SELECT 
        ar.*,
        p.full_name,
        p.email,
        p.company_name,
        a.title as assessment_title
      FROM assessment_responses ar
      JOIN participants p ON ar.participant_id = p.id
      JOIN assessments a ON ar.assessment_id = a.id
      WHERE 1=1
    `;

    const params: unknown[] = [];
    let paramIndex = 1;

    if (status) {
      queryStr += ` AND ar.status = $${paramIndex}`;
      params.push(status);
      paramIndex++;
    }

    if (assessment_id) {
      queryStr += ` AND ar.assessment_id = $${paramIndex}`;
      params.push(assessment_id);
      paramIndex++;
    }

    if (search) {
      queryStr += ` AND (p.email ILIKE $${paramIndex} OR p.company_name ILIKE $${paramIndex + 1})`;
      params.push(`%${search}%`, `%${search}%`);
      paramIndex += 2;
    }

    queryStr += ` ORDER BY ar.last_updated_at DESC LIMIT $${paramIndex} OFFSET $${paramIndex + 1}`;
    params.push(limit, offset);

    const result = await query(queryStr, params);

    // Keep count query parameterized to prevent SQL injection.
    let countQuery = `
      SELECT COUNT(*) 
      FROM assessment_responses ar
      JOIN participants p ON ar.participant_id = p.id
      WHERE 1=1
    `;
    const countParams: unknown[] = [];
    let countParamIndex = 1;

    if (status) {
      countQuery += ` AND ar.status = $${countParamIndex++}`;
      countParams.push(status);
    }

    if (assessment_id) {
      countQuery += ` AND ar.assessment_id = $${countParamIndex++}`;
      countParams.push(assessment_id);
    }

    if (search) {
      countQuery += ` AND (p.email ILIKE $${countParamIndex} OR p.company_name ILIKE $${countParamIndex + 1})`;
      countParams.push(`%${search}%`, `%${search}%`);
    }

    const countResult = await query(countQuery, countParams);

    res.json({
      data: result.rows,
      pagination: {
        total: Number(countResult.rows[0].count),
        page,
        limit,
        pages: Math.ceil(Number(countResult.rows[0].count) / limit),
      },
    });
  } catch (error) {
    next(error);
  }
});

router.get(
  '/:id',
  authenticateAdmin,
  requireRole('admin'),
  validate(responseIdSchema),
  async (req, res, next) => {
  try {
    const { id } = req.params;

    const response = await query(
      `SELECT 
        ar.*,
        p.full_name,
        p.email,
        p.company_name,
        p.job_title,
        p.industry,
        a.title as assessment_title
       FROM assessment_responses ar
       JOIN participants p ON ar.participant_id = p.id
       JOIN assessments a ON ar.assessment_id = a.id
       WHERE ar.id = $1`,
      [id]
    );

    if (response.rows.length === 0) {
      return res.status(404).json({ error: 'Response not found' });
    }

    // Fetch topics responses
    const topicResponses = await query(
      `SELECT 
        tr.*,
        t.label,
        d.title as dimension_title
       FROM topic_responses tr
       JOIN topics t ON tr.topic_id = t.id
       JOIN dimensions d ON t.dimension_id = d.id
       WHERE tr.response_id = $1
       ORDER BY d.order_index, t.order_index`,
      [id]
    );

    // Fetch computed priorities
    const priorities = await query(
      `SELECT 
        cp.*,
        d.title as dimension_title
       FROM computed_priorities cp
       JOIN dimensions d ON cp.dimension_id = d.id
       WHERE cp.response_id = $1
       ORDER BY cp.rank_order`,
      [id]
    );

    res.json({
      response: response.rows[0],
      answers: topicResponses.rows,
      priorities: priorities.rows,
    });
  } catch (error) {
    next(error);
  }
}
);

export default router;


==================================================
FILE: src/routes/admin/topic-levels.ts
==================================================

import { Router } from 'express';
import { z } from 'zod';
import { authenticateAdmin } from '../../middleware/auth';
import { requireRole } from '../../middleware/rbac';
import { csrfProtection } from '../../middleware/csrf';
import { validate } from '../../middleware/validation';
import { query } from '../../config/database';

const router = Router();

const topicParamsSchema = z.object({
  params: z.object({
    topicId: z.string().uuid(),
  }),
});

const updateTopicLevelsSchema = z.object({
  params: z.object({
    topicId: z.string().uuid(),
  }),
  body: z.object({
    level1Label: z.string().min(1).max(500),
    level2Label: z.string().min(1).max(500),
    level3Label: z.string().min(1).max(500),
    level4Label: z.string().min(1).max(500),
    level5Label: z.string().min(1).max(500),
  }),
});

router.get(
  '/:topicId/levels',
  authenticateAdmin,
  requireRole('admin'),
  validate(topicParamsSchema),
  async (req, res, next) => {
    try {
      const { topicId } = req.params;
      const result = await query(
        `SELECT
           id,
           level_1_label,
           level_2_label,
           level_3_label,
           level_4_label,
           level_5_label
         FROM topics
         WHERE id = $1`,
        [topicId]
      );

      if (result.rows.length === 0) {
        return res.status(404).json({ error: 'Topic not found' });
      }

      const row = result.rows[0];
      res.json({
        topicId: row.id,
        level1Label: row.level_1_label,
        level2Label: row.level_2_label,
        level3Label: row.level_3_label,
        level4Label: row.level_4_label,
        level5Label: row.level_5_label,
      });
    } catch (error) {
      next(error);
    }
  }
);

router.put(
  '/:topicId/levels',
  authenticateAdmin,
  requireRole('admin'),
  csrfProtection,
  validate(updateTopicLevelsSchema),
  async (req, res, next) => {
    try {
      const { topicId } = req.params;
      const { level1Label, level2Label, level3Label, level4Label, level5Label } = req.body;

      const result = await query(
        `UPDATE topics
         SET
           level_1_label = $1,
           level_2_label = $2,
           level_3_label = $3,
           level_4_label = $4,
           level_5_label = $5,
           updated_at = NOW()
         WHERE id = $6
         RETURNING id, level_1_label, level_2_label, level_3_label, level_4_label, level_5_label`,
        [level1Label, level2Label, level3Label, level4Label, level5Label, topicId]
      );

      if (result.rows.length === 0) {
        return res.status(404).json({ error: 'Topic not found' });
      }

      const row = result.rows[0];
      res.json({
        topicId: row.id,
        level1Label: row.level_1_label,
        level2Label: row.level_2_label,
        level3Label: row.level_3_label,
        level4Label: row.level_4_label,
        level5Label: row.level_5_label,
      });
    } catch (error) {
      next(error);
    }
  }
);

export default router;


==================================================
FILE: src/routes/admin/topic-recommendations.ts
==================================================

import { Router } from 'express';
import { z } from 'zod';
import { authenticateAdmin } from '../../middleware/auth';
import { requireRole } from '../../middleware/rbac';
import { csrfProtection } from '../../middleware/csrf';
import { validate } from '../../middleware/validation';
import { query } from '../../config/database';

const router = Router();

const scoreSchema = z.number().min(1).max(5).multipleOf(0.5).nullable().optional();
const gapSchema = z.number().min(0).max(4).multipleOf(0.5).nullable().optional();

const createTopicRecBodySchema = z.object({
  score_min: scoreSchema,
  score_max: scoreSchema,
  target_min: scoreSchema,
  target_max: scoreSchema,
  gap_min: gapSchema,
  gap_max: gapSchema,
  title: z.string().min(1).max(255),
  description: z.string().optional().nullable(),
  why: z.string().optional().nullable(),
  what: z.string().optional().nullable(),
  how: z.string().optional().nullable(),
  action_items: z.array(z.string().min(1)).default([]),
  category: z.enum(['Quick Win', 'Project', 'Big Bet']).default('Project'),
  priority: z.number().int().min(0).max(100).default(50),
  tags: z.array(z.string()).default([]),
  is_active: z.boolean().default(true),
  order_index: z.number().int().default(0),
});

const topicParamsSchema = z.object({
  params: z.object({
    topicId: z.string().uuid(),
  }),
});

const recParamsSchema = z.object({
  params: z.object({
    topicId: z.string().uuid(),
    recId: z.string().uuid(),
  }),
});

const createTopicRecSchema = z.object({
  params: z.object({
    topicId: z.string().uuid(),
  }),
  body: createTopicRecBodySchema,
});

const updateTopicRecSchema = z.object({
  params: z.object({
    topicId: z.string().uuid(),
    recId: z.string().uuid(),
  }),
  body: createTopicRecBodySchema,
});

const deleteTopicRecSchema = z.object({
  params: z.object({
    topicId: z.string().uuid(),
    recId: z.string().uuid(),
  }),
  query: z.object({
    hard: z
      .union([z.literal('true'), z.literal('false')])
      .optional(),
  }),
});

const testTopicRecSchema = z.object({
  params: z.object({
    topicId: z.string().uuid(),
  }),
  body: z.object({
    score: z.number().min(1).max(5).multipleOf(0.5),
    target: z.number().min(1).max(5).multipleOf(0.5),
  }),
});

type TopicRecommendationRow = {
  id: string;
  topic_id: string;
  score_min: number | null;
  score_max: number | null;
  target_min: number | null;
  target_max: number | null;
  gap_min: number | null;
  gap_max: number | null;
  title: string;
  description: string | null;
  why: string | null;
  what: string | null;
  how: string | null;
  action_items: string[];
  category: 'Quick Win' | 'Project' | 'Big Bet';
  priority: number;
  tags: string[];
  is_active: boolean;
  order_index: number;
  created_at: string;
  updated_at: string;
};

const matchesConditions = (
  rec: TopicRecommendationRow,
  score: number,
  target: number,
  gap: number
): boolean =>
  (rec.score_min == null || score >= rec.score_min) &&
  (rec.score_max == null || score <= rec.score_max) &&
  (rec.target_min == null || target >= rec.target_min) &&
  (rec.target_max == null || target <= rec.target_max) &&
  (rec.gap_min == null || gap >= rec.gap_min) &&
  (rec.gap_max == null || gap <= rec.gap_max);

router.get(
  '/:topicId/recommendations',
  authenticateAdmin,
  requireRole('admin'),
  validate(topicParamsSchema),
  async (req, res, next) => {
    try {
      const { topicId } = req.params;
      const result = await query(
        `SELECT
           id, topic_id, score_min, score_max, target_min, target_max, gap_min, gap_max,
           title, description, why, what, how, action_items, category, priority, tags,
           is_active, order_index, created_at, updated_at
         FROM topic_recommendations
         WHERE topic_id = $1
         ORDER BY order_index ASC, created_at ASC`,
        [topicId]
      );

      res.json(result.rows);
    } catch (error) {
      next(error);
    }
  }
);

router.post(
  '/:topicId/recommendations',
  authenticateAdmin,
  requireRole('admin'),
  csrfProtection,
  validate(createTopicRecSchema),
  async (req, res, next) => {
    try {
      const { topicId } = req.params;
      const body = req.body;
      const result = await query(
        `INSERT INTO topic_recommendations (
           topic_id, score_min, score_max, target_min, target_max, gap_min, gap_max,
           title, description, why, what, how, action_items, category, priority, tags,
           is_active, order_index
         ) VALUES (
           $1, $2, $3, $4, $5, $6, $7,
           $8, $9, $10, $11, $12, $13::jsonb, $14, $15, $16,
           $17, $18
         )
         RETURNING *`,
        [
          topicId,
          body.score_min ?? null,
          body.score_max ?? null,
          body.target_min ?? null,
          body.target_max ?? null,
          body.gap_min ?? null,
          body.gap_max ?? null,
          body.title,
          body.description ?? null,
          body.why ?? null,
          body.what ?? null,
          body.how ?? null,
          JSON.stringify(body.action_items ?? []),
          body.category,
          body.priority,
          body.tags ?? [],
          body.is_active,
          body.order_index,
        ]
      );

      res.status(201).json(result.rows[0]);
    } catch (error) {
      next(error);
    }
  }
);

router.put(
  '/:topicId/recommendations/:recId',
  authenticateAdmin,
  requireRole('admin'),
  csrfProtection,
  validate(updateTopicRecSchema),
  async (req, res, next) => {
    try {
      const { topicId, recId } = req.params;
      const body = req.body;
      const result = await query(
        `UPDATE topic_recommendations
         SET
           score_min = $1,
           score_max = $2,
           target_min = $3,
           target_max = $4,
           gap_min = $5,
           gap_max = $6,
           title = $7,
           description = $8,
           why = $9,
           what = $10,
           how = $11,
           action_items = $12::jsonb,
           category = $13,
           priority = $14,
           tags = $15,
           is_active = $16,
           order_index = $17
         WHERE id = $18
           AND topic_id = $19
         RETURNING *`,
        [
          body.score_min ?? null,
          body.score_max ?? null,
          body.target_min ?? null,
          body.target_max ?? null,
          body.gap_min ?? null,
          body.gap_max ?? null,
          body.title,
          body.description ?? null,
          body.why ?? null,
          body.what ?? null,
          body.how ?? null,
          JSON.stringify(body.action_items ?? []),
          body.category,
          body.priority,
          body.tags ?? [],
          body.is_active,
          body.order_index,
          recId,
          topicId,
        ]
      );

      if (result.rows.length === 0) {
        return res.status(404).json({ error: 'Recommendation not found' });
      }

      res.json(result.rows[0]);
    } catch (error) {
      next(error);
    }
  }
);

router.delete(
  '/:topicId/recommendations/:recId',
  authenticateAdmin,
  requireRole('admin'),
  csrfProtection,
  validate(deleteTopicRecSchema),
  async (req, res, next) => {
    try {
      const { topicId, recId } = req.params;
      const hardDelete = req.query.hard === 'true';

      const result = hardDelete
        ? await query(
            `DELETE FROM topic_recommendations
             WHERE id = $1 AND topic_id = $2
             RETURNING id`,
            [recId, topicId]
          )
        : await query(
            `UPDATE topic_recommendations
             SET is_active = false, updated_at = NOW()
             WHERE id = $1 AND topic_id = $2
             RETURNING id`,
            [recId, topicId]
          );

      if (result.rows.length === 0) {
        return res.status(404).json({ error: 'Recommendation not found' });
      }

      res.json({ message: hardDelete ? 'Recommendation deleted' : 'Recommendation deactivated' });
    } catch (error) {
      next(error);
    }
  }
);

router.post(
  '/:topicId/recommendations/test',
  authenticateAdmin,
  requireRole('admin'),
  csrfProtection,
  validate(testTopicRecSchema),
  async (req, res, next) => {
    try {
      const { topicId } = req.params;
      const { score, target } = req.body;
      const gap = Number(Math.max(0, target - score).toFixed(1));

      const result = await query(
        `SELECT
           id, topic_id, score_min, score_max, target_min, target_max, gap_min, gap_max,
           title, description, why, what, how, action_items, category, priority, tags,
           is_active, order_index, created_at, updated_at
         FROM topic_recommendations
         WHERE topic_id = $1
           AND is_active = true
         ORDER BY priority DESC, order_index ASC`,
        [topicId]
      );

      const matched = (result.rows as TopicRecommendationRow[]).filter((rec) =>
        matchesConditions(rec, score, target, gap)
      );

      res.json({
        score,
        target,
        gap,
        matchedRecommendations: matched,
      });
    } catch (error) {
      next(error);
    }
  }
);

export default router;


==================================================
FILE: src/routes/public/assessments.ts
==================================================

/**
 * File: src/routes/public/assessments.ts
 * Purpose: Public endpoints for retrieving assessment structure
 */

import { Router } from 'express';
import { query } from '../../config/database';
import { logger } from '../../utils/logger';

const router = Router();

const mapLevelAnchors = (topic: {
  level_1_label: string | null;
  level_2_label: string | null;
  level_3_label: string | null;
  level_4_label: string | null;
  level_5_label: string | null;
}): Array<string | null> => [
  topic.level_1_label ?? null,
  topic.level_2_label ?? null,
  topic.level_3_label ?? null,
  topic.level_4_label ?? null,
  topic.level_5_label ?? null,
];

/**
 * GET /api/public/assessments/active/structure
 * Returns the active assessment with dimensions and topics
 */
router.get('/active/structure', async (req, res, next) => {
  try {
    const result = await query(`
      WITH active_assessment AS (
        SELECT id, title, description, version
        FROM assessments
        WHERE is_active = true
          AND is_published = true
        ORDER BY published_at DESC NULLS LAST, created_at DESC
        LIMIT 1
      )
      SELECT
        a.id as assessment_id,
        a.title,
        a.description,
        a.version,
        json_agg(
          json_build_object(
            'id', d.id,
            'dimensionKey', d.dimension_key,
            'title', d.title,
            'description', d.description,
            'category', d.category,
            'orderIndex', d.order_index,
            'topics', (
              SELECT json_agg(
                json_build_object(
                  'id', t.id,
                  'topicKey', t.topic_key,
                  'label', t.label,
                  'prompt', t.prompt,
                  'helpText', t.help_text,
                  'orderIndex', t.order_index,
                  'level_1_label', t.level_1_label,
                  'level_2_label', t.level_2_label,
                  'level_3_label', t.level_3_label,
                  'level_4_label', t.level_4_label,
                  'level_5_label', t.level_5_label
                ) ORDER BY t.order_index
              )
              FROM topics t
              WHERE t.dimension_id = d.id
            )
          ) ORDER BY d.order_index
        ) as dimensions
      FROM assessments a
      JOIN active_assessment aa ON aa.id = a.id
      LEFT JOIN dimensions d ON a.id = d.assessment_id
      GROUP BY a.id, a.title, a.description, a.version
    `);

    if (result.rows.length === 0) {
      return res.status(404).json({ error: 'No active assessment found' });
    }

    const assessment = result.rows[0];

    const normalizedDimensions = (assessment.dimensions || [])
      .filter((dimension: any) => dimension?.id)
      .map((dimension: any) => {
        const normalizedTopics = (dimension.topics || [])
          .filter((topic: any) => topic?.id)
          .map((topic: any) => ({
            id: topic.id,
            topicKey: topic.topicKey,
            label: topic.label,
            prompt: topic.prompt,
            orderIndex: topic.orderIndex ?? topic.order ?? 0,
            helpText: topic.helpText ?? undefined,
            levelAnchors: mapLevelAnchors(topic),
          }));

        return {
          ...dimension,
          orderIndex: dimension.orderIndex ?? dimension.order ?? 0,
          topics: normalizedTopics,
        };
      });

    res.json({
      assessment: {
        id: assessment.assessment_id,
        title: assessment.title,
        description: assessment.description,
        version: assessment.version,
      },
      dimensions: normalizedDimensions,
    });
  } catch (error) {
    logger.error('Error fetching assessment structure:', error);
    next(error);
  }
});

router.get('/active', async (req, res, next) => {
  try {
    const assessment = await query(`
      SELECT
        id, version, title, description, estimated_duration_minutes
      FROM assessments
      WHERE is_active = true AND is_published = true
      ORDER BY created_at DESC
      LIMIT 1
    `);

    if (assessment.rows.length === 0) {
      return res.status(404).json({
        success: false,
        error: 'No active assessment available'
      });
    }

    const rawAssessment = assessment.rows[0];
    const normalizedAssessment = {
      ...rawAssessment,
      estimated_duration_minutes:
        typeof rawAssessment.estimated_duration_minutes === 'number'
          ? rawAssessment.estimated_duration_minutes
          : undefined,
      description: rawAssessment.description ?? '',
    };

    res.json({
      success: true,
      data: normalizedAssessment
    });
  } catch (error) {
    logger.error('Error fetching active assessment:', error);
    next(error);
  }
});

router.get('/:id/structure', async (req, res, next) => {
  try {
    const { id } = req.params;

    const assessmentResult = await query(
      `SELECT id, version, title, description
       FROM assessments
       WHERE id = $1 AND is_active = true AND is_published = true`,
      [id]
    );

    if (assessmentResult.rows.length === 0) {
      return res.status(404).json({
        success: false,
        error: 'Assessment not found or not active'
      });
    }

    const structureResult = await query(`
      SELECT
        d.id as dimension_id,
        d.dimension_key,
        d.title as dimension_title,
        d.description as dimension_description,
        d.order_index as dimension_order,
        json_agg(
          json_build_object(
            'id', t.id,
            'topicKey', t.topic_key,
            'label', t.label,
            'prompt', t.prompt,
            'order', t.order_index,
            'helpText', t.help_text,
            'level_1_label', t.level_1_label,
            'level_2_label', t.level_2_label,
            'level_3_label', t.level_3_label,
            'level_4_label', t.level_4_label,
            'level_5_label', t.level_5_label
          ) ORDER BY t.order_index
        ) as topics
      FROM dimensions d
      LEFT JOIN topics t ON d.id = t.dimension_id
      WHERE d.assessment_id = $1
      GROUP BY d.id
      ORDER BY d.order_index
    `, [id]);

    const dimensions = structureResult.rows.map((row) => {
      const normalizedTopics = Array.isArray(row.topics)
        ? row.topics
            .filter((topic: any) => topic?.id)
            .map((topic: any) => ({
              id: topic.id,
              topicKey: topic.topicKey,
              label: topic.label,
              prompt: topic.prompt,
              orderIndex: topic.order ?? topic.orderIndex ?? 0,
              helpText: topic.helpText ?? undefined,
              levelAnchors: mapLevelAnchors(topic),
            }))
        : [];

      return {
        id: row.dimension_id,
        dimensionKey: row.dimension_key,
        title: row.dimension_title,
        description: row.dimension_description ?? undefined,
        orderIndex: row.dimension_order ?? 0,
        topics: normalizedTopics,
      };
    });

    res.json({
      success: true,
      data: {
        assessment: assessmentResult.rows[0],
        dimensions
      }
    });

  } catch (error) {
    logger.error(`Error fetching assessment structure for ${req.params.id}:`, error);
    next(error);
  }
});

export default router;


==================================================
FILE: src/routes/public/index.ts
==================================================

import { Router } from 'express';
import assessmentsRoutes from './assessments';
import participantsRoutes from './participants';
import responsesRoutes from './responses';
import recommendationsDefRoutes from './recommendations-definition';
import narrativeDefRoutes from './narrative-definition';

const router = Router();

router.use('/assessments', assessmentsRoutes);
router.use('/participants', participantsRoutes);
router.use('/responses', responsesRoutes);
router.use('/recommendations/definition', recommendationsDefRoutes);
router.use('/narrative/definition', narrativeDefRoutes);

export default router;


==================================================
FILE: src/routes/public/narrative-definition.ts
==================================================

import { Router, Request, Response, NextFunction } from 'express';
import { query } from '../../config/database';
import { logger } from '../../utils/logger';

const router = Router();

/**
 * GET /api/public/narrative/definition
 * Returns the complete narrative definition including templates, theme map, and config
 */
router.get('/', async (req: Request, res: Response, next: NextFunction) => {
  try {
    const isRecord = (value: unknown): value is Record<string, unknown> =>
      typeof value === 'object' && value !== null && !Array.isArray(value);

    const normalizeGapThresholds = (value: unknown) => {
      if (!isRecord(value)) {
        return undefined;
      }

      const minor = Number(value.minor);
      const moderate = Number(value.moderate);
      const significant = Number(value.significant);

      if ([minor, moderate, significant].every((n) => Number.isFinite(n))) {
        return { minor, moderate, significant };
      }

      return undefined;
    };

    const normalizeHeadlines = (value: unknown) => {
      if (!isRecord(value) || typeof value.lowConfidencePrefix !== 'string') {
        return undefined;
      }

      const byStageId = isRecord(value.byStageId)
        ? Object.fromEntries(
            Object.entries(value.byStageId).filter(([, v]) => typeof v === 'string')
          )
        : {};

      return {
        lowConfidencePrefix: value.lowConfidencePrefix,
        byStageId,
      };
    };

    const normalizeExecutiveSummary = (value: unknown) => {
      if (!isRecord(value)) {
        return undefined;
      }

      if (
        typeof value.sentence1 === 'string' &&
        typeof value.sentence2 === 'string' &&
        typeof value.sentence3 === 'string'
      ) {
        return {
          sentence1: value.sentence1,
          sentence2: value.sentence2,
          sentence3: value.sentence3,
        };
      }

      return undefined;
    };

    // 1. Get theme map
    const themeMapResult = await query(`
      SELECT theme_key, theme_label
      FROM narrative_theme_map
    `);

    const themeMap: Record<string, string> = {};
    themeMapResult.rows.forEach((row: any) => {
      themeMap[row.theme_key] = row.theme_label;
    });

    // 2. Get config values
    const configResult = await query(`
      SELECT config_key, config_value
      FROM narrative_config
    `);

    // Initialize with defaults or empty
    let maturityThresholds: any = {};
    let maturityLabels: string[] = [];
    let gapThresholds: any = undefined;
    let headlines: any = undefined;
    let executiveSummary: any = undefined;
    let stageRationale: string = "";
    let priorityWhyTemplate: string = "";
    let notes: any = {};

    configResult.rows.forEach((row: any) => {
      switch (row.config_key) {
        case 'maturity_thresholds':
          maturityThresholds = row.config_value;
          break;
        case 'maturity_labels':
          maturityLabels = row.config_value;
          break;
        case 'gap_thresholds':
          gapThresholds = normalizeGapThresholds(row.config_value);
          break;
        case 'headlines':
          headlines = normalizeHeadlines(row.config_value);
          break;
        case 'executive_summary':
          executiveSummary = normalizeExecutiveSummary(row.config_value);
          break;
        case 'stage_rationale':
          stageRationale = row.config_value;
          break;
        case 'priority_why_template':
          priorityWhyTemplate = row.config_value;
          break;
        case 'notes':
          notes = row.config_value;
          break;
      }
    });

    // 3. Get templates
    // We need template_key to reconstruct the key-value pairs
    const templatesResult = await query(`
      SELECT template_key, template_type, category, template, priority
      FROM narrative_templates
      ORDER BY template_type, category, priority
    `);

    // Frontend expects specific structure for executiveTemplates
    const executiveTemplates: any = {
      maturityLevel: {},
      gapAnalysis: {
        large: '',
        moderate: '',
        minimal: '',
      },
      strengths: {
        multiple: '',
        single: '',
      },
      priorities: {
        high: '',
        balanced: '',
      },
    };
    
    // These might be unused in current frontend but good to map if they exist
    const dimensionTemplates: Record<string, string[]> = {};
    const gapTemplates: Record<string, string[]> = {};

    templatesResult.rows.forEach((row: any) => {
      const category = row.category || 'default';
      const template = row.template;
      
      switch (row.template_type) {
        case 'executive':
          // template_key format: executive_<category>_<key>
          // Example: executive_maturityLevel_leading
          // We need to extract "leading"
          const prefix = `executive_${category}_`;
          if (row.template_key && row.template_key.startsWith(prefix)) {
             const key = row.template_key.substring(prefix.length);
             if (!executiveTemplates[category]) executiveTemplates[category] = {};
             executiveTemplates[category][key] = template;
          }
          break;
          
        case 'dimension':
          if (!dimensionTemplates[category]) dimensionTemplates[category] = [];
          dimensionTemplates[category].push(template);
          break;
          
        case 'gap':
          if (!gapTemplates[category]) gapTemplates[category] = [];
          gapTemplates[category].push(template);
          break;
      }
    });

    // 4. Return in the expected format (NarrativeDefinitionAPI)
    const payload: any = {
      version: 2, // Hardcoded or fetch from somewhere if tracked
      themeMap,
      maturityThresholds,
      maturityLabels,
      stageRationale,
      priorityWhyTemplate,
      notes,
      executiveTemplates,
      dimensionTemplates,
      gapTemplates
    };

    if (gapThresholds) payload.gapThresholds = gapThresholds;
    if (headlines) payload.headlines = headlines;
    if (executiveSummary) payload.executiveSummary = executiveSummary;

    res.json(payload);

    logger.info('Narrative definition retrieved successfully');
  } catch (error) {
    logger.error('Error fetching narrative definition:', error);
    next(error);
  }
});

export default router;


==================================================
FILE: src/routes/public/participants.ts
==================================================

/**
 * File: src/routes/public/participants.ts
 * Purpose: Participant registration and management
 */

import { Router } from 'express';
import { query } from '../../config/database';
import { logger } from '../../utils/logger';
import { z } from 'zod';
import crypto from 'crypto';

const router = Router();

// Zod schema for participant validation
const createParticipantSchema = z.object({
  email: z.string().email(),
  fullName: z.string().min(2),
  companyName: z.string().optional(),
  jobTitle: z.string().optional(),
  phone: z.string().optional(),
  industry: z.string().optional(),
  companySize: z.string().optional(),
  country: z.string().optional(),
  consentGiven: z.boolean(),
});

router.post('/', async (req, res, next) => {
  try {
    // Validate request body
    const validated = createParticipantSchema.parse(req.body);

    if (!validated.consentGiven) {
      return res.status(400).json({ 
        success: false,
        error: 'Consent is required to participate' 
      });
    }

    // Check if participant exists
    const existingCheck = await query(
      'SELECT id, email, full_name, company_name FROM participants WHERE email = $1',
      [validated.email]
    );

    if (existingCheck.rows.length > 0) {
      const existing = existingCheck.rows[0];
      const participantToken = req.header('x-participant-token');
      if (!participantToken) {
        return res.status(403).json({ success: false, error: 'Forbidden' });
      }

      const tokenValidation = await query(
        `SELECT id
         FROM participants
         WHERE id = $1
           AND participant_token = $2`,
        [existing.id, participantToken]
      );

      if (tokenValidation.rows.length === 0) {
        return res.status(403).json({ success: false, error: 'Forbidden' });
      }
      
      // Update existing participant info to capture latest details
      await query(
        `UPDATE participants 
         SET full_name = $1, company_name = $2, job_title = $3, 
             industry = $4, phone = $5, company_size = $6, 
             country = $7, updated_at = NOW()
         WHERE id = $8`,
        [
          validated.fullName,
          validated.companyName || null,
          validated.jobTitle || null,
          validated.industry || null,
          validated.phone || null,
          validated.companySize || null,
          validated.country || null,
          existing.id,
        ]
      );

      return res.status(200).json({
        success: true,
        data: {
          participantId: existing.id,
          email: existing.email,
          fullName: validated.fullName,
          message: 'Participant updated successfully',
        },
      });
    }

    // Create new participant
    const participantToken = crypto.randomBytes(32).toString('hex');
    const result = await query(
      `INSERT INTO participants 
       (email, full_name, company_name, job_title, phone, industry, company_size, country, consent_given, consent_date, participant_token)
       VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, CURRENT_TIMESTAMP, $10)
       RETURNING id, email, full_name`,
      [
        validated.email,
        validated.fullName,
        validated.companyName || null,
        validated.jobTitle || null,
        validated.phone || null,
        validated.industry || null,
        validated.companySize || null,
        validated.country || null,
        validated.consentGiven,
        participantToken,
      ]
    );

    const participant = result.rows[0];

    logger.info(`New participant created: ${participant.email}`);

    res.status(201).json({
      success: true,
      data: {
        participantId: participant.id,
        email: participant.email,
        fullName: participant.full_name,
        participantToken,
      },
    });
  } catch (error: any) {
    logger.error('Error creating participant:', error);

    if (error.name === 'ZodError') {
      return res.status(400).json({
        success: false,
        error: 'Validation error',
        details: error.errors,
      });
    }

    next(error);
  }
});

export default router;


==================================================
FILE: src/routes/public/recommendations-definition.ts
==================================================

import { Router, Request, Response, NextFunction } from 'express';
import { query } from '../../config/database';
import { logger } from '../../utils/logger';

const router = Router();

/**
 * GET /api/public/recommendations/definition
 * Returns the complete recommendations definition including rules and metadata
 */
router.get('/', async (req: Request, res: Response, next: NextFunction) => {
  try {
    // 1. Get all recommendation rules grouped by dimension
    const rulesResult = await query(`
      SELECT 
        dimension_key,
        json_agg(
          json_build_object(
            'id', rule_key,
            'title', title,
            'description', description,
            'conditions', conditions,
            'why', why,
            'what', what,
            'how', how,
            'priority', priority_score,
            'tags', tags,
            'impactLevel', impact_level,
            'effortLevel', effort_level,
            'timeframe', timeframe,
            'actionItems', action_items,
            'resources', resources,
            'kpis', kpis
          ) ORDER BY priority_score DESC
        ) as recommendations
      FROM recommendation_rules
      GROUP BY dimension_key
    `);

    // 2. Get all metadata
    const metaResult = await query(`
      SELECT meta_key, meta_value
      FROM recommendation_meta
    `);

    const normalizeRecommendation = (recommendation: any) => ({
      ...recommendation,
      why: recommendation.why ?? undefined,
      what: recommendation.what ?? undefined,
      how: recommendation.how ?? undefined,
      resources: Array.isArray(recommendation.resources) ? recommendation.resources : [],
      kpis: Array.isArray(recommendation.kpis) ? recommendation.kpis : [],
      tags: Array.isArray(recommendation.tags) ? recommendation.tags : [],
      actionItems: Array.isArray(recommendation.actionItems) ? recommendation.actionItems : [],
      conditions: recommendation.conditions ?? {},
      priority: parseFloat(
        String(
          recommendation.priority_score ??
            recommendation.priority ??
            0
        )
      ),
    });

    // Transform rules into dimensions array and sanitize nullable fields.
    const dimensions = rulesResult.rows.map((row: any) => ({
      dimensionKey: row.dimension_key,
      recommendations: Array.isArray(row.recommendations)
        ? row.recommendations.map(normalizeRecommendation)
        : [],
    }));

    // Transform metadata into object
    const meta: any = {};
    metaResult.rows.forEach((row: any) => {
      // Convert snake_case to camelCase for consistency
      const camelKey = row.meta_key.replace(/_([a-z])/g, (g: string) => g[1].toUpperCase());
      meta[camelKey] = row.meta_value;
    });

    // 3. Return in the expected format
    res.json({
      dimensions,
      meta
    });

    logger.info('Recommendations definition retrieved successfully');
  } catch (error) {
    logger.error('Error fetching recommendations definition:', error);
    next(error);
  }
});

export default router;


==================================================
FILE: src/routes/public/responses.ts
==================================================

/**
 * File: src/routes/public/responses.ts
 * Purpose: Handle assessment responses, topic answers, and submission
 */

import { Router } from 'express';
import { query, getClient } from '../../config/database';
import { logger } from '../../utils/logger';
import { z } from 'zod';
import { ScoringService } from '../../services/scoringService';
import crypto from 'crypto';
import { requireResponseSession } from '../../middleware/responseSession';
import { AssessmentRepository } from '../../repositories/AssessmentRepository';

const router = Router();
const assessmentRepository = new AssessmentRepository();

const toNullableNumber = (value: unknown): number | null => {
  if (value === null || typeof value === 'undefined') return null;
  if (typeof value === 'number') return Number.isFinite(value) ? value : null;
  const parsed = Number(value);
  return Number.isFinite(parsed) ? parsed : null;
};

const normalizeTopicRecommendation = (rec: any) => ({
  id: rec.id,
  topicId: rec.topicId ?? rec.topic_id ?? '',
  scoreMin: toNullableNumber(rec.scoreMin ?? rec.score_min),
  scoreMax: toNullableNumber(rec.scoreMax ?? rec.score_max),
  targetMin: toNullableNumber(rec.targetMin ?? rec.target_min),
  targetMax: toNullableNumber(rec.targetMax ?? rec.target_max),
  gapMin: toNullableNumber(rec.gapMin ?? rec.gap_min),
  gapMax: toNullableNumber(rec.gapMax ?? rec.gap_max),
  title: rec.title,
  description: rec.description ?? null,
  why: rec.why ?? null,
  what: rec.what ?? null,
  how: rec.how ?? null,
  actionItems: Array.isArray(rec.actionItems) ? rec.actionItems : Array.isArray(rec.action_items) ? rec.action_items : [],
  category: rec.category,
  priority: Number(rec.priority ?? 0),
  tags: Array.isArray(rec.tags) ? rec.tags : [],
  isActive: typeof rec.isActive === 'boolean' ? rec.isActive : rec.is_active ?? true,
  orderIndex: Number(rec.orderIndex ?? rec.order_index ?? 0),
});

// Validation Schemas
const startResponseSchema = z.object({
  assessmentId: z.string().uuid(),
  participantId: z.string().uuid()
});

const answerTopicSchema = z.object({
  topicId: z.string().uuid(),
  currentRating: z.number().min(1).max(5),
  targetRating: z.number().min(1).max(5),
  timeSpentSeconds: z.number().optional(),
  notes: z.string().optional()
});

// POST /start
router.post('/start', async (req, res, next) => {
  try {
    const validated = startResponseSchema.parse(req.body);

    const topicCountResult = await query(
      `SELECT COUNT(*) as total FROM topics t 
       JOIN dimensions d ON t.dimension_id = d.id 
       WHERE d.assessment_id = $1`,
      [validated.assessmentId]
    );

    const totalQuestions = parseInt(topicCountResult.rows[0].total);
    const sessionToken = crypto.randomBytes(32).toString('hex');

    const result = await query(
      `INSERT INTO assessment_responses 
        (assessment_id, participant_id, total_questions, session_token, ip_address, user_agent, status)
       VALUES ($1, $2, $3, $4, $5, $6, 'in_progress')
       RETURNING id, session_token, status`,
      [
        validated.assessmentId, 
        validated.participantId, 
        totalQuestions, 
        sessionToken, 
        req.ip, 
        req.get('User-Agent')
      ]
    );

    res.status(201).json({
      success: true,
      data: {
        responseId: result.rows[0].id,
        sessionToken: result.rows[0].session_token
      }
    });
  } catch (error) {
    logger.error('Error starting assessment:', error);
    if (error instanceof z.ZodError) {
      return res.status(400).json({ success: false, error: 'Validation Error', details: error.errors });
    }
    next(error);
  }
});

// PUT /:responseId/answer
router.put('/:responseId/answer', requireResponseSession, async (req, res, next) => {
  try {
    const { responseId } = req.params;
    const validated = answerTopicSchema.parse(req.body);

    // Calculate gap using ScoringService
    const { gap, normalizedGap } = ScoringService.calculateTopicGap(
      validated.currentRating, 
      validated.targetRating
    );

    const upsertedRowCount = await assessmentRepository.upsertTopicResponseForAssessment({
      responseId,
      topicId: validated.topicId,
      currentRating: validated.currentRating,
      targetRating: validated.targetRating,
      gap,
      normalizedGap,
      timeSpentSeconds: validated.timeSpentSeconds || 0,
      notes: validated.notes || null,
    });

    if (upsertedRowCount === 0) {
      return res.status(422).json({
        success: false,
        error: 'Topic does not belong to this assessment',
      });
    }

    // Update progress
    const progressResult = await query(
      `UPDATE assessment_responses
       SET 
         answered_questions = (SELECT COUNT(*) FROM topic_responses WHERE response_id = $1),
         progress_percentage = (
           SELECT (COUNT(*)::DECIMAL / NULLIF(total_questions, 0) * 100)
           FROM assessment_responses ar
           JOIN topic_responses tr ON ar.id = tr.response_id
           WHERE ar.id = $1
           GROUP BY ar.total_questions
         ),
         last_updated_at = NOW()
       WHERE id = $1
       RETURNING answered_questions, progress_percentage`,
      [responseId]
    );

    res.json({
      success: true,
      data: {
        topicResponseId: validated.topicId, // We don't have the ID, returning topicId reference
        gap,
        normalizedGap,
        progress: progressResult.rows[0]?.progress_percentage || 0
      }
    });

  } catch (error) {
    logger.error('Error submitting answer:', error);
    if (error instanceof z.ZodError) {
      return res.status(400).json({ success: false, error: 'Validation Error', details: error.errors });
    }
    next(error);
  }
});

// GET /session/:sessionToken
router.get('/session/:sessionToken', async (req, res, next) => {
  try {
    const { sessionToken } = req.params;

    const sessionResult = await query(
      `SELECT id, status, progress_percentage, answered_questions, total_questions
       FROM assessment_responses 
       WHERE session_token = $1`,
      [sessionToken]
    );

    if (sessionResult.rows.length === 0) {
      return res.status(404).json({ success: false, error: 'Session not found' });
    }

    const responseId = sessionResult.rows[0].id;
    
    const answeredTopicsResult = await query(
      `SELECT topic_id, current_rating, target_rating, gap 
       FROM topic_responses 
       WHERE response_id = $1`,
      [responseId]
    );

    res.json({
      success: true,
      data: {
        responseId,
        status: sessionResult.rows[0].status,
        progress: parseFloat(sessionResult.rows[0].progress_percentage || 0),
        answeredTopics: answeredTopicsResult.rows.map(row => ({
          topicId: row.topic_id,
          currentRating: row.current_rating,
          targetRating: row.target_rating,
          gap: parseFloat(row.gap)
        }))
      }
    });

  } catch (error) {
    logger.error('Error fetching session:', error);
    next(error);
  }
});

// POST /:responseId/complete
router.post('/:responseId/complete', requireResponseSession, async (req, res, next) => {
  try {
    const { responseId } = req.params;

    // Verify completion
    const checkResult = await query(
      `SELECT total_questions, 
        (SELECT COUNT(*) FROM topic_responses WHERE response_id = $1) as answered_count
       FROM assessment_responses 
       WHERE id = $1`,
      [responseId]
    );

    if (checkResult.rows.length === 0) {
      return res.status(404).json({ success: false, error: 'Response not found' });
    }

    const { total_questions, answered_count } = checkResult.rows[0];
    if (parseInt(answered_count) < parseInt(total_questions)) {
      return res.status(400).json({ 
        success: false, 
        error: 'Assessment is incomplete',
        details: { answered: answered_count, total: total_questions }
      });
    }

    // Compute results
    const results = await ScoringService.computeResults(responseId);

    res.json({
      success: true,
      data: {
        responseId,
        completedAt: new Date().toISOString(),
        overallScore: results.overallScore,
        overallGap: results.overallGap
      }
    });

  } catch (error) {
    logger.error('Error completing assessment:', error);
    next(error);
  }
});

// GET /:responseId/results
router.get('/:responseId/results', requireResponseSession, async (req, res, next) => {
  try {
    const { responseId } = req.params;

    const resultsQuery = await query(
      `WITH
         response_check AS (
           SELECT
             ar.*,
             a.title AS assessment_title
           FROM assessment_responses ar
           JOIN assessments a ON a.id = ar.assessment_id
           WHERE ar.id = $1
         ),
         priorities AS (
           SELECT
             cp.priority_score,
             cp.rank_order,
             cp.recommendations,
             cp.dimension_score,
             cp.dimension_gap,
             d.dimension_key,
             d.title,
             d.category,
             d.order_index
           FROM computed_priorities cp
           JOIN dimensions d ON d.id = cp.dimension_id
           WHERE cp.response_id = $1
           ORDER BY cp.rank_order ASC
         ),
         top_gaps AS (
           SELECT
             t.id AS topic_id,
             t.label,
             tr.gap,
             d.title AS dimension_title,
             tr.target_rating
           FROM topic_responses tr
           JOIN topics t ON tr.topic_id = t.id
           JOIN dimensions d ON t.dimension_id = d.id
           WHERE tr.response_id = $1
             AND tr.gap > 0
           ORDER BY tr.gap DESC, tr.target_rating DESC
           LIMIT 5
         )
       SELECT
         (SELECT row_to_json(r) FROM response_check r) AS response,
         (SELECT COALESCE(json_agg(p ORDER BY p.order_index), '[]'::json) FROM priorities p) AS priorities,
         (SELECT COALESCE(json_agg(g ORDER BY g.gap DESC, g.target_rating DESC), '[]'::json) FROM top_gaps g) AS top_gaps`,
      [responseId]
    );

    const resultRow = resultsQuery.rows[0] as {
      response: {
        status: string;
        overall_score: string | number | null;
        overall_gap: string | number | null;
      } | null;
      priorities: Array<{
        priority_score: string | number;
        rank_order: number;
        recommendations: unknown[] | null;
        dimension_score: string | number;
        dimension_gap: string | number;
        dimension_key: string;
        title: string;
      }> | null;
      top_gaps: Array<{
        topic_id: string;
        label: string;
        gap: string | number;
        dimension_title: string;
      }> | null;
    };

    if (!resultRow?.response) {
      return res.status(404).json({ success: false, error: 'Response not found' });
    }

    if (resultRow.response.status !== 'completed') {
       return res.status(400).json({ success: false, error: 'Assessment not completed' });
    }

    const priorities = Array.isArray(resultRow.priorities) ? resultRow.priorities : [];
    const topGaps = Array.isArray(resultRow.top_gaps) ? resultRow.top_gaps : [];

    const allRecommendations = priorities
      .flatMap((row) => (Array.isArray(row.recommendations) ? row.recommendations.map(normalizeTopicRecommendation) : []))
      .sort((a, b) => Number(b?.priority ?? 0) - Number(a?.priority ?? 0))
      .slice(0, 5);

    res.json({
      success: true,
      data: {
        overallScore: parseFloat(String(resultRow.response.overall_score ?? 0)),
        overallGap: parseFloat(String(resultRow.response.overall_gap ?? 0)),
        dimensions: priorities.map(row => ({
          dimensionKey: row.dimension_key,
          title: row.title,
          score: parseFloat(String(row.dimension_score)),
          gap: parseFloat(String(row.dimension_gap)),
          priorityScore: parseFloat(String(row.priority_score)),
          recommendations: Array.isArray(row.recommendations) ? row.recommendations.map(normalizeTopicRecommendation) : [],
          topics: [] // We could populate this if needed, but not strictly required by prompt example
        })),
        topGaps: topGaps.map(row => ({
          topicId: row.topic_id,
          label: row.label,
          gap: parseFloat(String(row.gap)),
          dimensionTitle: row.dimension_title
        })),
        priorities: priorities.map(row => ({
          dimensionKey: row.dimension_key,
          title: row.title,
          priorityScore: parseFloat(String(row.priority_score)),
          rank: row.rank_order
        })),
        topRecommendations: allRecommendations,
      }
    });

  } catch (error) {
    logger.error('Error fetching results:', error);
    next(error);
  }
});

// GET /:responseId/recommendations
router.get('/:responseId/recommendations', requireResponseSession, async (req, res, next) => {
  try {
    const { responseId } = req.params;

    const recommendationsResult = await query(
      `SELECT 
         d.dimension_key, d.title as dimension_title, cp.priority_score, cp.recommendations
       FROM computed_priorities cp
       JOIN dimensions d ON cp.dimension_id = d.id
       WHERE cp.response_id = $1
       ORDER BY cp.rank_order ASC`,
      [responseId]
    );

    const recommendations = recommendationsResult.rows.map(row => ({
      dimensionKey: row.dimension_key,
      dimensionTitle: row.dimension_title,
      priorityScore: parseFloat(row.priority_score),
      items: row.recommendations || [] // Assuming JSONB is parsed automatically by pg
    }));

    res.json({
      success: true,
      data: {
        recommendations
      }
    });

  } catch (error) {
    logger.error('Error fetching recommendations:', error);
    next(error);
  }
});

export default router;


==================================================
FILE: src/routes/index.ts
==================================================

/**
 * File: src/routes/index.ts
 * Purpose: Aggregates all application routes
 * Updated: Force restart
 */

import { Router } from 'express';

// Admin Routes
import adminAuthRoutes from './admin/auth';
import adminDashboardRoutes from './admin/dashboard';
import adminAssessmentsRoutes from './admin/assessments';
import adminResponsesRoutes from './admin/responses';
import adminAnalyticsRoutes from './admin/analytics';
import adminRecommendationsRoutes from './admin/recommendations';
import adminTopicLevelsRoutes from './admin/topic-levels';
import adminTopicRecommendationsRoutes from './admin/topic-recommendations';

// Public Routes
import publicAssessmentsRoutes from './public/assessments';
import publicParticipantsRoutes from './public/participants';
import publicResponsesRoutes from './public/responses';
import recommendationsDefinitionRoutes from './public/recommendations-definition';
import narrativeDefinitionRoutes from './public/narrative-definition';

const router = Router();

// Admin Routes
router.use('/admin/auth', adminAuthRoutes);
router.use('/admin/dashboard', adminDashboardRoutes);
router.use('/admin/assessments', adminAssessmentsRoutes);
router.use('/admin/responses', adminResponsesRoutes);
router.use('/admin/analytics', adminAnalyticsRoutes);
router.use('/admin/recommendations', adminRecommendationsRoutes);
router.use('/admin/topics', adminTopicLevelsRoutes);
router.use('/admin/topics', adminTopicRecommendationsRoutes);

// Public Routes (No auth required)
router.use('/public/assessments', publicAssessmentsRoutes);
router.use('/public/participants', publicParticipantsRoutes);
router.use('/public/responses', publicResponsesRoutes);
router.use('/public/recommendations/definition', recommendationsDefinitionRoutes);
router.use('/public/narrative/definition', narrativeDefinitionRoutes);

// Health Check
router.get('/health', (req, res) => {
  res.json({ 
    status: 'ok', 
    timestamp: new Date().toISOString() 
  });
});

export default router;


==================================================
FILE: src/services/authService.ts
==================================================

/**
 * File: src/services/authService.ts
 * Purpose: Handles authentication, login, logout, and session management
 */

import bcrypt from 'bcrypt';
import * as jwt from 'jsonwebtoken';
import type { SignOptions } from 'jsonwebtoken';
import { query } from '../config/database';
import { config } from '../config/env';
import { logger } from '../utils/logger';
import { validatePasswordStrength } from '../utils/passwordValidator';

export const AUTH_FAIL_MSG = 'Invalid credentials';

export class AuthService {
  /**
   * Authenticates user and creates session
   */
  static async login(email: string, password: string, ip: string, userAgent: string) {
    const userResult = await query(
      `SELECT id, email, password_hash, full_name, role, is_active, 
              login_attempts, locked_until
       FROM users
       WHERE email = $1`,
      [email]
    );

    if (userResult.rows.length === 0) {
      throw new Error(AUTH_FAIL_MSG);
    }

    const user = userResult.rows[0];

    // Check account lock
    if (user.locked_until && new Date(user.locked_until) > new Date()) {
      logger.warn(`Login attempt on locked account: ${email}`);
      throw new Error(AUTH_FAIL_MSG);
    }

    if (!user.is_active) {
      logger.warn(`Login attempt on deactivated account: ${email}`);
      throw new Error(AUTH_FAIL_MSG);
    }

    const isValid = await bcrypt.compare(password, user.password_hash);

    if (!isValid) {
      const lockUpdateResult = await query(
        `UPDATE users
         SET
           login_attempts = login_attempts + 1,
           locked_until = CASE
             WHEN login_attempts + 1 >= 5
             THEN NOW() + INTERVAL '30 minutes'
             ELSE locked_until
           END
         WHERE email = $1
         RETURNING locked_until, login_attempts`,
        [email]
      );

      if (lockUpdateResult.rows.length > 0 && lockUpdateResult.rows[0].locked_until) {
        logger.warn(`Account lockout triggered for ${email}`);
      }

      throw new Error(AUTH_FAIL_MSG);
    }

    // Reset attempts and update login time
    await query(
      `UPDATE users 
       SET login_attempts = 0, locked_until = NULL, last_login_at = NOW()
       WHERE id = $1`,
      [user.id]
    );

    const token = jwt.sign(
      { userId: user.id, email: user.email, role: user.role },
      config.jwt.secret,
      { expiresIn: config.jwt.expiresIn as SignOptions['expiresIn'] }
    );

    const expiresAt = new Date(Date.now() + 24 * 60 * 60 * 1000); // 24h
    await query(
      `INSERT INTO admin_sessions (user_id, token, ip_address, user_agent, expires_at)
       VALUES ($1, $2, $3, $4, $5)`,
      [user.id, token, ip, userAgent, expiresAt]
    );

    logger.info(`User logged in: ${user.email}`);

    return {
      token,
      user: {
        id: user.id,
        email: user.email,
        full_name: user.full_name,
        role: user.role,
      },
    };
  }

  static async logout(token: string) {
    await query('DELETE FROM admin_sessions WHERE token = $1', [token]);
    logger.info('User logged out');
  }

  static async verifyToken(token: string) {
    const result = await query(
      `SELECT s.*, u.email, u.full_name, u.role
       FROM admin_sessions s
       JOIN users u ON s.user_id = u.id
       WHERE s.token = $1 AND s.expires_at > NOW()`,
      [token]
    );

    if (result.rows.length === 0) {
      return null;
    }

    return result.rows[0];
  }

  static async createUser(
    email: string,
    password: string,
    fullName: string,
    role: string = 'admin'
  ) {
    const passwordCheck = validatePasswordStrength(password);
    if (!passwordCheck.isStrong) {
      throw new Error(`Weak password: ${passwordCheck.feedback.join('; ')}`);
    }

    const hashedPassword = await bcrypt.hash(password, 10);

    const result = await query(
      `INSERT INTO users (email, password_hash, full_name, role)
       VALUES ($1, $2, $3, $4)
       RETURNING id, email, full_name, role`,
      [email, hashedPassword, fullName, role]
    );

    logger.info(`New user created: ${email}`);

    return result.rows[0];
  }
}


==================================================
FILE: src/services/scoringService.ts
==================================================

/**
 * File: src/services/scoringService.ts
 * Purpose: Handles assessment scoring logic, gap calculation, and ranking
 */

import { query, getClient } from '../config/database';
import { logger } from '../utils/logger';

export interface TopicGap {
  gap: number;
  normalizedGap: number;
}

export interface DimensionMetrics {
  score: number;
  gap: number;
}

export interface OverallMetrics {
  overallScore: number;
  overallGap: number;
}

export interface Recommendation {
  id: string;
  title: string;
  description: string | null;
  why: string | null;
  what: string | null;
  how: string | null;
  action_items: string[];
  category: 'Quick Win' | 'Project' | 'Big Bet';
  priority: number;
  tags: string[];
  topicId: string;
  topicKey: string;
}

export class ScoringService {
  private static toFiniteNumber(value: unknown): number | null {
    if (typeof value === 'number') {
      return Number.isFinite(value) ? value : null;
    }
    if (typeof value === 'string' && value.trim() !== '') {
      const parsed = Number(value);
      return Number.isFinite(parsed) ? parsed : null;
    }
    return null;
  }

  /**
   * Calculate gap for a single topic
   * gap = target - current
   * normalizedGap = gap > 0 ? gap : 0 (overperformance = 0)
   */
  static calculateTopicGap(current: number, target: number): TopicGap {
    const gap = target - current;
    const normalizedGap = gap > 0 ? gap : 0;
    return { gap, normalizedGap };
  }

  /**
   * Calculate dimension score and gap
   * score = average of all current ratings in dimension
   * gap = average of all normalized gaps in dimension
   */
  static calculateDimensionMetrics(topics: Array<{
    currentRating: number;
    targetRating: number;
  }>): DimensionMetrics {
    if (topics.length === 0) {
      return { score: 0, gap: 0 };
    }

    let totalCurrent = 0;
    let totalNormalizedGap = 0;
    let validTopics = 0;

    for (const topic of topics) {
      if (!Number.isFinite(topic.currentRating) || !Number.isFinite(topic.targetRating)) {
        continue;
      }

      totalCurrent += topic.currentRating;
      const { normalizedGap } = this.calculateTopicGap(topic.currentRating, topic.targetRating);
      totalNormalizedGap += normalizedGap;
      validTopics++;
    }

    if (validTopics === 0) {
      return { score: 0, gap: 0 };
    }

    const score = parseFloat((totalCurrent / validTopics).toFixed(2));
    const gap = parseFloat((totalNormalizedGap / validTopics).toFixed(2));

    return { score, gap };
  }

  /**
   * Calculate priority score for a dimension
   * priorityScore = gap * impact_weight
   * For now, use: priorityScore = gap (simple version)
   */
  static calculatePriorityScore(gap: number, impact: number = 1): number {
    return parseFloat(gap.toFixed(2));
  }

  /**
   * Calculate overall assessment metrics
   */
  static calculateOverallMetrics(dimensions: Array<{
    score: number;
    gap: number;
  }>): OverallMetrics {
    if (dimensions.length === 0) {
      return { overallScore: 0, overallGap: 0 };
    }

    let totalScore = 0;
    let totalGap = 0;

    for (const dim of dimensions) {
      totalScore += dim.score;
      totalGap += dim.gap;
    }

    return {
      overallScore: parseFloat((totalScore / dimensions.length).toFixed(2)),
      overallGap: parseFloat((totalGap / dimensions.length).toFixed(2))
    };
  }

  private static toNullableNumber(value: unknown): number | null {
    if (value === null || typeof value === 'undefined') {
      return null;
    }

    if (typeof value === 'number') {
      return Number.isFinite(value) ? value : null;
    }

    const parsed = Number(value);
    return Number.isFinite(parsed) ? parsed : null;
  }

  private static matchesConditions(
    rec: {
      score_min: number | null;
      score_max: number | null;
      target_min: number | null;
      target_max: number | null;
      gap_min: number | null;
      gap_max: number | null;
    },
    score: number,
    target: number,
    gap: number
  ): boolean {
    return (
      (rec.score_min == null || score >= rec.score_min) &&
      (rec.score_max == null || score <= rec.score_max) &&
      (rec.target_min == null || target >= rec.target_min) &&
      (rec.target_max == null || target <= rec.target_max) &&
      (rec.gap_min == null || gap >= rec.gap_min) &&
      (rec.gap_max == null || gap <= rec.gap_max)
    );
  }

  static async computeResults(responseId: string) {
    const client = await getClient();

    try {
      await client.query('BEGIN');

      // 1. Get all topic responses for calculation
      const topicResponses = await client.query(
        `SELECT 
          tr.id, tr.topic_id, tr.current_rating, tr.target_rating,
          t.topic_key, t.dimension_id, d.title as dimension_title, d.order_index
         FROM topic_responses tr
         JOIN topics t ON tr.topic_id = t.id
         JOIN dimensions d ON t.dimension_id = d.id
         WHERE tr.response_id = $1
         ORDER BY d.order_index, t.order_index`,
        [responseId]
      );

      // Group by dimension
      const dimensionsMap = new Map<string, {
        id: string;
        title: string;
        order: number;
        topics: Array<{
          topicId: string;
          topicKey: string;
          currentRating: number;
          targetRating: number;
        }>;
      }>();

      for (const row of topicResponses.rows) {
        if (!dimensionsMap.has(row.dimension_id)) {
          dimensionsMap.set(row.dimension_id, {
            id: row.dimension_id,
            title: row.dimension_title,
            order: row.order_index,
            topics: []
          });
        }
        dimensionsMap.get(row.dimension_id)?.topics.push({
          topicId: row.topic_id,
          topicKey: row.topic_key,
          currentRating: this.toFiniteNumber(row.current_rating) ?? NaN,
          targetRating: this.toFiniteNumber(row.target_rating) ?? NaN,
        });
      }

      const topicIds = Array.from(
        new Set(topicResponses.rows.map((row: { topic_id: string }) => row.topic_id))
      );

      const recsByTopic = new Map<string, any[]>();

      if (topicIds.length > 0) {
        const allTopicRecs = await client.query(
          `SELECT 
             tr.id,
             tr.topic_id,
             tr.score_min, tr.score_max,
             tr.target_min, tr.target_max,
             tr.gap_min, tr.gap_max,
             tr.title, tr.description,
             tr.why, tr.what, tr.how,
             tr.action_items,
             tr.category, tr.priority, tr.tags,
             t.dimension_id,
             t.topic_key
           FROM topic_recommendations tr
           JOIN topics t ON t.id = tr.topic_id
           WHERE tr.topic_id = ANY($1::uuid[])
             AND tr.is_active = true
           ORDER BY tr.priority DESC, tr.order_index ASC`,
          [topicIds]
        );

        for (const rec of allTopicRecs.rows) {
          if (!recsByTopic.has(rec.topic_id)) {
            recsByTopic.set(rec.topic_id, []);
          }

          recsByTopic.get(rec.topic_id)?.push({
            ...rec,
            score_min: this.toNullableNumber(rec.score_min),
            score_max: this.toNullableNumber(rec.score_max),
            target_min: this.toNullableNumber(rec.target_min),
            target_max: this.toNullableNumber(rec.target_max),
            gap_min: this.toNullableNumber(rec.gap_min),
            gap_max: this.toNullableNumber(rec.gap_max),
            priority: Number(rec.priority ?? 0),
          });
        }
      }

      const matchedRecommendationsByDimension = new Map<string, Recommendation[]>();

      for (const row of topicResponses.rows) {
        const score = this.toFiniteNumber(row.current_rating);
        const target = this.toFiniteNumber(row.target_rating);
        if (score === null || target === null) {
          continue;
        }
        const gap = Number(Math.max(0, target - score).toFixed(2));
        const topicRecs = recsByTopic.get(row.topic_id) || [];

        for (const rec of topicRecs) {
          if (!this.matchesConditions(rec, score, target, gap)) {
            continue;
          }

          const dimensionRecommendations = matchedRecommendationsByDimension.get(row.dimension_id) || [];
          dimensionRecommendations.push({
            id: rec.id,
            title: rec.title,
            description: rec.description ?? null,
            why: rec.why ?? null,
            what: rec.what ?? null,
            how: rec.how ?? null,
            action_items: Array.isArray(rec.action_items) ? rec.action_items : [],
            category: rec.category,
            priority: Number(rec.priority ?? 0),
            tags: Array.isArray(rec.tags) ? rec.tags : [],
            topicId: rec.topic_id,
            topicKey: rec.topic_key,
          });
          matchedRecommendationsByDimension.set(row.dimension_id, dimensionRecommendations);
        }
      }

      for (const [dimensionId, recommendations] of matchedRecommendationsByDimension.entries()) {
        const sorted = recommendations.sort((a, b) => b.priority - a.priority).slice(0, 5);
        matchedRecommendationsByDimension.set(dimensionId, sorted);
      }

      const calculatedDimensions = [];

      // 2. Calculate metrics for each dimension
      let rankCounter = 0;
      // Convert map to array to sort by whatever we need later, but first calc scores
      const dimArray = Array.from(dimensionsMap.values());
      
      for (const dim of dimArray) {
        const { score, gap } = this.calculateDimensionMetrics(dim.topics);
        const priorityScore = this.calculatePriorityScore(gap);

        calculatedDimensions.push({
          ...dim,
          score,
          gap,
          priorityScore
        });
      }

      // Sort by priority score DESC for ranking
      calculatedDimensions.sort((a, b) => b.priorityScore - a.priorityScore);

      if (calculatedDimensions.length > 0) {
        const values: string[] = [];
        const params: unknown[] = [];
        let paramIndex = 1;

        for (let i = 0; i < calculatedDimensions.length; i++) {
          const dim = calculatedDimensions[i];
          const rank = i + 1;
          const recommendations = matchedRecommendationsByDimension.get(dim.id) || [];

          values.push(
            `($${paramIndex++}, $${paramIndex++}, $${paramIndex++}, $${paramIndex++}, $${paramIndex++}, $${paramIndex++}, $${paramIndex++}, NOW())`
          );
          params.push(
            responseId,
            dim.id,
            dim.score,
            dim.gap,
            dim.priorityScore,
            rank,
            JSON.stringify(recommendations)
          );
        }

        await client.query(
          `INSERT INTO computed_priorities
            (response_id, dimension_id, dimension_score, dimension_gap, priority_score, rank_order, recommendations, computed_at)
           VALUES ${values.join(', ')}
           ON CONFLICT (response_id, dimension_id) DO UPDATE SET
             dimension_score = EXCLUDED.dimension_score,
             dimension_gap = EXCLUDED.dimension_gap,
             priority_score = EXCLUDED.priority_score,
             rank_order = EXCLUDED.rank_order,
             recommendations = EXCLUDED.recommendations,
             computed_at = NOW()`,
          params
        );
      }

      // 4. Calculate overall metrics
      const { overallScore, overallGap } = this.calculateOverallMetrics(calculatedDimensions);

      // 5. Update assessment response
      await client.query(
        `UPDATE assessment_responses
         SET 
           status = 'completed',
           completed_at = NOW(),
           overall_score = $2,
           overall_gap = $3,
           last_updated_at = NOW()
         WHERE id = $1`,
        [
          responseId,
          overallScore,
          overallGap,
        ]
      );

      await client.query('COMMIT');

      logger.info(`Results computed for response: ${responseId}`);

      return {
        overallScore,
        overallGap,
        dimensions: calculatedDimensions.map(d => ({
          dimensionId: d.id,
          title: d.title,
          score: d.score,
          gap: d.gap,
          priorityScore: d.priorityScore
        }))
      };

    } catch (error) {
      await client.query('ROLLBACK');
      logger.error('Error computing results:', error);
      throw error;
    } finally {
      client.release();
    }
  }

  static async getTopPriorities(responseId: string, limit: number = 5) {
    const result = await query(
      `SELECT 
        cp.*,
        d.title as dimension_title,
        d.category
       FROM computed_priorities cp
       JOIN dimensions d ON cp.dimension_id = d.id
       WHERE cp.response_id = $1
       ORDER BY cp.priority_score DESC, cp.rank_order ASC
       LIMIT $2`,
      [responseId, limit]
    );

    return result.rows;
  }
}


==================================================
FILE: src/tests/security.test.ts
==================================================

﻿import assert from 'assert';
import fs from 'fs';
import os from 'os';
import path from 'path';
import { requireRole } from '../middleware/rbac';
import { csrfProtection } from '../middleware/csrf';
import { requireResponseSession } from '../middleware/responseSession';
import { validatePasswordStrength } from '../utils/passwordValidator';
import * as database from '../config/database';
import adminAuthRouter from '../routes/admin/auth';
import publicResponsesRouter from '../routes/public/responses';
import { AuthService, AUTH_FAIL_MSG } from '../services/authService';
import { AssessmentRepository } from '../repositories/AssessmentRepository';

interface MockState {
  statusCode?: number;
  payload?: unknown;
}

const mockResponse = () => {
  const state: MockState = {};
  const res = {
    status(code: number) {
      state.statusCode = code;
      return this;
    },
    json(payload: unknown) {
      state.payload = payload;
      return this;
    },
  };

  return { res, state };
};

const getRouteHandler = (
  router: { stack: Array<{ route?: { path: string; methods: Record<string, boolean>; stack: Array<{ handle: Function }> } }> },
  routePath: string,
  method: 'get' | 'post' | 'put',
  stackIndex: number
): Function => {
  const routeLayer = router.stack.find(
    (layer) => layer.route?.path === routePath && Boolean(layer.route.methods[method])
  );

  if (!routeLayer || !routeLayer.route) {
    throw new Error(`Route handler not found for ${method.toUpperCase()} ${routePath}`);
  }

  return routeLayer.route.stack[stackIndex].handle;
};

const asMutableDatabase = (): { query: typeof database.query } =>
  database as unknown as { query: typeof database.query };

const testRequireRoleRejectsUnauthorizedRole = async () => {
  const middleware = requireRole('admin');
  const { res, state } = mockResponse();
  let nextCalled = false;

  middleware(
    {
      user: { role: 'creator', userId: 'u1', email: 'a@b.com', fullName: 'A B' },
    } as never,
    res as never,
    () => {
      nextCalled = true;
    }
  );

  assert.equal(nextCalled, false, 'next() should not be called for disallowed roles');
  assert.equal(state.statusCode, 403, 'middleware should return HTTP 403');
};

const testCsrfProtectionRejectsMissingToken = async () => {
  const { res, state } = mockResponse();
  let nextCalled = false;

  csrfProtection(
    {
      method: 'POST',
      headers: {},
      header: () => undefined,
    } as never,
    res as never,
    () => {
      nextCalled = true;
    }
  );

  assert.equal(nextCalled, false, 'next() should not be called when CSRF token is missing');
  assert.equal(state.statusCode, 403, 'middleware should return HTTP 403');
};

const testResponseSessionMiddlewareRejectsMissingToken = async () => {
  const { res, state } = mockResponse();
  let nextCalled = false;

  await requireResponseSession(
    {
      params: { responseId: '11111111-1111-1111-1111-111111111111' },
      header: () => undefined,
    } as never,
    res as never,
    () => {
      nextCalled = true;
    }
  );

  assert.equal(nextCalled, false, 'next() should not be called when response session token is missing');
  assert.equal(state.statusCode, 401, 'missing response session token should return HTTP 401');
};

const testResponseSessionMiddlewareRejectsMismatch = async () => {
  const mutableDb = asMutableDatabase();
  const originalQuery = mutableDb.query;
  mutableDb.query = (async () => ({ rows: [], rowCount: 0 } as never)) as typeof database.query;

  try {
    const { res, state } = mockResponse();
    let nextCalled = false;

    await requireResponseSession(
      {
        params: { responseId: '11111111-1111-1111-1111-111111111111' },
        header: () => 'invalid-token',
      } as never,
      res as never,
      () => {
        nextCalled = true;
      }
    );

    assert.equal(nextCalled, false, 'next() should not be called when response session token mismatches');
    assert.equal(state.statusCode, 403, 'mismatched response session token should return HTTP 403');
  } finally {
    mutableDb.query = originalQuery;
  }
};

const testCrossAssessmentTopicInjectionReturns422 = async () => {
  const originalMethod = AssessmentRepository.prototype.upsertTopicResponseForAssessment;
  AssessmentRepository.prototype.upsertTopicResponseForAssessment = async () => 0;

  try {
    const answerHandler = getRouteHandler(
      publicResponsesRouter as unknown as {
        stack: Array<{ route?: { path: string; methods: Record<string, boolean>; stack: Array<{ handle: Function }> } }>;
      },
      '/:responseId/answer',
      'put',
      1
    );

    const { res, state } = mockResponse();
    let nextCalled = false;

    await answerHandler(
      {
        params: { responseId: '11111111-1111-1111-1111-111111111111' },
        body: {
          topicId: '22222222-2222-2222-2222-222222222222',
          currentRating: 2,
          targetRating: 4,
          timeSpentSeconds: 12,
          notes: 'test',
        },
      },
      res,
      () => {
        nextCalled = true;
      }
    );

    assert.equal(nextCalled, false, 'next() should not be called for topic/assessment mismatch');
    assert.equal(state.statusCode, 422, 'cross-assessment topic submission must return HTTP 422');
  } finally {
    AssessmentRepository.prototype.upsertTopicResponseForAssessment = originalMethod;
  }
};

const testAuthLockoutReturnsGenericMessage = async () => {
  const loginHandler = getRouteHandler(
    adminAuthRouter as unknown as {
      stack: Array<{ route?: { path: string; methods: Record<string, boolean>; stack: Array<{ handle: Function }> } }>;
    },
    '/login',
    'post',
    1
  );

  const originalLogin = AuthService.login;
  AuthService.login = (async () => {
    throw new Error('Account locked until tomorrow');
  }) as typeof AuthService.login;

  try {
    let sixthState: MockState | null = null;

    for (let attempt = 1; attempt <= 6; attempt += 1) {
      const { res, state } = mockResponse();
      await loginHandler(
        {
          body: { email: 'admin@leadership.com', password: 'WrongPassword123!' },
          ip: '127.0.0.1',
          headers: { 'user-agent': 'security-test' },
        },
        res,
        () => undefined
      );

      assert.equal(state.statusCode, 401, `attempt ${attempt} should return HTTP 401`);
      assert.deepEqual(state.payload, { success: false, error: AUTH_FAIL_MSG });

      if (attempt === 6) {
        sixthState = state;
      }
    }

    assert.ok(sixthState, 'sixth attempt must be captured');
  } finally {
    AuthService.login = originalLogin;
  }
};

const testMigrationRunnerIsIdempotent = async () => {
  const migrationRunner = require('../../migrations/run') as {
    applyMigrations: (migrationsDir?: string) => Promise<void>;
  };

  const tmpDir = fs.mkdtempSync(path.join(os.tmpdir(), 'migration-idempotency-'));
  const migrationPath = path.join(tmpDir, '009_test.sql');
  fs.writeFileSync(migrationPath, 'BEGIN;\nSELECT 1;\nCOMMIT;\n', 'utf-8');

  const applied = new Set<string>();
  let executedSqlMigrationBodyCount = 0;

  const mutablePool = database.pool as unknown as {
    connect: typeof database.pool.connect;
  };
  const originalConnect = mutablePool.connect;

  const mockClient = {
    query: async (queryText: string, params?: unknown[]) => {
      const normalized = queryText.trim();

      if (normalized.startsWith('CREATE TABLE IF NOT EXISTS schema_migrations')) {
        return { rows: [], rowCount: 0 } as never;
      }

      if (normalized === 'SELECT filename FROM schema_migrations') {
        return {
          rows: Array.from(applied).map((filename) => ({ filename })),
          rowCount: applied.size,
        } as never;
      }

      if (normalized.startsWith('INSERT INTO schema_migrations')) {
        applied.add(String(params?.[0]));
        return { rows: [], rowCount: 1 } as never;
      }

      if (normalized === 'BEGIN' || normalized === 'COMMIT' || normalized === 'ROLLBACK') {
        return { rows: [], rowCount: 0 } as never;
      }

      executedSqlMigrationBodyCount += 1;
      return { rows: [], rowCount: 0 } as never;
    },
    release: () => undefined,
  };

  mutablePool.connect = (async () => mockClient as never) as typeof database.pool.connect;

  try {
    await migrationRunner.applyMigrations(tmpDir);
    await migrationRunner.applyMigrations(tmpDir);

    assert.equal(executedSqlMigrationBodyCount, 1, 'migration SQL body should execute only once across two runs');
    assert.equal(applied.size, 1, 'schema_migrations should contain one applied migration');
  } finally {
    mutablePool.connect = originalConnect;
    fs.rmSync(tmpDir, { recursive: true, force: true });
  }
};

const testPasswordStrengthValidation = async () => {
  const weak = validatePasswordStrength('password123');
  assert.equal(weak.isStrong, false, 'weak password should be rejected');

  const strong = validatePasswordStrength('Str0ng!Mosaic#2026');
  assert.equal(strong.isStrong, true, 'strong password should be accepted');
};

const run = async () => {
  await testRequireRoleRejectsUnauthorizedRole();
  await testCsrfProtectionRejectsMissingToken();
  await testResponseSessionMiddlewareRejectsMissingToken();
  await testResponseSessionMiddlewareRejectsMismatch();
  await testCrossAssessmentTopicInjectionReturns422();
  await testAuthLockoutReturnsGenericMessage();
  await testMigrationRunnerIsIdempotent();
  await testPasswordStrengthValidation();
  console.log('Security tests passed');
};

run().catch((error) => {
  console.error('Security tests failed:', error);
  process.exit(1);
});


==================================================
FILE: src/types/index.ts
==================================================

/**
 * File: src/types/index.ts
 * Purpose: Common type definitions for the application
 */

export interface User {
  id: string;
  email: string;
  full_name: string;
  role: string;
  is_active: boolean;
  last_login_at?: Date;
  created_at: Date;
  updated_at: Date;
}

export interface Assessment {
  id: string;
  title: string;
  description?: string;
  version: number;
  is_active: boolean;
  is_published: boolean;
  published_at?: Date;
  created_by: string;
  created_at: Date;
  updated_at: Date;
}

export interface Dimension {
  id: string;
  assessment_id: string;
  dimension_key: string;
  title: string;
  description?: string;
  category: string;
  order_index: number;
}

export interface Topic {
  id: string;
  dimension_id: string;
  topic_key: string;
  label: string;
  prompt: string;
  order_index: number;
  help_text?: string;
  example?: string;
}

export interface Participant {
  id: string;
  email: string;
  full_name: string;
  company_name?: string;
  job_title?: string;
  industry?: string;
  phone?: string;
  company_size?: string;
  country?: string;
  consent_given: boolean;
  consent_date?: Date;
}


==================================================
FILE: src/utils/helpers.ts
==================================================

/**
 * File: src/utils/helpers.ts
 * Purpose: Utility functions for tokens, validation, formatting, etc.
 */

import crypto from 'crypto';

export const generateToken = (length: number = 32): string => {
  return crypto.randomBytes(length).toString('hex');
};

export const generateSessionToken = (): string => {
  return generateToken(32);
};

export const validateEmail = (email: string): boolean => {
  const emailRegex = /^[^\s@]+@[^\s@]+\.[^\s@]+$/;
  return emailRegex.test(email);
};

export const sanitizeInput = (input: string): string => {
  return input.trim().replace(/[<>]/g, '');
};

export const formatDate = (date: Date): string => {
  return date.toISOString().split('T')[0];
};

export const calculatePercentage = (
  value: number, 
  total: number
): number => {
  if (total === 0) return 0;
  return Math.round((value / total) * 100 * 100) / 100;
};

export const sleep = (ms: number): Promise<void> => {
  return new Promise(resolve => setTimeout(resolve, ms));
};


==================================================
FILE: src/utils/logger.ts
==================================================

/**
 * File: src/utils/logger.ts
 * Purpose: Configures Winston logger for application logging
 */

import winston from 'winston';
import { config } from '../config/env';
import fs from 'fs';
import path from 'path';

const logDir = path.join(process.cwd(), 'logs');

// Create logs directory if it doesn't exist
if (!fs.existsSync(logDir)) {
  fs.mkdirSync(logDir);
}

const logFormat = winston.format.combine(
  winston.format.timestamp({ format: 'YYYY-MM-DD HH:mm:ss' }),
  winston.format.errors({ stack: true }),
  winston.format.splat(),
  winston.format.json()
);

export const logger = winston.createLogger({
  level: config.nodeEnv === 'production' ? 'info' : 'debug',
  format: logFormat,
  transports: [
    new winston.transports.Console({
      format: winston.format.combine(
        winston.format.colorize(),
        winston.format.printf(
          (info) => `${info.timestamp} ${info.level}: ${info.message}`
        )
      ),
    }),
    new winston.transports.File({ 
      filename: path.join(logDir, 'error.log'), 
      level: 'error' 
    }),
    new winston.transports.File({ 
      filename: path.join(logDir, 'combined.log') 
    }),
  ],
});


==================================================
FILE: src/utils/passwordValidator.ts
==================================================

export interface PasswordStrength {
  score: number;
  feedback: string[];
  isStrong: boolean;
}

const COMMON_WEAK_PATTERNS = [
  /password/i,
  /qwerty/i,
  /12345/,
  /letmein/i,
  /admin/i,
  /welcome/i,
];

export const validatePasswordStrength = (password: string): PasswordStrength => {
  const feedback: string[] = [];
  let score = 0;

  if (password.length >= 12) {
    score++;
  } else {
    feedback.push('Password must be at least 12 characters');
  }

  if (/[a-z]/.test(password)) {
    score++;
  } else {
    feedback.push('Password must contain a lowercase letter');
  }

  if (/[A-Z]/.test(password)) {
    score++;
  } else {
    feedback.push('Password must contain an uppercase letter');
  }

  if (/[0-9]/.test(password)) {
    score++;
  } else {
    feedback.push('Password must contain a number');
  }

  if (/[^a-zA-Z0-9]/.test(password)) {
    score++;
  } else {
    feedback.push('Password must contain a special character');
  }

  if (COMMON_WEAK_PATTERNS.some((pattern) => pattern.test(password))) {
    feedback.push('Password contains common weak patterns');
    score = Math.max(0, score - 2);
  }

  return {
    score: Math.min(4, Math.floor((score / 5) * 4)),
    feedback,
    isStrong: feedback.length === 0,
  };
};


==================================================
FILE: src/index.ts
==================================================

﻿/**
 * File: src/index.ts
 * Purpose: Application entry point and server startup
 */

import express, { Application } from 'express';
import cors from 'cors';
import helmet from 'helmet';
import rateLimit from 'express-rate-limit';
import fs from 'fs';
import path from 'path';
import { config } from './config/env';
import { logger } from './utils/logger';
import { errorHandler, notFoundHandler } from './middleware/errorHandler';
import { issueCsrfToken } from './middleware/csrf';
import { query } from './config/database';

import publicRoutes from './routes/public';
import adminRoutes from './routes/admin';

const app: Application = express();
const MIGRATIONS_TABLE_SQL = `
CREATE TABLE IF NOT EXISTS schema_migrations (
  filename TEXT PRIMARY KEY,
  applied_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);
`;

const isMigrationFilename = (filename: string): boolean => {
  if (filename === 'run.ts' || filename === 'run.js') {
    return false;
  }
  return /^\d+.*\.(sql|ts)$/i.test(filename);
};

const verifyMigrationsApplied = async (): Promise<void> => {
  const migrationsDir = path.resolve(process.cwd(), 'migrations');
  if (!fs.existsSync(migrationsDir)) {
    logger.error('[CRITICAL] Migrations directory not found', { migrationsDir });
    if (config.requireMigrations) {
      throw new Error('Migrations directory is required but missing');
    }
    return;
  }

  const expectedMigrations = fs
    .readdirSync(migrationsDir)
    .filter(isMigrationFilename)
    .sort((a, b) => a.localeCompare(b, undefined, { numeric: true }));

  if (expectedMigrations.length === 0) {
    return;
  }

  try {
    await query(MIGRATIONS_TABLE_SQL);
    const appliedResult = await query('SELECT filename FROM schema_migrations');
    const applied = new Set(appliedResult.rows.map((row) => row.filename as string));
    const missing = expectedMigrations.filter((name) => !applied.has(name));

    if (missing.length > 0) {
      logger.error('[CRITICAL] Pending migrations detected', { missing });
      if (config.requireMigrations) {
        throw new Error('Pending migrations detected and REQUIRE_MIGRATIONS=true');
      }
    }
  } catch (error) {
    logger.error('[CRITICAL] Unable to validate migration state', error);
    if (config.requireMigrations) {
      throw error;
    }
  }
};

app.use(helmet());
app.use(cors(config.cors));

const limiter = rateLimit({
  windowMs: config.rateLimit.windowMs,
  max: config.rateLimit.max,
  message: 'Too many requests from this IP, please try again later.',
  standardHeaders: true,
  legacyHeaders: false,
});

const authLimiter = rateLimit({
  windowMs: 15 * 60 * 1000,
  max: 10,
  standardHeaders: true,
  legacyHeaders: false,
  message: { success: false, error: 'Too many attempts, please try again later' },
  skipSuccessfulRequests: true,
});

app.use('/api/admin/auth/login', authLimiter);
app.use('/api/', limiter);

app.use(express.json({ limit: '10mb' }));
app.use(express.urlencoded({ extended: true, limit: '10mb' }));

app.use((req, res, next) => {
  logger.info(`${req.method} ${req.path}`, {
    ip: req.ip,
    userAgent: req.headers['user-agent'],
  });
  next();
});

app.get('/api/csrf-token', issueCsrfToken);
app.use('/api/public', publicRoutes);
app.use('/api/admin', adminRoutes);

app.get('/', (req, res) => {
  res.json({
    message: 'Leadership Assessment API',
    version: '1.0.0',
    status: 'running',
  });
});

app.use(notFoundHandler);
app.use(errorHandler);

const startServer = async (): Promise<void> => {
  await verifyMigrationsApplied();

  const port = config.port;
  const server = app.listen(port, () => {
    logger.info(`Server running on port ${port}`);
    logger.info(`Environment: ${config.nodeEnv}`);
    logger.info(`CORS enabled for: ${config.cors.origin}`);
  });

  process.on('SIGTERM', () => {
    logger.info('SIGTERM signal received: closing HTTP server');
    server.close(() => {
      logger.info('HTTP server closed');
      process.exit(0);
    });
  });

  process.on('SIGINT', () => {
    logger.info('SIGINT signal received: closing HTTP server');
    server.close(() => {
      logger.info('HTTP server closed');
      process.exit(0);
    });
  });

  process.on('unhandledRejection', (reason, promise) => {
    logger.error('Unhandled Rejection at:', promise, 'reason:', reason);
  });
};

startServer().catch((error) => {
  logger.error('Failed to start server', error);
  process.exit(1);
});

export default app;


==================================================
FILE: debug-output.json
==================================================

{
  "structure": {
    "assessment": {
      "id": "536227ea-765f-488d-95c7-5598e1886fa1",
      "title": "Leadership Capabilities Assessment",
      "description": "Comprehensive assessment of leadership capabilities",
      "version": 1
    },
    "dimensions": [
      {
        "id": "0c7e1dab-635c-41dc-b5d5-fe3aae705cc2",
        "dimensionKey": "strategic_vision",
        "title": "Strategic Vision",
        "description": "Ability to foresee market trends and set long-term goals.",
        "category": "Strategic Leadership",
        "orderIndex": 1,
        "topics": [
          {
            "id": "29ae7254-0c3d-4155-8b8a-5dbcb7d00eac",
            "topicKey": "strategic_vision_topic_1",
            "label": "Strategic Vision Topic 1",
            "prompt": "How would you rate your current capability in strategic vision area 1?",
            "helpText": null,
            "orderIndex": 1
          },
          {
            "id": "c5657fbc-704f-4b1c-9202-9d12790d2f16",
            "topicKey": "strategic_vision_topic_2",
            "label": "Strategic Vision Topic 2",
            "prompt": "How would you rate your current capability in strategic vision area 2?",
            "helpText": null,
            "orderIndex": 2
          },
          {
            "id": "6c4cacb6-601d-4f95-a78b-b991a9f071e7",
            "topicKey": "strategic_vision_topic_3",
            "label": "Strategic Vision Topic 3",
            "prompt": "How would you rate your current capability in strategic vision area 3?",
            "helpText": null,
            "orderIndex": 3
          },
          {
            "id": "6a4596be-86c6-4175-96ce-0e2775516e88",
            "topicKey": "strategic_vision_topic_4",
            "label": "Strategic Vision Topic 4",
            "prompt": "How would you rate your current capability in strategic vision area 4?",
            "helpText": null,
            "orderIndex": 4
          },
          {
            "id": "88f8a21f-7980-408e-bcce-fc6fc2a8c96c",
            "topicKey": "strategic_vision_topic_5",
            "label": "Strategic Vision Topic 5",
            "prompt": "How would you rate your current capability in strategic vision area 5?",
            "helpText": null,
            "orderIndex": 5
          }
        ]
      },
      {
        "id": "ae59b884-d32a-4e76-bb01-971d5cb92753",
        "dimensionKey": "decision_making",
        "title": "Decision Making",
        "description": "Making timely and data-driven choices.",
        "category": "Strategic Leadership",
        "orderIndex": 2,
        "topics": [
          {
            "id": "47a92bb5-8096-4fbe-902b-c64e2d8e45bb",
            "topicKey": "decision_making_topic_1",
            "label": "Decision Making Topic 1",
            "prompt": "How would you rate your current capability in decision making area 1?",
            "helpText": null,
            "orderIndex": 1
          },
          {
            "id": "06673133-c328-468a-95a2-e9eabeda6e4f",
            "topicKey": "decision_making_topic_2",
            "label": "Decision Making Topic 2",
            "prompt": "How would you rate your current capability in decision making area 2?",
            "helpText": null,
            "orderIndex": 2
          },
          {
            "id": "00aaecce-e30c-48c5-853a-7e87da17f230",
            "topicKey": "decision_making_topic_3",
            "label": "Decision Making Topic 3",
            "prompt": "How would you rate your current capability in decision making area 3?",
            "helpText": null,
            "orderIndex": 3
          },
          {
            "id": "b144b35e-db12-4db9-b539-8484479d05ce",
            "topicKey": "decision_making_topic_4",
            "label": "Decision Making Topic 4",
            "prompt": "How would you rate your current capability in decision making area 4?",
            "helpText": null,
            "orderIndex": 4
          },
          {
            "id": "d399017f-5da7-4aa7-b5cd-cc3c1f291b8e",
            "topicKey": "decision_making_topic_5",
            "label": "Decision Making Topic 5",
            "prompt": "How would you rate your current capability in decision making area 5?",
            "helpText": null,
            "orderIndex": 5
          }
        ]
      },
      {
        "id": "5dd141d2-8abd-4400-86bc-7b5f18312594",
        "dimensionKey": "operational_efficiency",
        "title": "Operational Efficiency",
        "description": "Optimizing processes and resource allocation.",
        "category": "Operational Excellence",
        "orderIndex": 3,
        "topics": [
          {
            "id": "1f0f2870-1dd5-4237-94a4-8a98d5f9ba3c",
            "topicKey": "operational_efficiency_topic_1",
            "label": "Operational Efficiency Topic 1",
            "prompt": "How would you rate your current capability in operational efficiency area 1?",
            "helpText": null,
            "orderIndex": 1
          },
          {
            "id": "83402652-b00f-4f3e-a1dc-5e65b2f57e2f",
            "topicKey": "operational_efficiency_topic_2",
            "label": "Operational Efficiency Topic 2",
            "prompt": "How would you rate your current capability in operational efficiency area 2?",
            "helpText": null,
            "orderIndex": 2
          },
          {
            "id": "0171eac5-f088-4c32-a859-52e27eaf4241",
            "topicKey": "operational_efficiency_topic_3",
            "label": "Operational Efficiency Topic 3",
            "prompt": "How would you rate your current capability in operational efficiency area 3?",
            "helpText": null,
            "orderIndex": 3
          },
          {
            "id": "fb939443-5482-483a-b116-8f4219288d8a",
            "topicKey": "operational_efficiency_topic_4",
            "label": "Operational Efficiency Topic 4",
            "prompt": "How would you rate your current capability in operational efficiency area 4?",
            "helpText": null,
            "orderIndex": 4
          },
          {
            "id": "6dc884e3-9175-4dfe-a5ba-7c3d9b13f9df",
            "topicKey": "operational_efficiency_topic_5",
            "label": "Operational Efficiency Topic 5",
            "prompt": "How would you rate your current capability in operational efficiency area 5?",
            "helpText": null,
            "orderIndex": 5
          }
        ]
      },
      {
        "id": "d959f133-8422-431d-9ea6-8ab83b74a7e8",
        "dimensionKey": "team_development",
        "title": "Team Development",
        "description": "Mentoring and growing talent within the organization.",
        "category": "People & Culture",
        "orderIndex": 4,
        "topics": [
          {
            "id": "877e954f-a824-4af4-b32d-0b74a7e1cf22",
            "topicKey": "team_development_topic_1",
            "label": "Team Development Topic 1",
            "prompt": "How would you rate your current capability in team development area 1?",
            "helpText": null,
            "orderIndex": 1
          },
          {
            "id": "e2a56cd9-55cf-49bf-980f-e14f6899a18f",
            "topicKey": "team_development_topic_2",
            "label": "Team Development Topic 2",
            "prompt": "How would you rate your current capability in team development area 2?",
            "helpText": null,
            "orderIndex": 2
          },
          {
            "id": "ea7cb530-e933-4a1a-8616-d8f59e4b7bce",
            "topicKey": "team_development_topic_3",
            "label": "Team Development Topic 3",
            "prompt": "How would you rate your current capability in team development area 3?",
            "helpText": null,
            "orderIndex": 3
          },
          {
            "id": "f3c853ee-b96f-4e34-9a71-661ff1d3b154",
            "topicKey": "team_development_topic_4",
            "label": "Team Development Topic 4",
            "prompt": "How would you rate your current capability in team development area 4?",
            "helpText": null,
            "orderIndex": 4
          },
          {
            "id": "dbb66247-7fb0-4cad-9930-066e69d550b5",
            "topicKey": "team_development_topic_5",
            "label": "Team Development Topic 5",
            "prompt": "How would you rate your current capability in team development area 5?",
            "helpText": null,
            "orderIndex": 5
          }
        ]
      }
    ]
  },
  "recDef": {
    "dimensions": [
      {
        "dimensionKey": "data",
        "recommendations": [
          {
            "id": "rec-data-quality",
            "title": "Establish Data Quality Baseline",
            "description": "High ambition requires reliable data foundations.",
            "conditions": {
              "gapMin": 1.5
            },
            "why": null,
            "what": null,
            "how": null,
            "priority": 0.88,
            "tags": [
              "data",
              "quality"
            ],
            "impactLevel": "high",
            "effortLevel": "medium",
            "timeframe": "short-term",
            "actionItems": [
              {
                "id": "1",
                "text": "Audit core datasets for completeness and accuracy"
              },
              {
                "id": "2",
                "text": "Implement automated data quality checks"
              },
              {
                "id": "3",
                "text": "Define data ownership for critical domains"
              }
            ],
            "resources": null,
            "kpis": null
          }
        ]
      },
      {
        "dimensionKey": "governance",
        "recommendations": [
          {
            "id": "rec-governance-framework",
            "title": "Establish AI Governance Framework",
            "description": "Structured governance accelerates responsible AI adoption.",
            "conditions": {
              "gapMin": 1
            },
            "why": null,
            "what": null,
            "how": null,
            "priority": 0.8,
            "tags": [
              "governance",
              "compliance"
            ],
            "impactLevel": "high",
            "effortLevel": "medium",
            "timeframe": "short-term",
            "actionItems": [
              {
                "id": "1",
                "text": "Create an AI governance charter with clear roles and responsibilities"
              },
              {
                "id": "2",
                "text": "Establish a cross-functional AI review board"
              },
              {
                "id": "3",
                "text": "Define model approval and deployment policies"
              }
            ],
            "resources": null,
            "kpis": null
          },
          {
            "id": "rec-ethics-review",
            "title": "Implement AI Ethics Review Process",
            "description": "Proactive ethics management reduces regulatory and reputational risk.",
            "conditions": {
              "currentMax": 2.5
            },
            "why": null,
            "what": null,
            "how": null,
            "priority": 0.78,
            "tags": [
              "ethics",
              "fairness"
            ],
            "impactLevel": "high",
            "effortLevel": "medium",
            "timeframe": "short-term",
            "actionItems": [
              {
                "id": "1",
                "text": "Publish organizational AI ethics principles"
              },
              {
                "id": "2",
                "text": "Implement bias testing for high-risk models"
              },
              {
                "id": "3",
                "text": "Create an ethics advisory panel with diverse representation"
              }
            ],
            "resources": null,
            "kpis": null
          }
        ]
      },
      {
        "dimensionKey": "strategy",
        "recommendations": [
          {
            "id": "rec-strategy-alignment",
            "title": "Clarify AI Strategy Alignment",
            "description": "Your target ambition exceeds your current alignment with business goals.",
            "conditions": {
              "gapMin": 1,
              "currentMax": 2.5
            },
            "why": null,
            "what": null,
            "how": null,
            "priority": 0.9,
            "tags": [
              "strategy",
              "governance"
            ],
            "impactLevel": "high",
            "effortLevel": "medium",
            "timeframe": "short-term",
            "actionItems": [
              {
                "id": "1",
                "text": "Create a 1-page AI strategy linked to 3 business KPIs"
              },
              {
                "id": "2",
                "text": "Define ownership: sponsor + product lead + data lead"
              },
              {
                "id": "3",
                "text": "Review quarterly with leadership"
              }
            ],
            "resources": null,
            "kpis": null
          },
          {
            "id": "rec-leadership-buyin",
            "title": "Secure Executive Sponsorship",
            "description": "Leadership buy-in is critical for scaling AI initiatives beyond pilots.",
            "conditions": {
              "currentMax": 2
            },
            "why": null,
            "what": null,
            "how": null,
            "priority": 0.85,
            "tags": [
              "leadership",
              "change-management"
            ],
            "impactLevel": "high",
            "effortLevel": "medium",
            "timeframe": "short-term",
            "actionItems": [
              {
                "id": "1",
                "text": "Identify an executive sponsor for AI initiatives"
              },
              {
                "id": "2",
                "text": "Schedule monthly steering committee updates"
              },
              {
                "id": "3",
                "text": "Quantify ROI for early wins to demonstrate value"
              }
            ],
            "resources": null,
            "kpis": null
          }
        ]
      },
      {
        "dimensionKey": "technology",
        "recommendations": [
          {
            "id": "rec-infra-scalability",
            "title": "Plan for Scalable Infrastructure",
            "description": "Your infrastructure may bottleneck AI adoption as you scale.",
            "conditions": {
              "gapMin": 1
            },
            "why": null,
            "what": null,
            "how": null,
            "priority": 0.82,
            "tags": [
              "infrastructure",
              "cloud"
            ],
            "impactLevel": "high",
            "effortLevel": "medium",
            "timeframe": "short-term",
            "actionItems": [
              {
                "id": "1",
                "text": "Evaluate cloud vs. on-premise compute needs"
              },
              {
                "id": "2",
                "text": "Set up auto-scaling for inference workloads"
              },
              {
                "id": "3",
                "text": "Monitor GPU utilization and costs"
              }
            ],
            "resources": null,
            "kpis": null
          },
          {
            "id": "rec-ops-mlops",
            "title": "Adopt MLOps Practices",
            "description": "Moving models to production requires standardized operations.",
            "conditions": {
              "targetMin": 3,
              "currentMax": 2
            },
            "why": null,
            "what": null,
            "how": null,
            "priority": 0.75,
            "tags": [
              "mlops",
              "engineering"
            ],
            "impactLevel": "high",
            "effortLevel": "medium",
            "timeframe": "short-term",
            "actionItems": [
              {
                "id": "1",
                "text": "Version control data and models"
              },
              {
                "id": "2",
                "text": "Automate deployment pipelines (CI/CD for ML)"
              },
              {
                "id": "3",
                "text": "Implement model performance monitoring"
              }
            ],
            "resources": null,
            "kpis": null
          }
        ]
      },
      {
        "dimensionKey": "value",
        "recommendations": [
          {
            "id": "rec-use-case-pipeline",
            "title": "Build an AI Use Case Pipeline",
            "description": "Systematic use case identification drives AI value realization.",
            "conditions": {
              "gapMin": 1,
              "currentMax": 3
            },
            "why": null,
            "what": null,
            "how": null,
            "priority": 0.76,
            "tags": [
              "value",
              "use-cases"
            ],
            "impactLevel": "high",
            "effortLevel": "medium",
            "timeframe": "short-term",
            "actionItems": [
              {
                "id": "1",
                "text": "Run cross-functional ideation workshops quarterly"
              },
              {
                "id": "2",
                "text": "Score use cases on business impact, feasibility, and data readiness"
              },
              {
                "id": "3",
                "text": "Maintain a living backlog of prioritized AI opportunities"
              }
            ],
            "resources": null,
            "kpis": null
          }
        ]
      }
    ],
    "meta": {
      "dimensionColors": {
        "data": "#3b82f6",
        "value": "#f97316",
        "strategy": "#10b981",
        "governance": "#ec4899",
        "technology": "#1d4ed8",
        "capabilities": "#8b5cf6"
      },
      "dimensionWeights": {
        "data": 0.9,
        "value": 0.7,
        "strategy": 0.85,
        "governance": 0.95,
        "technology": 0.75,
        "capabilities": 0.8
      },
      "themeMap": {
        "data": "Data Foundations",
        "cloud": "Technology & Infrastructure",
        "mlops": "Operations & MLOps",
        "quality": "Data Foundations",
        "security": "Security & Compliance",
        "strategy": "Strategy & Governance",
        "compliance": "Security & Compliance",
        "governance": "Strategy & Governance",
        "leadership": "Leadership Enablement",
        "engineering": "Operations & MLOps",
        "infrastructure": "Technology & Infrastructure",
        "change-management": "Leadership Enablement"
      },
      "urgencyTags": [
        "strategy",
        "governance",
        "security",
        "compliance"
      ],
      "dimensionOrder": [
        "governance",
        "data",
        "strategy",
        "capabilities",
        "technology",
        "value"
      ],
      "titleTemplates": {
        "data": [
          "Optimize Data Infrastructure",
          "Ensure Data Quality",
          "Enhance Data Analytics",
          "Implement Data Governance"
        ],
        "value": [
          "Demonstrate ROI",
          "Operationalize AI Use Cases",
          "Measure Business Value",
          "Institutionalize AI Literacy"
        ],
        "default": [
          "Improve {topicLabel}"
        ],
        "strategy": [
          "Align AI Strategy",
          "Execute AI Roadmap",
          "Strengthen Leadership Support",
          "Foster AI-Driven Culture"
        ],
        "governance": [
          "Enforce Ethical Standards",
          "Implement Governance Controls",
          "Strengthen Compliance Framework",
          "Build Ethics Committee"
        ],
        "technology": [
          "Modernize Technology Stack",
          "Integrate AI Fully",
          "Enhance Infrastructure",
          "Deploy AI Platforms"
        ],
        "capabilities": [
          "Build AI Expertise",
          "Develop Technical Capabilities",
          "Provide Continuous AI Upskilling",
          "Build AI Centers of Excellence"
        ]
      },
      "descriptionTemplate": "Elevate {dimensionTitle} by embedding advanced analytics, automation, or predictive capabilities where relevant. Formalize continuous improvement frameworks and link them with organizational strategy. Promote cross-functional collaboration to ensure practices are scaled effectively. Strengthen monitoring and assurance to build trust in outcomes."
    }
  },
  "results": {
    "success": true,
    "data": {
      "overallScore": 3.25,
      "overallGap": 0.75,
      "dimensions": [
        {
          "dimensionKey": "operational_efficiency",
          "title": "Operational Efficiency",
          "score": 3.2,
          "gap": 1.8,
          "priorityScore": 1.8,
          "topics": []
        },
        {
          "dimensionKey": "strategic_vision",
          "title": "Strategic Vision",
          "score": 2.6,
          "gap": 0.8,
          "priorityScore": 0.8,
          "topics": []
        },
        {
          "dimensionKey": "decision_making",
          "title": "Decision Making",
          "score": 3.6,
          "gap": 0.4,
          "priorityScore": 0.4,
          "topics": []
        },
        {
          "dimensionKey": "team_development",
          "title": "Team Development",
          "score": 3.6,
          "gap": 0,
          "priorityScore": 0,
          "topics": []
        }
      ],
      "topGaps": [
        {
          "topicId": "6dc884e3-9175-4dfe-a5ba-7c3d9b13f9df",
          "label": "Operational Efficiency Topic 5",
          "gap": 2,
          "dimensionTitle": "Operational Efficiency"
        },
        {
          "topicId": "d399017f-5da7-4aa7-b5cd-cc3c1f291b8e",
          "label": "Decision Making Topic 5",
          "gap": 2,
          "dimensionTitle": "Decision Making"
        },
        {
          "topicId": "1f0f2870-1dd5-4237-94a4-8a98d5f9ba3c",
          "label": "Operational Efficiency Topic 1",
          "gap": 2,
          "dimensionTitle": "Operational Efficiency"
        },
        {
          "topicId": "0171eac5-f088-4c32-a859-52e27eaf4241",
          "label": "Operational Efficiency Topic 3",
          "gap": 2,
          "dimensionTitle": "Operational Efficiency"
        },
        {
          "topicId": "fb939443-5482-483a-b116-8f4219288d8a",
          "label": "Operational Efficiency Topic 4",
          "gap": 2,
          "dimensionTitle": "Operational Efficiency"
        }
      ],
      "priorities": [
        {
          "dimensionKey": "operational_efficiency",
          "title": "Operational Efficiency",
          "priorityScore": 1.8,
          "rank": 1
        },
        {
          "dimensionKey": "strategic_vision",
          "title": "Strategic Vision",
          "priorityScore": 0.8,
          "rank": 2
        },
        {
          "dimensionKey": "decision_making",
          "title": "Decision Making",
          "priorityScore": 0.4,
          "rank": 3
        },
        {
          "dimensionKey": "team_development",
          "title": "Team Development",
          "priorityScore": 0,
          "rank": 4
        }
      ]
    }
  }
}

==================================================
FILE: narrative.json
==================================================

{
  "version": 1,
  "themeMap": {
    "strategy": "Strategy & Governance",
    "governance": "Strategy & Governance",
    "data": "Data Foundations",
    "quality": "Data Foundations",
    "infrastructure": "Technology & Infrastructure",
    "cloud": "Technology & Infrastructure",
    "mlops": "Operations & MLOps",
    "engineering": "Operations & MLOps",
    "leadership": "Leadership Enablement",
    "change-management": "Leadership Enablement",
    "security": "Security & Compliance",
    "compliance": "Security & Compliance"
  },
  "headlines": {
    "lowConfidencePrefix": "Preliminary {stageLabel} assessment indicates potential",
    "byStageId": {
      "explorer": "Foundational AI capabilities identified for future growth",
      "structured": "Structured AI programs emerging with clear scaling potential",
      "integrated": "Integrated AI capabilities driving cross-functional value",
      "optimized": "Optimized AI maturity driving strategic competitive advantage",
      "default": "{stageLabel} maturity stage assessment"
    }
  },
  "executiveSummary": {
    "sentence1": "The organization is currently assessed at the {stageLabel} stage with an overall readiness score of {currentAvg} / 5.0.",
    "sentence2": "Assessment data indicates immediate opportunities to strengthen {topTheme} capabilities.",
    "sentence3": "Strategic focus should be placed on closing the {gapAvg} point capability gap to reach the target state."
  },
  "stageRationale": "This stage determination reflects a calculated aggregate score of {currentAvg} based on {confidenceLabel} data confidence ({confidenceRatio}% coverage).",
  "priorityWhyTemplate": "Closes a +{gap} gap in {dimensionTitle} by targeting {theme}.",
  "notes": {
    "low": "Confidence is low. Expand survey coverage to key stakeholders to validate these findings.",
    "moderate": "Confidence is moderate. Additional input from cross-functional teams is recommended."
  },
  "maturityThresholds": {
    "leading": 4.5,
    "advanced": 4.0,
    "ready": 3.0,
    "exploring": 2.0
  },
  "executiveTemplates": {
    "maturityLevel": {
      "leading": "The organization has achieved leading-edge maturity ({score}/5.0), demonstrating exceptional capabilities and systematic optimization.",
      "advanced": "The organization shows advanced maturity ({score}/5.0), with well-established processes and strong commitment to continuous improvement.",
      "ready": "The organization has integrated AI readiness into key business processes ({score}/5.0), demonstrating agility and solid foundational capabilities.",
      "exploring": "The organization is actively exploring AI adoption ({score}/5.0), with defined processes taking shape across multiple areas.",
      "beginner": "The organization is in the early stages of AI readiness ({score}/5.0), with significant opportunities for foundational development."
    },
    "gapAnalysis": {
      "large": "{count} critical area{pluralS} ({dimensions}) require{verbS} immediate attention with gaps exceeding 1.0 points.",
      "moderate": "{count} area{pluralS} show{verbS} moderate improvement opportunities.",
      "minimal": "Current capabilities align well with strategic targets, with minimal gaps identified."
    },
    "strengths": {
      "multiple": "Notable strengths include {list}, and {lastItem}.",
      "single": "The organization demonstrates particular strength in {area}."
    },
    "priorities": {
      "high": "Top priorities for advancement include {topics}.",
      "balanced": "The organization shows balanced development across all dimensions."
    }
  }
}


==================================================
FILE: package.json
==================================================

{
  "name": "leadership-assessment-backend",
  "version": "1.0.0",
  "description": "Backend API for Leadership Assessment Platform",
  "main": "dist/index.js",
  "scripts": {
    "dev": "nodemon --exec ts-node src/index.ts",
    "build": "tsc",
    "start": "node dist/index.js",
    "migrate": "ts-node migrations/run.ts",
    "db:migrate": "ts-node migrations/run.ts",
    "migration:run": "ts-node scripts/run-migration.ts",
    "seed": "ts-node seeds/seed-data.ts",
    "seed:all": "ts-node seeds/seed-all.ts",
    "seed:recommendations-narrative": "ts-node seeds/seed-recommendations-and-narrative.ts",
    "verify": "ts-node scripts/post-implementation-verification.ts",
    "test:e2e": "ts-node scripts/e2e-flow-test.ts",
    "test:all": "npm run test:security && npm run test:e2e && npm run verify",
    "setup": "npm run db:migrate && npm run seed",
    "db:setup": "npm run db:migrate && npm run seed",
    "unlock-admin": "ts-node _ops/unlock-admin.ts",
    "reset-admin-password": "ts-node _ops/reset-admin-password.ts",
    "test:security": "ts-node --transpile-only src/tests/security.test.ts"
  },
  "keywords": [
    "leadership",
    "assessment",
    "api"
  ],
  "author": "",
  "license": "ISC",
  "overrides": {
  "tar": "^6.2.1"
},
  "dependencies": {
    "bcrypt": "^5.1.1",
    "cors": "^2.8.5",
    "dotenv": "^16.3.1",
    "express": "^4.18.2",
    "express-rate-limit": "^7.1.5",
    "helmet": "^7.1.0",
    "jsonwebtoken": "^9.0.2",
    "pg": "^8.11.3",
    "typeorm": "^0.3.28",
    "uuid": "^9.0.1",
    "winston": "^3.11.0",
    "zod": "^3.22.4"
  },
  "devDependencies": {
    "@types/bcrypt": "^5.0.2",
    "@types/cors": "^2.8.17",
    "@types/express": "^4.17.21",
    "@types/jsonwebtoken": "^9.0.5",
    "@types/node": "^20.10.6",
    "@types/pg": "^8.10.9",
    "nodemon": "^3.0.2",
    "ts-node": "^10.9.2",
    "typescript": "^5.3.3"
  }
}


==================================================
FILE: questions.json
==================================================

{
  "version": 2,
  "dimensions": [
    {
      "id": "data",
      "title": "Data",
      "order": 10,
      "topics": [
        {
          "id": "data-quality-availability",
          "label": "Data Quality & Availability",
          "prompt": "How clean, complete, and accessible is the data your AI initiatives rely on?",
          "order": 10,
          "anchors": {
            "1": "Data is siloed, inconsistent, and often manually managed with no quality checks.",
            "2": "Some clean data exists but accessibility and cataloguing are limited.",
            "3": "Key data assets are governed, catalogued, and accessible to project teams.",
            "4": "Automated quality checks and pipelines ensure reliable data across the organization.",
            "5": "Data is democratized, real-time, and continuously optimized for AI workloads."
          }
        },
        {
          "id": "data-infrastructure",
          "label": "Data Infrastructure",
          "prompt": "How mature is the infrastructure supporting data storage, processing, and delivery?",
          "order": 20,
          "anchors": {
            "1": "Legacy on-premise systems with manual ETL and no scalability.",
            "2": "Basic cloud or hybrid setup; limited automation in data pipelines.",
            "3": "Scalable cloud data platform with automated ingestion and processing.",
            "4": "Modern lakehouse architecture with streaming and batch capabilities.",
            "5": "Fully automated, event-driven data mesh with self-serve access."
          }
        },
        {
          "id": "data-governance-framework",
          "label": "Data Governance Framework",
          "prompt": "How well-defined are data ownership, policies, and stewardship practices?",
          "order": 30,
          "anchors": {
            "1": "No formal data governance; ownership and policies are undefined.",
            "2": "Informal ownership exists for some datasets; policies are ad-hoc.",
            "3": "Formal governance council with defined owners and documented policies.",
            "4": "Governance is embedded in workflows with automated compliance checks.",
            "5": "Organization-wide data governance drives strategic decision-making."
          }
        },
        {
          "id": "data-analytics-capabilities",
          "label": "Data Analytics Capabilities",
          "prompt": "How advanced are your analytics capabilities from descriptive to predictive?",
          "order": 40,
          "anchors": {
            "1": "Reporting is manual, spreadsheet-based, and backward-looking only.",
            "2": "Basic BI dashboards provide descriptive analytics for some functions.",
            "3": "Diagnostic and trend analytics are used across key business areas.",
            "4": "Predictive models are operationalized and inform business decisions.",
            "5": "Prescriptive analytics and AI-driven insights are embedded enterprise-wide."
          }
        }
      ]
    },
    {
      "id": "capabilities",
      "title": "Capabilities",
      "order": 20,
      "topics": [
        {
          "id": "ai-ml-expertise",
          "label": "AI/ML Expertise",
          "prompt": "How deep is the AI and machine learning expertise within your organization?",
          "order": 10,
          "anchors": {
            "1": "No dedicated AI/ML talent; reliance on external vendors for all efforts.",
            "2": "A few individuals have AI skills but no formal team or career path.",
            "3": "A dedicated data science team delivers projects with defined methodologies.",
            "4": "Cross-functional AI teams with MLOps, research, and engineering capabilities.",
            "5": "World-class AI talent embedded across the organization with continuous upskilling."
          }
        },
        {
          "id": "technology-stack",
          "label": "Technology Stack",
          "prompt": "How mature and integrated is your AI technology stack?",
          "order": 20,
          "anchors": {
            "1": "No AI-specific tools or platforms; experiments use ad-hoc scripts.",
            "2": "Basic ML frameworks in use; no standardized platform or MLOps tooling.",
            "3": "Standardized AI platform with experiment tracking and model registry.",
            "4": "End-to-end MLOps pipeline with CI/CD, monitoring, and automated retraining.",
            "5": "Cutting-edge platform with AutoML, feature stores, and real-time serving."
          }
        },
        {
          "id": "training-opportunities",
          "label": "Training Opportunities",
          "prompt": "How effectively does your organization upskill employees in AI and digital skills?",
          "order": 30,
          "anchors": {
            "1": "No training programs; employees learn informally on their own time.",
            "2": "Occasional workshops or external courses available but not tracked.",
            "3": "Structured learning paths and certifications for key roles.",
            "4": "Comprehensive AI academy with role-based curricula and hands-on labs.",
            "5": "Continuous learning culture with AI literacy programs for all employees."
          }
        },
        {
          "id": "innovation-adoption",
          "label": "Innovation & Adoption",
          "prompt": "How effectively does your organization adopt and scale AI innovations?",
          "order": 40,
          "anchors": {
            "1": "No process for evaluating or adopting AI innovations.",
            "2": "Innovation is sporadic; pilots rarely move beyond proof-of-concept.",
            "3": "Structured innovation process with stage-gate evaluation of AI pilots.",
            "4": "Innovation lab with fast prototyping and clear paths to production.",
            "5": "Innovation is embedded in culture with systematic horizon scanning."
          }
        }
      ]
    },
    {
      "id": "strategy",
      "title": "Strategy",
      "order": 30,
      "topics": [
        {
          "id": "ai-strategy-alignment",
          "label": "AI Strategy Alignment",
          "prompt": "How well is your AI strategy aligned with overall business objectives?",
          "order": 10,
          "anchors": {
            "1": "AI initiatives are disconnected from business strategy and pursued in isolation.",
            "2": "Some AI projects align with strategic objectives, but efforts remain fragmented.",
            "3": "AI initiatives are systematically aligned with business goals and key functions.",
            "4": "AI strategy is integrated with overall business strategy and drives measurable outcomes.",
            "5": "AI is a core enabler of business strategy and creates competitive advantage."
          }
        },
        {
          "id": "leadership-sponsorship",
          "label": "Leadership & Sponsorship",
          "prompt": "Does leadership actively champion and resource AI initiatives?",
          "order": 20,
          "anchors": {
            "1": "No executive ownership; AI efforts are ad-hoc and unsupported.",
            "2": "A few leaders sponsor isolated initiatives without consistent governance.",
            "3": "Clear executive sponsorship exists with defined goals and accountability.",
            "4": "Leadership actively steers AI portfolio with KPIs and cross-functional alignment.",
            "5": "Executives champion AI as a strategic capability and continuously invest to scale impact."
          }
        },
        {
          "id": "change-management",
          "label": "Change Management",
          "prompt": "How effectively does your organization manage the change required for AI adoption?",
          "order": 30,
          "anchors": {
            "1": "No change management; AI projects create confusion and resistance.",
            "2": "Basic communication about AI projects but no structured change process.",
            "3": "Change management plans accompany major AI initiatives with stakeholder engagement.",
            "4": "Dedicated change team manages adoption, training, and impact assessment.",
            "5": "Change agility is a core competency; organization adapts fluidly to AI-driven transformation."
          }
        },
        {
          "id": "roadmap-planning",
          "label": "Roadmap & Planning",
          "prompt": "How mature is your AI roadmap and investment planning process?",
          "order": 40,
          "anchors": {
            "1": "No AI roadmap; initiatives are reactive and uncoordinated.",
            "2": "Informal list of AI ideas without prioritization or resource planning.",
            "3": "Documented AI roadmap with prioritized initiatives and milestones.",
            "4": "Multi-year AI investment plan with portfolio management and governance.",
            "5": "Dynamic AI roadmap continuously updated based on outcomes and market signals."
          }
        }
      ]
    },
    {
      "id": "governance",
      "title": "Governance",
      "order": 40,
      "topics": [
        {
          "id": "governance-framework",
          "label": "Governance Framework",
          "prompt": "How well-established is your AI governance framework?",
          "order": 10,
          "anchors": {
            "1": "No AI governance; decisions are made ad-hoc without oversight.",
            "2": "Basic guidelines exist but are not enforced or widely known.",
            "3": "Formal governance framework with clear roles, policies, and review boards.",
            "4": "Governance is embedded in AI lifecycle with automated compliance gates.",
            "5": "Best-in-class governance that balances innovation speed with risk management."
          }
        },
        {
          "id": "ethical-issues",
          "label": "Ethical Issues",
          "prompt": "How does your organization handle AI ethics, fairness, and bias?",
          "order": 20,
          "anchors": {
            "1": "No awareness of AI ethics; bias and fairness are not considered.",
            "2": "Informal awareness but no documented guidelines or review process.",
            "3": "Published AI ethics principles with bias testing for high-risk models.",
            "4": "Ethics review board evaluates models pre-deployment with fairness metrics.",
            "5": "Ethics-by-design approach with continuous monitoring and external auditing."
          }
        },
        {
          "id": "risk-management",
          "label": "Risk Management",
          "prompt": "How mature is your AI risk identification and mitigation process?",
          "order": 30,
          "anchors": {
            "1": "AI risks are not identified, tracked, or mitigated.",
            "2": "Some risks are recognized but management is reactive and inconsistent.",
            "3": "Risk assessment is part of AI project planning with defined mitigation steps.",
            "4": "Enterprise AI risk register with quantified impact and automated monitoring.",
            "5": "Proactive risk intelligence informs AI strategy and investment decisions."
          }
        },
        {
          "id": "compliance-standards",
          "label": "Compliance & Standards",
          "prompt": "How well does your AI practice comply with regulatory and industry standards?",
          "order": 40,
          "anchors": {
            "1": "No awareness of AI-related regulations or industry standards.",
            "2": "Basic awareness but no formal compliance program or documentation.",
            "3": "Compliance requirements are documented and tracked for AI systems.",
            "4": "Automated compliance checks integrated into the AI development lifecycle.",
            "5": "Proactive regulatory engagement with industry-leading standards adoption."
          }
        }
      ]
    },
    {
      "id": "value",
      "title": "Value",
      "order": 50,
      "topics": [
        {
          "id": "use-cases",
          "label": "Use Cases",
          "prompt": "How effectively does your organization identify and prioritize AI use cases?",
          "order": 10,
          "anchors": {
            "1": "No systematic process for identifying AI opportunities.",
            "2": "Ad-hoc ideas collected but no structured evaluation or prioritization.",
            "3": "Use case pipeline with business impact scoring and feasibility analysis.",
            "4": "Portfolio approach with strategic alignment, ROI estimation, and stage-gating.",
            "5": "AI opportunity radar with continuous scanning and rapid validation."
          }
        },
        {
          "id": "roi-measurement",
          "label": "ROI Measurement",
          "prompt": "How well does your organization measure the return on AI investments?",
          "order": 20,
          "anchors": {
            "1": "AI ROI is never measured or discussed.",
            "2": "Anecdotal success stories but no formal metrics or tracking.",
            "3": "KPIs defined for AI projects with periodic ROI reviews.",
            "4": "Standardized value measurement framework across all AI initiatives.",
            "5": "Real-time value dashboards with attribution modeling and impact analytics."
          }
        },
        {
          "id": "stakeholder-engagement",
          "label": "Stakeholder Engagement",
          "prompt": "How engaged are business stakeholders in AI initiative design and outcomes?",
          "order": 30,
          "anchors": {
            "1": "Business stakeholders are uninvolved; AI is purely a technical exercise.",
            "2": "Stakeholders are consulted occasionally but not actively engaged.",
            "3": "Business owners co-design AI solutions and validate outcomes.",
            "4": "Cross-functional teams with embedded business and AI expertise.",
            "5": "Stakeholders champion AI adoption and drive demand for new capabilities."
          }
        },
        {
          "id": "ai-solutions",
          "label": "AI Solutions",
          "prompt": "How successfully has your organization deployed AI solutions into production?",
          "order": 40,
          "anchors": {
            "1": "No AI solutions in production; all efforts are experimental.",
            "2": "One or two pilots running but none scaled to production impact.",
            "3": "Several AI solutions in production delivering measurable business value.",
            "4": "AI solutions are core to key business processes with continuous improvement.",
            "5": "AI-native products and services generate significant competitive advantage."
          }
        }
      ]
    },
    {
      "id": "technology",
      "title": "Technology",
      "order": 60,
      "topics": [
        {
          "id": "ai-ml-platforms",
          "label": "AI/ML Platforms",
          "prompt": "How mature are the AI and ML platforms supporting your initiatives?",
          "order": 10,
          "anchors": {
            "1": "No dedicated AI/ML platform; teams use disparate tools and scripts.",
            "2": "Basic ML framework adoption but no shared platform or standards.",
            "3": "Centralized ML platform with experiment tracking and shared resources.",
            "4": "Enterprise-grade platform with AutoML, feature stores, and model serving.",
            "5": "Best-in-class AI platform with real-time inference and continuous learning."
          }
        },
        {
          "id": "cloud-infrastructure",
          "label": "Cloud Infrastructure",
          "prompt": "How ready is your cloud infrastructure for AI workloads at scale?",
          "order": 20,
          "anchors": {
            "1": "Infrastructure is legacy, on-premise, and unsuited for AI workloads.",
            "2": "Some cloud usage but infrastructure lacks scalability for AI.",
            "3": "Hybrid or cloud setup supports basic model training and deployment.",
            "4": "Scalable MLOps infrastructure is in place for efficient model lifecycle.",
            "5": "Fully automated, serverless, and optimized for AI at scale."
          }
        },
        {
          "id": "integration-capabilities",
          "label": "Integration Capabilities",
          "prompt": "How well can AI systems integrate with existing business applications?",
          "order": 30,
          "anchors": {
            "1": "AI outputs are copy-pasted manually; no system integration.",
            "2": "Basic API integration for a few use cases; mostly point-to-point.",
            "3": "Standard integration patterns with APIs and event-driven architecture.",
            "4": "AI services are composable microservices integrated into core platforms.",
            "5": "Seamless AI-native integration across the entire digital ecosystem."
          }
        },
        {
          "id": "security-privacy",
          "label": "Security & Privacy",
          "prompt": "How mature are your AI-specific security and privacy practices?",
          "order": 40,
          "anchors": {
            "1": "No AI-specific security measures; models and data are unprotected.",
            "2": "Basic access controls but no adversarial testing or privacy measures.",
            "3": "Security reviews for AI systems with data privacy impact assessments.",
            "4": "Automated vulnerability scanning for models with differential privacy.",
            "5": "Zero-trust AI security with federated learning and privacy-preserving AI."
          }
        }
      ]
    }
  ]
}


==================================================
FILE: recommendations.json
==================================================

{
  "version": 2,
  "rules": [
    {
      "id": "rec-strategy-alignment",
      "dimensionId": "strategy",
      "topicId": "ai-strategy-alignment",
      "priority": 90,
      "conditions": {
        "gapMin": 1.0,
        "currentMax": 2.5
      },
      "title": "Clarify AI Strategy Alignment",
      "summary": "Your target ambition exceeds your current alignment with business goals.",
      "actions": [
        "Create a 1-page AI strategy linked to 3 business KPIs",
        "Define ownership: sponsor + product lead + data lead",
        "Review quarterly with leadership"
      ],
      "tags": ["strategy", "governance"]
    },
    {
      "id": "rec-leadership-buyin",
      "dimensionId": "strategy",
      "topicId": "leadership-sponsorship",
      "priority": 85,
      "conditions": {
        "currentMax": 2.0
      },
      "title": "Secure Executive Sponsorship",
      "summary": "Leadership buy-in is critical for scaling AI initiatives beyond pilots.",
      "actions": [
        "Identify an executive sponsor for AI initiatives",
        "Schedule monthly steering committee updates",
        "Quantify ROI for early wins to demonstrate value"
      ],
      "tags": ["leadership", "change-management"]
    },
    {
      "id": "rec-data-quality",
      "dimensionId": "data",
      "topicId": "data-quality-availability",
      "priority": 88,
      "conditions": {
        "gapMin": 1.5
      },
      "title": "Establish Data Quality Baseline",
      "summary": "High ambition requires reliable data foundations.",
      "actions": [
        "Audit core datasets for completeness and accuracy",
        "Implement automated data quality checks",
        "Define data ownership for critical domains"
      ],
      "tags": ["data", "quality"]
    },
    {
      "id": "rec-infra-scalability",
      "dimensionId": "technology",
      "topicId": "cloud-infrastructure",
      "priority": 82,
      "conditions": {
        "gapMin": 1.0
      },
      "title": "Plan for Scalable Infrastructure",
      "summary": "Your infrastructure may bottleneck AI adoption as you scale.",
      "actions": [
        "Evaluate cloud vs. on-premise compute needs",
        "Set up auto-scaling for inference workloads",
        "Monitor GPU utilization and costs"
      ],
      "tags": ["infrastructure", "cloud"]
    },
    {
      "id": "rec-ops-mlops",
      "dimensionId": "technology",
      "topicId": "ai-ml-platforms",
      "priority": 75,
      "conditions": {
        "currentMax": 2.0,
        "targetMin": 3.0
      },
      "title": "Adopt MLOps Practices",
      "summary": "Moving models to production requires standardized operations.",
      "actions": [
        "Version control data and models",
        "Automate deployment pipelines (CI/CD for ML)",
        "Implement model performance monitoring"
      ],
      "tags": ["mlops", "engineering"]
    },
    {
      "id": "rec-governance-framework",
      "dimensionId": "governance",
      "topicId": "governance-framework",
      "priority": 80,
      "conditions": {
        "gapMin": 1.0
      },
      "title": "Establish AI Governance Framework",
      "summary": "Structured governance accelerates responsible AI adoption.",
      "actions": [
        "Create an AI governance charter with clear roles and responsibilities",
        "Establish a cross-functional AI review board",
        "Define model approval and deployment policies"
      ],
      "tags": ["governance", "compliance"]
    },
    {
      "id": "rec-ethics-review",
      "dimensionId": "governance",
      "topicId": "ethical-issues",
      "priority": 78,
      "conditions": {
        "currentMax": 2.5
      },
      "title": "Implement AI Ethics Review Process",
      "summary": "Proactive ethics management reduces regulatory and reputational risk.",
      "actions": [
        "Publish organizational AI ethics principles",
        "Implement bias testing for high-risk models",
        "Create an ethics advisory panel with diverse representation"
      ],
      "tags": ["ethics", "fairness"]
    },
    {
      "id": "rec-use-case-pipeline",
      "dimensionId": "value",
      "topicId": "use-cases",
      "priority": 76,
      "conditions": {
        "gapMin": 1.0,
        "currentMax": 3.0
      },
      "title": "Build an AI Use Case Pipeline",
      "summary": "Systematic use case identification drives AI value realization.",
      "actions": [
        "Run cross-functional ideation workshops quarterly",
        "Score use cases on business impact, feasibility, and data readiness",
        "Maintain a living backlog of prioritized AI opportunities"
      ],
      "tags": ["value", "use-cases"]
    }
  ],
  "meta": {
    "themeMap": {
      "strategy": "Strategy & Governance",
      "governance": "Strategy & Governance",
      "data": "Data Foundations",
      "quality": "Data Foundations",
      "infrastructure": "Technology & Infrastructure",
      "cloud": "Technology & Infrastructure",
      "mlops": "Operations & MLOps",
      "engineering": "Operations & MLOps",
      "leadership": "Leadership Enablement",
      "change-management": "Leadership Enablement",
      "security": "Security & Compliance",
      "compliance": "Security & Compliance"
    },
    "urgencyTags": ["strategy", "governance", "security", "compliance"],
    "dimensionOrder": ["governance", "data", "strategy", "capabilities", "technology", "value"],
    "dimensionWeights": {
      "governance": 0.95,
      "data": 0.90,
      "strategy": 0.85,
      "capabilities": 0.80,
      "technology": 0.75,
      "value": 0.70
    },
    "dimensionColors": {
      "governance": "#ec4899",
      "data": "#3b82f6",
      "strategy": "#10b981",
      "capabilities": "#8b5cf6",
      "technology": "#1d4ed8",
      "value": "#f97316"
    },
    "titleTemplates": {
      "governance": ["Enforce Ethical Standards", "Implement Governance Controls", "Strengthen Compliance Framework", "Build Ethics Committee"],
      "data": ["Optimize Data Infrastructure", "Ensure Data Quality", "Enhance Data Analytics", "Implement Data Governance"],
      "strategy": ["Align AI Strategy", "Execute AI Roadmap", "Strengthen Leadership Support", "Foster AI-Driven Culture"],
      "capabilities": ["Build AI Expertise", "Develop Technical Capabilities", "Provide Continuous AI Upskilling", "Build AI Centers of Excellence"],
      "technology": ["Modernize Technology Stack", "Integrate AI Fully", "Enhance Infrastructure", "Deploy AI Platforms"],
      "value": ["Demonstrate ROI", "Operationalize AI Use Cases", "Measure Business Value", "Institutionalize AI Literacy"],
      "default": ["Improve {topicLabel}"]
    },
    "descriptionTemplate": "Elevate {dimensionTitle} by embedding advanced analytics, automation, or predictive capabilities where relevant. Formalize continuous improvement frameworks and link them with organizational strategy. Promote cross-functional collaboration to ensure practices are scaled effectively. Strengthen monitoring and assurance to build trust in outcomes."
  }
}


==================================================
FILE: tsconfig.json
==================================================

{
  "compilerOptions": {
    "target": "ES2020",
    "module": "commonjs",
    "lib": ["ES2020"],
    "outDir": "./dist",
    "rootDir": "./src",
    "strict": true,
    "esModuleInterop": true,
    "skipLibCheck": true,
    "forceConsistentCasingInFileNames": true,
    "resolveJsonModule": true,
    "moduleResolution": "node",
    "types": ["node"]
  },
  "include": ["src/**/*"],
  "exclude": ["node_modules", "dist"]
}
